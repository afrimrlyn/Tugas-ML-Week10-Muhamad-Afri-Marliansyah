{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZlKMU_gmI_ef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca dataset\n",
        "df = pd.read_csv('/content/garments_worker_productivity.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "w75lwBVqKLpf",
        "outputId": "c60776d9-bdf9-46cd-8ade-a10be0145743"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       date   quarter  department       day  team  targeted_productivity  \\\n",
              "0  1/1/2015  Quarter1      sweing  Thursday     8                   0.80   \n",
              "1  1/1/2015  Quarter1  finishing   Thursday     1                   0.75   \n",
              "2  1/1/2015  Quarter1      sweing  Thursday    11                   0.80   \n",
              "3  1/1/2015  Quarter1      sweing  Thursday    12                   0.80   \n",
              "4  1/1/2015  Quarter1      sweing  Thursday     6                   0.80   \n",
              "\n",
              "     smv     wip  over_time  incentive  idle_time  idle_men  \\\n",
              "0  26.16  1108.0       7080         98        0.0         0   \n",
              "1   3.94     NaN        960          0        0.0         0   \n",
              "2  11.41   968.0       3660         50        0.0         0   \n",
              "3  11.41   968.0       3660         50        0.0         0   \n",
              "4  25.90  1170.0       1920         50        0.0         0   \n",
              "\n",
              "   no_of_style_change  no_of_workers  actual_productivity  \n",
              "0                   0           59.0             0.940725  \n",
              "1                   0            8.0             0.886500  \n",
              "2                   0           30.5             0.800570  \n",
              "3                   0           30.5             0.800570  \n",
              "4                   0           56.0             0.800382  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bec7eaad-b8fc-443a-8095-59722f9cde0e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>quarter</th>\n",
              "      <th>department</th>\n",
              "      <th>day</th>\n",
              "      <th>team</th>\n",
              "      <th>targeted_productivity</th>\n",
              "      <th>smv</th>\n",
              "      <th>wip</th>\n",
              "      <th>over_time</th>\n",
              "      <th>incentive</th>\n",
              "      <th>idle_time</th>\n",
              "      <th>idle_men</th>\n",
              "      <th>no_of_style_change</th>\n",
              "      <th>no_of_workers</th>\n",
              "      <th>actual_productivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>Quarter1</td>\n",
              "      <td>sweing</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>8</td>\n",
              "      <td>0.80</td>\n",
              "      <td>26.16</td>\n",
              "      <td>1108.0</td>\n",
              "      <td>7080</td>\n",
              "      <td>98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.940725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>Quarter1</td>\n",
              "      <td>finishing</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.94</td>\n",
              "      <td>NaN</td>\n",
              "      <td>960</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.886500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>Quarter1</td>\n",
              "      <td>sweing</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>11</td>\n",
              "      <td>0.80</td>\n",
              "      <td>11.41</td>\n",
              "      <td>968.0</td>\n",
              "      <td>3660</td>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.800570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>Quarter1</td>\n",
              "      <td>sweing</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>12</td>\n",
              "      <td>0.80</td>\n",
              "      <td>11.41</td>\n",
              "      <td>968.0</td>\n",
              "      <td>3660</td>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.800570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/1/2015</td>\n",
              "      <td>Quarter1</td>\n",
              "      <td>sweing</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>6</td>\n",
              "      <td>0.80</td>\n",
              "      <td>25.90</td>\n",
              "      <td>1170.0</td>\n",
              "      <td>1920</td>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.800382</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bec7eaad-b8fc-443a-8095-59722f9cde0e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bec7eaad-b8fc-443a-8095-59722f9cde0e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bec7eaad-b8fc-443a-8095-59722f9cde0e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c3ef050-006b-4782-a7ee-ff2e1717ae12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c3ef050-006b-4782-a7ee-ff2e1717ae12')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c3ef050-006b-4782-a7ee-ff2e1717ae12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1197,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"1/1/2015\",\n          \"1/7/2015\",\n          \"2/10/2015\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Quarter2\",\n          \"Quarter5\",\n          \"Quarter3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"department\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"sweing\",\n          \"finishing \",\n          \"finishing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Thursday\",\n          \"Saturday\",\n          \"Wednesday\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"team\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          5,\n          10,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"targeted_productivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09789096325790585,\n        \"min\": 0.07,\n        \"max\": 0.8,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.07,\n          0.75,\n          0.35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.943219199514333,\n        \"min\": 2.9,\n        \"max\": 54.56,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          14.61,\n          26.16,\n          30.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1837.4550011056342,\n        \"min\": 7.0,\n        \"max\": 23122.0,\n        \"num_unique_values\": 548,\n        \"samples\": [\n          1287.0,\n          970.0,\n          958.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"over_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3348,\n        \"min\": 0,\n        \"max\": 25920,\n        \"num_unique_values\": 143,\n        \"samples\": [\n          5820,\n          6780,\n          7470\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incentive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 160,\n        \"min\": 0,\n        \"max\": 3600,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          55,\n          65,\n          81\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idle_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.709756518546547,\n        \"min\": 0.0,\n        \"max\": 300.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          4.0,\n          3.5,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idle_men\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 45,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          25,\n          10,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_style_change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_workers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.19768668225038,\n        \"min\": 2.0,\n        \"max\": 89.0,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          59.0,\n          55.0,\n          26.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"actual_productivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17448790350939525,\n        \"min\": 0.233705476,\n        \"max\": 1.1204375,\n        \"num_unique_values\": 879,\n        \"samples\": [\n          0.750031447,\n          0.750031898,\n          0.927291667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXhcj9hGKjzg",
        "outputId": "af86a366-833b-4f8c-c37b-fe0996d2e47d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1197 entries, 0 to 1196\n",
            "Data columns (total 15 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   date                   1197 non-null   object \n",
            " 1   quarter                1197 non-null   object \n",
            " 2   department             1197 non-null   object \n",
            " 3   day                    1197 non-null   object \n",
            " 4   team                   1197 non-null   int64  \n",
            " 5   targeted_productivity  1197 non-null   float64\n",
            " 6   smv                    1197 non-null   float64\n",
            " 7   wip                    691 non-null    float64\n",
            " 8   over_time              1197 non-null   int64  \n",
            " 9   incentive              1197 non-null   int64  \n",
            " 10  idle_time              1197 non-null   float64\n",
            " 11  idle_men               1197 non-null   int64  \n",
            " 12  no_of_style_change     1197 non-null   int64  \n",
            " 13  no_of_workers          1197 non-null   float64\n",
            " 14  actual_productivity    1197 non-null   float64\n",
            "dtypes: float64(6), int64(5), object(4)\n",
            "memory usage: 140.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "WbUcvI3mKqa7",
        "outputId": "daeb00bd-e3bc-4b38-c7bc-8fd4a197fc3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              team  targeted_productivity          smv           wip  \\\n",
              "count  1197.000000            1197.000000  1197.000000    691.000000   \n",
              "mean      6.426901               0.729632    15.062172   1190.465991   \n",
              "std       3.463963               0.097891    10.943219   1837.455001   \n",
              "min       1.000000               0.070000     2.900000      7.000000   \n",
              "25%       3.000000               0.700000     3.940000    774.500000   \n",
              "50%       6.000000               0.750000    15.260000   1039.000000   \n",
              "75%       9.000000               0.800000    24.260000   1252.500000   \n",
              "max      12.000000               0.800000    54.560000  23122.000000   \n",
              "\n",
              "          over_time    incentive    idle_time     idle_men  \\\n",
              "count   1197.000000  1197.000000  1197.000000  1197.000000   \n",
              "mean    4567.460317    38.210526     0.730159     0.369256   \n",
              "std     3348.823563   160.182643    12.709757     3.268987   \n",
              "min        0.000000     0.000000     0.000000     0.000000   \n",
              "25%     1440.000000     0.000000     0.000000     0.000000   \n",
              "50%     3960.000000     0.000000     0.000000     0.000000   \n",
              "75%     6960.000000    50.000000     0.000000     0.000000   \n",
              "max    25920.000000  3600.000000   300.000000    45.000000   \n",
              "\n",
              "       no_of_style_change  no_of_workers  actual_productivity  \n",
              "count         1197.000000    1197.000000          1197.000000  \n",
              "mean             0.150376      34.609858             0.735091  \n",
              "std              0.427848      22.197687             0.174488  \n",
              "min              0.000000       2.000000             0.233705  \n",
              "25%              0.000000       9.000000             0.650307  \n",
              "50%              0.000000      34.000000             0.773333  \n",
              "75%              0.000000      57.000000             0.850253  \n",
              "max              2.000000      89.000000             1.120437  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b066e6e2-b91f-44c1-a5cd-3897fa0e06c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>team</th>\n",
              "      <th>targeted_productivity</th>\n",
              "      <th>smv</th>\n",
              "      <th>wip</th>\n",
              "      <th>over_time</th>\n",
              "      <th>incentive</th>\n",
              "      <th>idle_time</th>\n",
              "      <th>idle_men</th>\n",
              "      <th>no_of_style_change</th>\n",
              "      <th>no_of_workers</th>\n",
              "      <th>actual_productivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>691.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "      <td>1197.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.426901</td>\n",
              "      <td>0.729632</td>\n",
              "      <td>15.062172</td>\n",
              "      <td>1190.465991</td>\n",
              "      <td>4567.460317</td>\n",
              "      <td>38.210526</td>\n",
              "      <td>0.730159</td>\n",
              "      <td>0.369256</td>\n",
              "      <td>0.150376</td>\n",
              "      <td>34.609858</td>\n",
              "      <td>0.735091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.463963</td>\n",
              "      <td>0.097891</td>\n",
              "      <td>10.943219</td>\n",
              "      <td>1837.455001</td>\n",
              "      <td>3348.823563</td>\n",
              "      <td>160.182643</td>\n",
              "      <td>12.709757</td>\n",
              "      <td>3.268987</td>\n",
              "      <td>0.427848</td>\n",
              "      <td>22.197687</td>\n",
              "      <td>0.174488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.233705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>3.940000</td>\n",
              "      <td>774.500000</td>\n",
              "      <td>1440.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.650307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>15.260000</td>\n",
              "      <td>1039.000000</td>\n",
              "      <td>3960.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.773333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>24.260000</td>\n",
              "      <td>1252.500000</td>\n",
              "      <td>6960.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>0.850253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>54.560000</td>\n",
              "      <td>23122.000000</td>\n",
              "      <td>25920.000000</td>\n",
              "      <td>3600.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>1.120437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b066e6e2-b91f-44c1-a5cd-3897fa0e06c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b066e6e2-b91f-44c1-a5cd-3897fa0e06c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b066e6e2-b91f-44c1-a5cd-3897fa0e06c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ccbe44d-4c60-42f4-b837-1fed8aa47cb4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ccbe44d-4c60-42f4-b837-1fed8aa47cb4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ccbe44d-4c60-42f4-b837-1fed8aa47cb4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"team\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 421.1525987837647,\n        \"min\": 1.0,\n        \"max\": 1197.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.426900584795321,\n          6.0,\n          1197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"targeted_productivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 423.004138802404,\n        \"min\": 0.07,\n        \"max\": 1197.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1197.0,\n          0.7296324143692565,\n          0.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 417.1126021762678,\n        \"min\": 2.9,\n        \"max\": 1197.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          15.062172096908938,\n          15.26,\n          1197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7849.391769111412,\n        \"min\": 7.0,\n        \"max\": 23122.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1190.4659913169319,\n          1039.0,\n          691.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"over_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8373.31562922651,\n        \"min\": 0.0,\n        \"max\": 25920.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4567.460317460317,\n          3960.0,\n          1197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incentive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1267.191974096936,\n        \"min\": 0.0,\n        \"max\": 3600.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1197.0,\n          38.21052631578947,\n          3600.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idle_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 420.5086170079323,\n        \"min\": 0.0,\n        \"max\": 1197.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7301587301587301,\n          300.0,\n          12.709756518546547\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idle_men\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 421.0349482146399,\n        \"min\": 0.0,\n        \"max\": 1197.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3692564745196324,\n          45.0,\n          3.26898732417817\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_style_change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 423.07373830286184,\n        \"min\": 0.0,\n        \"max\": 1197.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.15037593984962405,\n          2.0,\n          0.4278478565061976\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_workers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 411.6130246489885,\n        \"min\": 2.0,\n        \"max\": 1197.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          34.60985797827903,\n          34.0,\n          1197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"actual_productivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 422.97433975712767,\n        \"min\": 0.17448790350939525,\n        \"max\": 1197.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.7350910969791145,\n          0.773333333,\n          1197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "gwc5v_--KwFz",
        "outputId": "f0fa217c-1c03-4bbe-e509-ae26e2acb6d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                       0\n",
              "quarter                    0\n",
              "department                 0\n",
              "day                        0\n",
              "team                       0\n",
              "targeted_productivity      0\n",
              "smv                        0\n",
              "wip                      506\n",
              "over_time                  0\n",
              "incentive                  0\n",
              "idle_time                  0\n",
              "idle_men                   0\n",
              "no_of_style_change         0\n",
              "no_of_workers              0\n",
              "actual_productivity        0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>department</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>team</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>targeted_productivity</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smv</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wip</th>\n",
              "      <td>506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>over_time</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incentive</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idle_time</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idle_men</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>no_of_style_change</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>no_of_workers</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual_productivity</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGlFyJffLxve",
        "outputId": "11cf1ffa-3900-4b93-d838-52934ad2e50e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'quarter', 'department', 'day', 'team', 'targeted_productivity',\n",
              "       'smv', 'wip', 'over_time', 'incentive', 'idle_time', 'idle_men',\n",
              "       'no_of_style_change', 'no_of_workers', 'actual_productivity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonversi nilai kategorikal pada kolom tertentu menjadi nilai numerik menggunakan LabelEncoder\n",
        "label_cols = ['date', 'quarter', 'department', 'day', 'team', 'targeted_productivity',\n",
        "'smv', 'wip', 'over_time', 'incentive', 'idle_time', 'idle_men',\n",
        "'no_of_style_change', 'no_of_workers', 'actual_productivity']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in label_cols:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        ""
      ],
      "metadata": {
        "id": "YkDZZOoiL2LP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "source": [
        "# Menetapkan fitur (X) dan target (y)\n",
        "X = df.drop(['actual_productivity'], axis=1)  # Menghapus kolom target 'actual_productivity'\n",
        "y = df['actual_productivity']  # Kolom target 'actual_productivity'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oBUL-dodM1i-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi training dan testing (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4G0hDGwNM6qq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan standardisasi pada fitur numerik\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BnBBoti7M-h0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi data ke tensor\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "FYeJ_qAmNHzd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model MLP untuk tugas regresi\n",
        "class MLPRegression(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, neurons, activation):\n",
        "        super(MLPRegression, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.neurons = neurons\n",
        "        self.activation = activation\n",
        "\n",
        "        # Menambahkan layer pertama dari input ke hidden layer pertama\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(self.input_size, self.neurons))\n",
        "\n",
        "        # Menambahkan hidden layers tambahan sesuai jumlah yang ditentukan\n",
        "        for _ in range(self.hidden_layers - 1):\n",
        "            layers.append(self.activation())  # Menambahkan fungsi aktivasi\n",
        "            layers.append(nn.Linear(self.neurons, self.neurons))  # Menambahkan layer dense\n",
        "\n",
        "        layers.append(nn.Linear(self.neurons, 1))  # Menambahkan layer output (1 neuron)\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze()  # Menghasilkan output dengan menghapus dimensi ekstra\n"
      ],
      "metadata": {
        "id": "eyBeLjAjNaJv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan perangkat yang akan digunakan (GPU jika tersedia, jika tidak gunakan CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Daftar hyperparameter untuk diuji\n",
        "hidden_layers = [1, 2, 3]  # Jumlah hidden layers\n",
        "neurons = [4, 8, 16, 32, 64]  # Jumlah neuron di setiap layer\n",
        "activations = [nn.Sigmoid, nn.ReLU, nn.Softmax, nn.Tanh]  # Fungsi aktivasi\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]  # Jumlah epoch untuk pelatihan\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]  # Nilai learning rate\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]  # Ukuran batch\n",
        "\n",
        "# Variabel untuk menyimpan hasil eksperimen\n",
        "results = []\n",
        "\n",
        "# Melakukan eksperimen menggunakan semua kombinasi hyperparameter\n",
        "for layers in hidden_layers:\n",
        "    for neuron in neurons:\n",
        "        for activation in activations:\n",
        "            for epochs in epochs_list:\n",
        "                for lr in learning_rates:\n",
        "                    for batch_size in batch_sizes:\n",
        "                        # Membuat model dan memindahkannya ke perangkat (GPU/CPU)\n",
        "                        model = MLPRegression(input_size=X_train_tensor.shape[1],\n",
        "                                              hidden_layers=layers,\n",
        "                                              neurons=neuron,\n",
        "                                              activation=activation).to(device)\n",
        "\n",
        "                        # Mendefinisikan fungsi loss dan optimizer\n",
        "                        criterion = nn.MSELoss()  # Mean Squared Error sebagai fungsi loss\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n",
        "\n",
        "                        # Training loop untuk melatih model\n",
        "                        for epoch in range(epochs):\n",
        "                            model.train()\n",
        "                            optimizer.zero_grad()  # Reset gradien\n",
        "                            outputs = model(X_train_tensor.to(device))  # Forward pass\n",
        "                            loss = criterion(outputs, y_train_tensor.to(device))  # Hitung loss\n",
        "                            loss.backward()  # Backpropagation\n",
        "                            optimizer.step()  # Perbarui parameter model\n",
        "\n",
        "                        # Evaluasi model setelah selesai pelatihan\n",
        "                        model.eval()\n",
        "                        with torch.no_grad():\n",
        "                            y_pred = model(X_test_tensor.to(device)).cpu().numpy()  # Prediksi pada data uji\n",
        "                            mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "                            mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
        "                            r2 = r2_score(y_test, y_pred)  # R-squared score\n",
        "                            accuracy = (1 - mae / max(y_test)) * 100  # Estimasi akurasi dalam persentase\n",
        "\n",
        "                        # Menyimpan hasil ke dalam daftar\n",
        "                        results.append({\n",
        "                            'layers': layers,\n",
        "                            'neurons': neuron,\n",
        "                            'activation': activation.__name__,\n",
        "                            'epochs': epochs,\n",
        "                            'lr': lr,\n",
        "                            'batch_size': batch_size,\n",
        "                            'mae': mae,\n",
        "                            'mse': mse,\n",
        "                            'r2': r2,\n",
        "                            'accuracy': accuracy\n",
        "                        })\n",
        "                        # Menampilkan hasil setiap kombinasi hyperparameter\n",
        "                        print(f\"Layers: {layers}, Neurons: {neuron}, Activation: {activation.__name__}, Epochs: {epochs}, LR: {lr}, Batch Size: {batch_size}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhfJFroNbVi",
        "outputId": "9a504045-62fb-4bb8-c5df-f9296d64c4aa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 46.89788759006226\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 47.53811725264177\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 3.882094046646123\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 47.000929741828344\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 47.469198027926474\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 0.7281449113625604\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.27500809362144\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.39148537372783\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.13773247925405\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.413752996320234\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.5552548165517\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.211079439364795\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.179387903949134\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.228170958866684\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.22332409913336\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.139050658307845\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.230404165186535\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.15921928018002\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.19876811589072\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.22746963602947\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.16936410125595\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.192703482772444\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.14234735275\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21744303037328\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.116557642198764\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.18551119404055\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.13227477487881\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.20118685585697\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.12630017195648\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1086199317609\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.19379959513527\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18322462109027\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14239154284871\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.208672233966176\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17136839224154\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1293989000424\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 16.53902310633687\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 56.85719375551821\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 57.87542881250474\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 49.87841342571918\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 59.89866324755073\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 21.138153815114002\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 60.29182610998311\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.27803200773687\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 73.65305253496379\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 71.10614864497884\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 73.40076918751973\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 62.43791491536286\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.58204843838084\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.54660742824499\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.585766068513614\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.57164005123544\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.438500406098385\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.56669946078327\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.22136062203603\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.12877828495504\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.1683827274628\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.2329833238551\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24746842455157\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.186502789231156\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.2129805248908\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20228987347119\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20107182052872\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.16683826339833\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18615342143103\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.109729901003305\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16748016985582\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.13789910758089\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15897859417339\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.12385229786946\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.197922853816564\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.21760098899078\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 70.49028837776842\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 64.17496904725081\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 66.69305622143264\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 66.95264051003524\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 62.0825296203707\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 58.22116576715315\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.98948822873828\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 83.36222036707022\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.70798235777286\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.77466522014868\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.36859356108688\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.15058640844757\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 48.24060239872349\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 47.46148390803339\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 48.69022376929657\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 48.51956489782796\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 48.01130102222159\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 47.87832904305548\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.21808331163538\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.27903332970508\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.229077052287856\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.167268055302166\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.15958981616704\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.27728911573556\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.136003175837544\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.15225469357308\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.201775916756674\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.202772629879185\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20177048611721\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.16609562315135\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.20771245383896\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.223014909712624\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.204426617596305\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.19490967277119\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19660216708652\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.18795805689238\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 82.67098864331116\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 78.86334029659714\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 79.97851681974504\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 81.2711358481913\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 82.31666018416884\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 83.29063178480564\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.89687783131893\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.84599555787064\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.16538083859683\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.15675331250341\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.86370827587436\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.2074633566502\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 59.014993163878124\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 59.955147265884946\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 60.28267950242536\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 60.78128583434902\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 60.85471560786316\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 60.805229843691656\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.314824365873896\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.26800915978971\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.32386240349688\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.403035861616125\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.301794359324\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.269858692067146\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.192119669707544\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.18110116298721\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.22071402071963\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.140734562663894\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.15665648735154\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.12333088625532\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.19874236113125\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.235000030263706\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1272896171905\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.19242213534904\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.157354671490566\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.21582484191714\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.15690801845092\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.18491268121377\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.99138503372784\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.12308282563181\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.22157273404113\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.24139582228285\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.19314065178921\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1598995817453\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16438526406706\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.1634253709308\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.15988647100978\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.14607443551611\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.17542789080247\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.17439036118742\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.18986332823502\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.18096331906217\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.15642248591931\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.1610191339362\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 45.82767773852592\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 45.76608960531397\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 45.77329678355928\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 45.87081598515912\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 45.948029255925306\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 45.90815574736672\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.1764128231531\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.15326384404238\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.2137837864067\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.2291479614025\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.224378787704154\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.145623175063655\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.132107137754915\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.21155327045522\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.1992975159162\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.167432308599274\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.12195005699975\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.13804272004936\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.15956096700383\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.10209317092105\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.15779608787592\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.14487703101682\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.1622366581468\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.16214299906154\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16595719516346\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595278345949\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594481605381\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595813896083\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.1659432430582\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595363214549\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16604061880284\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16604060234125\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16604250639756\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16604195402005\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.1660364284157\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16604518231918\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 51.69156485811979\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 51.637527903103944\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 51.54861329997185\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 51.586556266986136\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 51.60645766680296\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 51.56990490856644\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.26634603483002\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.289387516834566\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.237950809617175\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.18238410236843\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.23084533159876\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.31265106496566\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.188665376003726\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.11815741891131\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.1342122097713\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.13408520644424\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.176440020526584\n",
            "Layers: 1, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.13292411643059\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -18.248233745781285\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -15.914439342924581\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -111.77304372414345\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 0.6368931696979163\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 2.6793845871832866\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 46.90082074583468\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.43924067182288\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.234553486017326\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.39046146282815\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.66205075572951\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.21013477612122\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.465931786623635\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.16294070222284\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.25039538928305\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.154882736709844\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.228161877767825\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.20924438289815\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.26853439119821\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.14685715322919\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18071586785943\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.215373286355096\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.1867820191121\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.181023279286414\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.11676178818571\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.190351551768735\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.17748913631604\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.12710872255222\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.11971322823163\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.1672272149791\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.212479861868296\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.19817673770184\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.100211494048196\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.1741100017658\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.14157824764135\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17605485083085\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.146712453344264\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 53.980434528931745\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 48.98495891974902\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 12.733123147903758\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 2.2014288688282146\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 48.66570634406602\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 46.407415854962885\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 64.75045439334588\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 72.25398667845452\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 58.698975187542345\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 68.45725918575224\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.355502235327\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 60.68859987529612\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.483318918999416\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.40641697841698\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.57010876667147\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.537285685230586\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.522309787082726\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.47143778243215\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.16691622635444\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.21083892416316\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.211389858974414\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.180818134217546\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.17538228668458\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.18475730724827\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18268901546456\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17514846951597\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.13726888459546\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.159687656054714\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.16345336721045\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.145193700298904\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.20869773676483\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.1781135617327\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15394899309486\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14444262003666\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.117558968579594\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.13639397625527\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 67.30931524211618\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 77.45022929578985\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 61.32165763951556\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 60.84083180707398\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 66.05713585669572\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 65.99452431008969\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.93976453149003\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 83.3188918435258\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.3081877221538\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.45345226036528\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 83.17958716738943\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.9803825961158\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 47.460576249325825\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 47.77020774384454\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 48.61902493276173\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 48.120164180972544\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 47.995287712204714\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 48.000420043362844\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.14948096838296\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.20828785716996\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.24484519998564\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.27017774605516\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.22164027030481\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.24989089463799\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.184738137552074\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18014888387919\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21199185709855\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.121452725479195\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.2101486838288\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.158298388370014\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.13008896639412\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.17395391376524\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.182834917352196\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.12953668944952\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.150786628150016\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.22589868436862\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 81.16556857525731\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 81.76334212829566\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 82.25794879604584\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 82.07667885221738\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 82.37189304979542\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 82.34287045401634\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.96311457677503\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.9121143588354\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.69471247990926\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.90589719885008\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.87171311519317\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.21593066948239\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 61.78846343511542\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 59.174140325487\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 61.28716052424464\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 60.42771723885897\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 60.04780513793515\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 60.353973913247614\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.36561023792602\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.32318068921172\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.31327451732707\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.386683536064\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.30939166603083\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.312392898382626\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.16450838039111\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.11850556857401\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.138448584134615\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.147177566866006\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.2401373163723\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.16044588545376\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.175993935968464\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.10914866060539\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.20905277144848\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.20262450415614\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.15494445904712\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.11598423432014\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.0999140488859\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.29683655745049\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.01028096909079\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.13604457286019\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.18883404221442\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.16627945075055\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16871324077528\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16657729198249\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.1636618433851\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.15441305718753\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.16158244561362\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.1652358733332\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.18013250667747\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.16543121469584\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.1686563596078\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.14459456764605\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.18377023096235\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.17858618248638\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 45.79581625328056\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 45.69644440338444\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 45.865212504948104\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 45.75677449298765\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 45.75419837966888\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 45.830735385212165\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.21149031526406\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.16122999028411\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.15049282863656\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.12802084605665\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.13800338794462\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.17065542809362\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.170114987383705\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.19206191656026\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.141824165737454\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.15289357261418\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.236607726476166\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20768949152536\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.15184835003032\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.16176569960243\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.1622634612604\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.14327245843242\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.16547309296041\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.14002029366269\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16595061418879\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595962416133\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595418818116\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.1659544552246\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594825469538\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16594160787439\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16603494687331\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16605362345261\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16603383114386\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16604491344667\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.1660417235579\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16603913726046\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 51.50374708319241\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 51.4010270493763\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 51.56179904663385\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 51.78611786568078\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 51.56820214121653\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 51.38033525898944\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.24207107128313\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.206916051397705\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.204850723918724\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.24432590200577\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.29265837950363\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.26170197653534\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.216747517295616\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.18644449847997\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.17655276143076\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.20674043963404\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.18616458608997\n",
            "Layers: 1, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.18510691827291\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -15.17520338171141\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -91.3611123147576\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 47.119412122213575\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 0.5355458553147452\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -15.770990683925445\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 47.18847109249922\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.34831804785296\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.34185392869973\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.2656855687962\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.05200797492602\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.483636475147804\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.16982106897033\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.19206625526956\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.22808116483801\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.15219887709985\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.16191520509183\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.231155266461585\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.18457966631241\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18574913451394\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.24187723847015\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.14553090630332\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.131234958779324\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.10672780322529\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.19345905761458\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.11815239043386\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19229508418765\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.22748125036146\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.167041903877525\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.123484483511\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.2075102549985\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.21552404589554\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15142843479132\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.191811748559175\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.16389783183352\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15408582173753\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16644996649857\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 12.256136591698773\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -6.289871709301953\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 45.85949626150011\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 14.305766836911904\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 18.018602862136767\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 22.444622638264878\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 74.03668922302501\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 62.56200384673004\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 67.19934641651875\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 79.42388276327694\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 79.2343783433473\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 58.82542992991691\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.505762464175014\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.46182078314002\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.51312410010087\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.48383259840701\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.59538463288494\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.64135117341286\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.241252740007\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.18934752747822\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.158774172454955\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.165075425460145\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.22266240562568\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.12749623356507\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.22673504946502\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.21887017944116\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.09123433917892\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.21054610930809\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18156384340462\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17775804241033\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.127621813329654\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.22430621694858\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.1211431164453\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.12016522290494\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.179719253613584\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.11998793993347\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 72.01279898461685\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 68.63757460154835\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 63.098102462762704\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 59.06556004347543\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 72.55329227557255\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 60.92471022508955\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.28378079794447\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 83.16456971152057\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.516064800062\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.7919859450852\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.16125385012258\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.85789493129863\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 48.392684106219534\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 48.19549953298501\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 48.42354413217679\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 48.174110313210846\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 48.035478231593196\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 48.28279995528454\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.27485778037277\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.210172352788305\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.248364082522954\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.26298175545536\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.17678245021109\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.28758572515259\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.187204167444015\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.198623497242\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.14731040058674\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.17741013746273\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.216703168186164\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.123414934556\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.163517412793865\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.16506431352434\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.15839116837355\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.19931208773858\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19762120043518\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.146520553352495\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 81.00484636582586\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 82.21739148197386\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 82.96030528891988\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 82.08417947327995\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 80.30951982446224\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 82.71936345658267\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.71370733108931\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.83066678239233\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.18206244060076\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.9482040507701\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.8894892203474\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.84334440363016\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 60.20089788641745\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 60.32152237954338\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 61.10470221789805\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 61.1085351861479\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 61.46572646540977\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 60.29285182887034\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.319852695357746\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.323945577979785\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.34746630985389\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.28996073844153\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.30035051536837\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.35612268389803\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.15066076601999\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.22377260267951\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.11560801019028\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.2054157801827\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.1742224691998\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.15986783082386\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.1625402109561\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.159886445838346\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.15825482879847\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.13678524417407\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18934047615887\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.20069665738497\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.12281366779195\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.19796335756618\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.15426354212553\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.16584657699065\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.16970695697535\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.14214905423134\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.15853841742839\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16825071421903\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16494786972555\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.15293469555222\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.1562879984526\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.17886928694432\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.17293148457799\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.177384278488\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.15100121324348\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.17850016340425\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.17203793253165\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.17763571817846\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 45.71663961160284\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 45.94202221611656\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 45.75064531840044\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 45.722304721604736\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 45.810203017073206\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 45.77258112852858\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.2220745947845\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.16036513682682\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.19699059827607\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.16396449768425\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.223102385073176\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.17742710204018\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.14004798973994\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.19125522080398\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.18338404366504\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.2003516595472\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.18635775990024\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14824082151107\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16590889688192\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.15909809292529\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.1621353096737\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.1604219809043\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.16094819183408\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.15514992881104\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594123108706\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594034947789\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594352473416\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595781704544\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16596270430622\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16594915825331\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16604619379193\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16602910666984\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16604312462142\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16605145235285\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16604407939319\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16604454580468\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 51.427344928127994\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 51.48678051965832\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 51.54395180515371\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 51.45094497214344\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 51.49246785105257\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 51.43583955826959\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.26073967650716\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.27314517928532\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.20670566105366\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.2264259208178\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.29772207785382\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.25830548485269\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.215315627792265\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.20823792326163\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.16122535397672\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.192005288267225\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.185543079408596\n",
            "Layers: 1, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.14752311817802\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -106.11740217658814\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -17.834966023351484\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -17.84997275838642\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -18.44431272310272\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 2.029712539913564\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 1.2137915734180327\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.385399738781786\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.38352583713993\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.431946655960445\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.64133483314931\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.365483620595924\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.53988334504544\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.21859315853804\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.24153595603788\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.24440406358329\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.16145474072739\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.259761735825506\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.225511694774966\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.186689068585785\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.12598915615037\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.208547913141075\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.124095641083706\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.198072192959685\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.157405849565656\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.200457113338075\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.14283753951528\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.12544047050279\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15026866191856\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16711510987271\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.19769155070627\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.20799372190882\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.158445090590426\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.130959814653735\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.2021345771359\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.1579162805553\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.2225445872112\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 49.3107219120987\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 47.724463960700994\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 47.41131528108844\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 18.931953918901854\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 51.16740929655157\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 45.50795828194101\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 72.83866886028075\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 69.30279652872075\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 71.23943243668735\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 68.14518641456134\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 69.79010969228932\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 69.55677738581116\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.467576329986805\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.45453949633873\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.417027607743066\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.51401851866659\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.54060546534817\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.53393426823876\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.147912164612045\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.167137191176856\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.18086133024105\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.166845487285656\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.2130297683165\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.16595569614279\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.16216382861572\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18081503142548\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.17243985524297\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.16381473346579\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.13319490531028\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.152442174866124\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.20428515829954\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.19772158533415\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14022593960555\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14970488733414\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15013936528558\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18235374054517\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 70.51945779988442\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 63.58197260872358\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 68.97695534797204\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 71.90752257686215\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 75.37352906546467\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 49.92206435574852\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 83.33161536331235\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.09382763464971\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.54956674914182\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.74000807827261\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.19097098016537\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.83073921664537\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 48.3408163766272\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 48.3543635929367\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 47.60020574859883\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 47.49111052463921\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 48.2183575575316\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 47.81394919742786\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.2565528625586\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.22002294002652\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.1848559052618\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.23988915790954\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.29021365623057\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.26182465165236\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.15602909995577\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.13988290212143\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.219123750156285\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.188754079259695\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.119335040704456\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19446106640622\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.14901919531024\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.10648120663372\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.14777188391882\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.211980830625095\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.127269209987375\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.158696624817985\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 83.09045954046918\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 82.55623045944371\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 82.94576800367958\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 81.0814342857276\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 83.00177984234013\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 79.72883619931085\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.02846994261014\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.56780953621288\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.19833719094777\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.83027059875506\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.88537879624859\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.17269175638859\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 60.945115763312515\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 59.52346956142303\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 60.619050782898356\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 60.1523147131869\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 61.03125092578108\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 59.75757946342082\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.34782843286329\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.327474263642806\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.332814864071224\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.281449162587386\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.365766084966964\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.35070279319486\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.21875034713361\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.24388929585365\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.18166589086442\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.217828728039976\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.130570823341706\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.15690653511663\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.15812293951099\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.193021226907234\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.23060250615806\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.24204569846238\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20152137521547\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.150610212278444\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.1044450956695\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.1429003242519\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.05695546555893\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.20341106571362\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.97701916476983\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.05088907994664\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16847135794771\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15743960134363\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16890559618986\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.19717188836242\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17210013079013\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.18713867851817\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.14437208387831\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.18707190302537\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.17302769337937\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.17996644772559\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.16939826207368\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.16568536597465\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 45.753820232409694\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 45.85825725529653\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 45.77163225299196\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 45.668264084081564\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 45.78728959493063\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 45.712166475321446\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.17466703622013\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.1636046598893\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.12460888368065\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.171519323778355\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.25486963382759\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.1354731284482\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.21543261528981\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.11568607618845\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.12202608459109\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19663394264817\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.124864683082485\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.188659830615464\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16630720131965\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.15777205030808\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.15561759869715\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.16906946940948\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.15206758172108\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.15634868316006\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594810471207\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595992412795\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594816324215\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16593762782966\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594059457256\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595655499083\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16604729488887\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16604283197108\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16605350273434\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16605310399824\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16603407989666\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.1660440062306\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 51.45794850864491\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 51.547793770458306\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 51.426322143917844\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 51.55746596214823\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 51.511678253959595\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 51.503966943109724\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.16869894283534\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.290586553602594\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.28194626438169\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.23459005243519\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.2641973344557\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.199735847697966\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.17258866705907\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.128058967008\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15966527835771\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.20292368133406\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.17356669461326\n",
            "Layers: 1, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.1537903672778\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -89.65417353549951\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -0.5886400107181444\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 47.699861533348255\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -13.480464948014736\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -0.36695047818613435\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -92.83168923594187\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.71002803894355\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.74493275055791\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.37189233849504\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.41056978483057\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.34479894041215\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.3525230670037\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.23413257877723\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.16396546566671\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.215265838901594\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.18919516961034\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.21679238632383\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.23256106062078\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.136567610867914\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.16746530117839\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.19495618902754\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.142633148733744\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.18926862331474\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21527009835024\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.144107695401004\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.18871054665614\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.204201329758135\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.20976450505027\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.1407106062751\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1417067752473\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.20424765773334\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.1715132806523\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.119695390428525\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.161468235612034\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.20839176471172\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.129605118595386\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -94.7360418983287\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 16.05589703119802\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -20.28196090475829\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -48.46920964907173\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 39.089833127158236\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -84.33422371762258\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 80.52325210490811\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 77.01343405415923\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.38653451197442\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 81.60122948219642\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.3451411353705\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 77.59531212072514\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.63745592771178\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.74918159549841\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.6022775889952\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.77798667639254\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.71510094807758\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.81912286885406\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.18239309286608\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.18504756892019\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.21074735641331\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.24567367473998\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.20662930185145\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.16743324420157\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.13984043110773\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.16319358752098\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.12352863216639\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.155350853566475\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.188634436774755\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.18030235037996\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.19793596151197\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14031218599184\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.17552605680897\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.20558105251532\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14098429426874\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16712081610748\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 57.1370731261575\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 58.33758511852385\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 55.11881820626034\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 55.79232737896403\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 49.059501321644085\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 59.38700667550658\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.45898313123602\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 79.05657948229474\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 79.14162630681841\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.84237245677484\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.2691362561693\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.32597497573153\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 51.16836768196696\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 51.19762222360824\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 51.651153155428545\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 52.16502041744921\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 51.69362213644069\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 51.29643289097251\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.30123783239047\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.255684267928146\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.280909664846035\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.24084202541195\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.258343722721285\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.34895952136513\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.15271281089747\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.1755141451468\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.15618341883818\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.192014781410236\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20850839092613\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.146286554990375\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18005423862444\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.15791791768608\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.178234629648465\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.14621448559143\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19089307226983\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19902174442858\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 83.42625505676386\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 81.40811739820239\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 79.25297284610984\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 80.9808115635423\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 75.55134794837358\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 71.77151076035524\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.12294860886668\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.17621269489459\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.02078101568219\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.06727381926822\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.0594128944154\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.01060479950254\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 72.11856336098865\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 71.06947371019547\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 71.76981159357626\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 71.40883472078276\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 71.91868328801233\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 71.01944824425617\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.38021700931867\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.477217190184824\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.450604363760085\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.397491884994054\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.395888523336204\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.417664350793885\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.143300679972796\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.18006653045163\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.18562896277915\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.2279278456859\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.21272348061398\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.171373186571\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.17110513913549\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.219622685430735\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.19553741807062\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.199716900599284\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.19948272178136\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15168333250713\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.1954555270038\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.93586201991529\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.07033387005993\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.05981550081324\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.2505712190898\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.11045694223161\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16401025093879\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15120082730117\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.14981995023984\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.18422426517137\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.14327299343383\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15305719350003\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.19431810850926\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.19537876847608\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.19405517314739\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.1954251060003\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.19341544314824\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.19319564993464\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 46.530081148594796\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 46.642125199952936\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 46.71782269196512\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 46.696681933764026\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 46.47214071960183\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 46.53164957828982\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.16999528551489\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.150439257479526\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.19507806887029\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.22134424616644\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.24316214307954\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.21545219787056\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.148428271381356\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.13515738170918\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16067354404287\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.182131181767915\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.146078623614784\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.184686410051135\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.15012575521934\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.16531517517535\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.15701850127296\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.16101777677021\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.15593771793671\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.15714323250873\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594213464501\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594882536354\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16784840778779\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595127996833\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16595121046387\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595250544165\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16599611034327\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.1659983344859\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16599469830535\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16599716571359\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16599801257053\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16599919780442\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 57.11802909451515\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 57.037061418113375\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 57.045513333993526\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 56.90543378764383\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 56.77120514788479\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 57.233138548994056\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.221849871489624\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.32524129577299\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.316882070738394\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.35571082550525\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.29142277240239\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.27364938387359\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20514657362256\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.170659855931405\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15308748213356\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.170685547748015\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.21218875950359\n",
            "Layers: 1, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.2059640263189\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -96.01781840108572\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -1.964902822935688\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -94.25014966650853\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 3.3193600571013726\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -106.55297457303038\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -2.738354052672398\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.53787537114884\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.35096913055986\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.45986780908928\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.1555647251761\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.50671748959863\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.50263808297845\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.218911720350484\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.193475678503816\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.22230356160763\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.22616614268591\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.18601227592781\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.19343114991025\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16679586640995\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.149342218834775\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.14707439159314\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.164738559518845\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.210819702343976\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.14591499802981\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.164172343118814\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19046852381569\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.204676354756224\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.146548019666376\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.12290042548898\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.19210397100986\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.20959398879641\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.13347800967177\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.18048992398005\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.20166101475008\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16057555914914\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1894482913794\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -0.4685654967600428\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -45.752911002507005\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -8.163875511061146\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -78.28846058444954\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -57.012762586230004\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 21.85139702708043\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.405510437548\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 77.76170082370302\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 81.81420912448135\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 81.62457815111026\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 79.2459808225416\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.94392158558053\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.56422219663504\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.57498707344215\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.67965797305473\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.65855262057279\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.63691950817195\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.85042527782126\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.227694711553255\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.233885661406305\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.13766570100336\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.23673900595634\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.159484700663825\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.217478483512366\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.19091126766938\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.14203988373963\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.16883376618189\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19846606144405\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.17507134309259\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.12912321357055\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.13264307287841\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.15163223118398\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.116875099198985\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14261689290725\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.168275626266066\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.154407015345384\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 57.49430748983777\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 32.51118989936411\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 55.28503337489905\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 55.51162772403625\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 52.77998357976952\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 59.269492724045136\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 78.92683369106473\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 79.87219621726366\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 79.80344160516736\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.51235287409885\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 78.88768218009005\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 78.92428215454491\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 50.86522896570129\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 51.897441289996024\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 52.35788453137413\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 51.649476911516814\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 52.66941351192318\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 51.51200759545751\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.19222348151099\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.23295553104425\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.26227795122042\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.23292043200602\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.22326177089609\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.26462864011418\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.164825298182976\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.14885788990988\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.188188036224275\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.141560097454004\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.15633043912474\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.204552398222205\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19542829838403\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.20329319477213\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19649939087485\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.18573415889696\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.12492065564781\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.14910363785831\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 81.07746575680737\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 77.87866621672262\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 77.1909707379972\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 80.78617057171094\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 78.15519978179756\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 81.21391016905638\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.11861962251477\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.77153996978632\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.1347932850304\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.04541875724368\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.27482796556703\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.95348159005736\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 72.17664733443354\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 71.23815065926411\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 71.32135745570905\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 72.41035103957955\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 72.19026781358525\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 71.94216781367642\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.43552349554106\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.45325813995761\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.344989522848614\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.38242322088751\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.44112472876043\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.42977546394946\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.15428691185757\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.18366480340498\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.219948573575984\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.159734605365344\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.20067364074959\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.228634467780715\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.19503398570844\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.20071587921129\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.19585721617001\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16847352364118\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.168357207991775\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.162913742413416\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.23757263815719\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.92611028849049\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.15227801926895\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 82.92746899088635\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.27043557468849\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.24689050812715\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.1600638171197\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1556158693165\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.18465424169284\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17348416115892\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17740836909871\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.16555599531635\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.18726486203128\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.19823944756112\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.20494642283298\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.19376139060995\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.19422334466897\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.19289818731907\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 46.70834483951044\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 46.66798159853053\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 46.39943931807628\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 46.508541358988495\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 46.57547880731463\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 46.6912206473264\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.19494507810452\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.202182783439085\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.16249454945703\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.2428859981555\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.18655470306269\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.22495635216796\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17788566985123\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.15003640155369\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.13164426721661\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.145120428211335\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.14720652148836\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20342360377829\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16507426541018\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.14207563191755\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.14548306232858\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.19672868038714\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.16947967008379\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.15378472957751\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16595807677263\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594820165251\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594818153278\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594713347874\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594992280235\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595051541931\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.1659957408722\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16599207176849\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16599802720305\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16600427162982\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16599801257053\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16599557991452\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 57.08664166159131\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 57.05105975813275\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 56.79568439619176\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 57.123497034861394\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 56.99556331022699\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 57.073688944683795\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.282362021983104\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.30718912149372\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.2886103495161\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.26148497914888\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.29195272264768\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.308674249525126\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.17792828656672\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.20545364801579\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.19865106952731\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.15186540512938\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.17711253654381\n",
            "Layers: 1, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.17028899902748\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -2.029382157792159\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -1.6705914352147744\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -93.36914567321845\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -105.51501386047644\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 48.078163573693445\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 48.4308297514321\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.42987504168791\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.37703059928076\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.318818712056164\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.425011213021484\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.42825478401663\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.29706984475443\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.25618773093787\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.15018132997167\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.21437358540188\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.21285733138042\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.14938637881465\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.172963560554216\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.206450709710865\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.167627187457725\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.17813290279833\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.182642820650244\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.14935137509677\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.1760578653366\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18570263160583\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16879603004502\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.18005919583088\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14961695137794\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.20899785626115\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.19533719395198\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17272329664177\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.21413781672665\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.209892242953984\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.20946173843298\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.1268542665198\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16930987340576\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -44.05937460861673\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -47.96277452804689\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 8.979069468707113\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -92.78723538767848\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -46.237142121695804\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -42.204069149279256\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 81.32465142512348\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 78.37088778232038\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 81.68389200386893\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.90425084841677\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 81.73190600304116\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 78.97951890647663\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.67346463197829\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.73339092754911\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.64376185263397\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.81391041188873\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.614745629487665\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.635805608019496\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.23819393381879\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.18327021612498\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.16395426324401\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.23706638736855\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.164230157934504\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.23068084442608\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.177873367487265\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.14535830028592\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.15580892522498\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.21891797032874\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18565907898975\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.13370731455393\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.220695273714206\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14525523338514\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14865475769273\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.157113129154816\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16170779977238\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.212319503996646\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 58.563159041629696\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 51.06709478822393\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 63.91482748288417\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 36.46802453735024\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 50.02645470970748\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 48.71348995242395\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.73130687279038\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.18695765086687\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.1786941810728\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.97097392899806\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.64266305871517\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.56391976681714\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 51.52172162871641\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 51.265520654421536\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 51.0686496368852\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 51.88981554679226\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 52.04769222065092\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 51.68611228654656\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.26224105579241\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.24307002927173\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.26827566879385\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.230408737855335\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.19219012512662\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.223776241878745\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.153797608251864\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.1726430601181\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.1536584435989\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.142313716951\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20605442474629\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.201109021892236\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.20069066721605\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.143375734340985\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.1767964471606\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.191279358727456\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.165417450003254\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17713690952887\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 78.10149250242688\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 81.08983753191177\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 80.9424202790606\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 80.02185757150127\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 82.06892529346041\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 81.87168896129683\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.8032882889432\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.13754109207771\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.1676530536217\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.21323923923798\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.9074618301494\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.95328982954608\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 72.01284640540462\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 71.31592304211811\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 73.19889084196292\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 72.28450327437181\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 72.14598344576784\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 71.99745426518184\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.41409539409613\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.42178110087003\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.349789862908594\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.44692350087789\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.46092453002244\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.33700965156283\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.22848823382496\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.21529424183234\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.18229004749789\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21462383402828\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.18533109755144\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.13859529609785\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18551841077774\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.146788661591465\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.15466728228857\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18803024277989\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.13707378719861\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.209540566840325\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.07690409348301\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.88658641424497\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.21967706947342\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.16277868682779\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.03073540655735\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.21059464768152\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.17197185665609\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15524853002901\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16916290534883\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.13657839727165\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17186040625884\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.17837039675344\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.1922562222029\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.19723333030265\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.18967212513118\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.197998680463\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.19417342766519\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.19371937882364\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 46.68615519205957\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 46.50333865325273\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 46.52626847653244\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 46.57439634575439\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 46.70741785939255\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 46.35279577949928\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.15250500550584\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.189691786029115\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.23037432638287\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.1511115731838\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.20993089207992\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.21536532030692\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.146186508925645\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.185230935555445\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.1840588481083\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.122710924265505\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.191317913417194\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14770975600789\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.15249983359901\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.15308924237111\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.15325448373108\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.14606137233626\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.15963512094349\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.14486491529247\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16595629526364\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594534648256\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594903753504\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.1659395776126\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594471362619\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595274322006\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16599619448026\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16599823571642\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16599876248704\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16599483548521\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16599735410725\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16599743275704\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 57.05631132550197\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 56.95507390178114\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 57.03384051331626\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 57.13848087675141\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 57.024061115116375\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 56.99907934203841\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.21474294533784\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.240741368014504\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.276625339137574\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.25451566682829\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.27200945779127\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.29973047307833\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20077101036758\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.168443722894224\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.17998448001783\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.14800322181708\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.14557643468435\n",
            "Layers: 1, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19247368573971\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 1.738526503793758\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -14.781668593154617\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -0.6053116891317378\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -94.08625653067173\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -2.676918193394995\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -14.600800388290104\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.49672142714319\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.34139818863069\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.000915757148576\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.57500530964555\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.71403341145638\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.542092578786395\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.186781046903505\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.220237152803136\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.24152801502503\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.174669679147115\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.195276828189456\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.1567868475231\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.20233464896347\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.163148792073315\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.148105014117\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.191124760187485\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.175794754573865\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.139214089190304\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18187954370969\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.12802035227349\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.20986198549101\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16683203840181\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.15551503560865\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.161506991415834\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.144561680793295\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.122697683023375\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.205271407978984\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.20605123207104\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.157882454531794\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.185360809681654\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -59.71600658005573\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -44.780872679323735\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -49.14070702623399\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 37.87713899894103\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -72.80270852025643\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 3.7056519109991526\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.19717911024134\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 81.82870296760679\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 77.30708263274305\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.64691274770612\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 76.83443225294317\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 80.0614634811261\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 45.64053687665327\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.566796686458865\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.80000997098369\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.63735381363222\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 45.73665871193092\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.67636768691925\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.19294522093244\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.24369712434581\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.234517712656185\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.20788236327918\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.212149285830314\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.15313000711897\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.13614733774168\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.145292702132046\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.185791866900814\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19250453926343\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.13413043673417\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.15897380421742\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.11682998176091\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14267168849804\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16019142891048\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.1567815019423\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.19357268263754\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16379424033592\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 31.03074046721347\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 50.45572292589803\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 47.24778585704662\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 36.237400239301365\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 39.75176125319012\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 47.85875323361074\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 78.97494639479707\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 79.25754578624779\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 79.44406098957303\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.91655399482003\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.7915772693021\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.86725096278234\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 52.002916222143135\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 50.824601853264205\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 51.17293033716\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 51.76650309109989\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 52.75093944892602\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 51.64695760763189\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.289000476169996\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.24147226877431\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.234575231038356\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.30680296148806\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.28425032990986\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.26848341121492\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20219851503313\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.203163113543646\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.19524489216846\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20171659061914\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.16174681165328\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.175488033010595\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.20476392562826\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.19670556061824\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.178696571038\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.15253563166468\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.1877281901194\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.20290215190686\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 78.28457841481749\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 81.73179829856835\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 82.81017749758574\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 81.44872478109052\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 81.56992448075869\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 72.71382884774394\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.14790259662331\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.16559890608794\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.12054268647118\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.10817720517831\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.12494129068237\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.9937267733297\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 71.6165352745627\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 72.87411808396007\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 71.51613361816274\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 72.59403662522452\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 71.34752257989658\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 71.32874959814718\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.38742608180335\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.52551981860484\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.402900409832235\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.4280198711829\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.39699238569608\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.485413253656006\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.178849621626604\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.223697541795296\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.23074306418321\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.22637955567185\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.19237976290834\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.19301817148686\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.1295108057267\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.15607615643459\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.12534228156741\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.139224373017086\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.14538800514274\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.136348014028684\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.25172607954498\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.11849736051629\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.9500290439373\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.007129073646\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.12514160618638\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.22442657974062\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16731784368412\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15142329826544\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.17775372028534\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17808670516575\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.18010825693814\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15008091752813\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.19472072222337\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.19407105857401\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.19218206277606\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.1995898203965\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.18945198439528\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.19493748833496\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 46.65051360047659\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 46.60516804697919\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 46.60221528586273\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 46.40444197843804\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 46.554560415306725\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 46.47906473980331\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.23727390468606\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.17991446185818\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.22569972267614\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.20718800197997\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.234361381756194\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.19835021986266\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.192411432237776\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.213255725420886\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.19979641798886\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16924602755476\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.173284325557205\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.172585155236156\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.15167800192181\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.14740157923023\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.15484963297433\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.16324855524962\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.14153273991803\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.15337585680412\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594298150196\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595648548638\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.11402559829115\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595619283602\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594119999297\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595178479018\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.1659983454603\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16600142194707\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16599626581377\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16599665540454\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16599790282663\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16600170728114\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 56.966962372154676\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 56.93095101037151\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 57.14917517492586\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 56.94341233705349\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 56.81841722743513\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 56.85483486180476\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.29503094461451\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.25872760614874\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.32813180906046\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.29209576745771\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.26372535011328\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.25528160806947\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.18390110607777\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.16312400302604\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.18784560581368\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.15763812950277\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.14826313554984\n",
            "Layers: 1, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19136043962436\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -2.2317250711413994\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 49.56934807992869\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -3.700131681352259\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -11.905070857431044\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -13.476140350960764\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -12.337894621231937\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.866390139230326\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.73440153984832\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.92141597634264\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.816900938532115\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.454057117143165\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.56529200941289\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.217820540196286\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.18830575493329\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.26501242093539\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.248933334137895\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.19190224542382\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.200295956007984\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.14375750116195\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.20824163262978\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.18812997344528\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17444358698489\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.15951682891344\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.17576383153855\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18348125660864\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19201006944852\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.17291229154209\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.19165127989338\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.15638405972635\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.194720939233314\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.1730070601953\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15446421896412\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.167681986751305\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.15894681755789\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.196729914434755\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15901006933974\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -24.52796929176162\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -82.48979492803896\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -92.32997508720277\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -57.27079503738335\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -76.11719720977268\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -79.82463124066328\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.6717147563763\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.12186111432636\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 77.02107141792523\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.79461031029587\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.26795020028463\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 70.93845348409367\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.03820308019798\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 46.10206045721929\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.04392413025644\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.18251614041103\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.237292682354045\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.12899072585455\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.21496956747513\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.240970558444374\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.22036129548377\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.217107219198574\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.1710942625645\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.222901058946654\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.187976234087\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.179743995085516\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.15438905932112\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.168602055018695\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.200622164346726\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.19529279182824\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.160455435373706\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18282190684728\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15014282579555\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.19537658117336\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16401261970755\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18864138669178\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 27.797882641966797\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 34.79601110129874\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -124.11425539301324\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 44.67918319175378\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 27.206547661764123\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 25.480190929344072\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.3692042384058\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.58020761399878\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.3146370491581\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.50994944014585\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.10199727772478\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.98929623054005\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 56.76040603218748\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 57.49748083678159\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 56.80433222558885\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 57.13157112885583\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.42815729274939\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 56.693854318709505\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.295158951544344\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.29592039983107\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.31044885295751\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.29996155847991\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.30926828288716\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.34381444026878\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20417565975365\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.1847075735332\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.18199874659751\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.226690210973594\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.16283631369154\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.196780563447525\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1780378757817\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.14004506596222\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.1589144833063\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17162084109162\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.16523441279969\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17591716631719\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 63.74812125245135\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.69291826057214\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 78.84894317989838\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 60.57195420918173\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.74807902995681\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 70.29310199553179\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.07719281499853\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.89702684247361\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.23891737381526\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.59519444236273\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.02045123989572\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.89799031326037\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.1273244895101\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.39296953752209\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.38392246789205\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.36322453802099\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.27681670647999\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.38041063446992\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.54360933214608\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.63461206068893\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.61703165295764\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.525571617487515\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.527473513221295\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.60113962346743\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.22620783035622\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.235915885052904\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.179806540510825\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.2046473536499\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.16331328819483\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.16630360342228\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18687159793632\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.14487019454968\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17800642860147\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.173447800680165\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18047324473936\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.144109664103325\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.17701527456876\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.63059986657369\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.55866805392296\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 82.94574676823882\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.92472041151649\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.09748713431524\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.18266338162556\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16090789022326\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16184300684957\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.15741333414597\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.1703028113713\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15702156678533\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.15693177525732\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.15381307093449\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.15523595886758\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.15653820443895\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.15409237276698\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.1532949719065\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.32131024705847\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.33886330314967\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.10193695090811\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 48.417266282339774\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.0354707463747\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 48.52415369872873\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.16955195583002\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.22918712523507\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.1944396092396\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.21384137481491\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.244282479703394\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.18167446415134\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19231724703923\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.19774819916813\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.18920430368778\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.17109531132858\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.12720184613235\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20477598891702\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16588617258252\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.15793590889518\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.1806317663394\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.14737747581607\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.13906115775919\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.16555706897731\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594142862604\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594302905763\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595529659435\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.13671637916637\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16595237740714\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16594618053604\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16592116807675\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16592916657652\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16591536079643\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16593012866454\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16592328613363\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16591881407052\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 65.81276679866652\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 65.54179167125615\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 65.6243901279954\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 65.80065409445332\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 65.80952464779722\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 65.90478256944957\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.36940833561531\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.35297697676488\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.324711586010416\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.388708718429704\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.40211166281969\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.395092291540976\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.185368637521115\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19514558285598\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.191345084308544\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.13422629776678\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.171879399850056\n",
            "Layers: 1, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.15372591951986\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -3.456004311126626\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -95.4607798863884\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -99.3227862346936\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -11.910984025039939\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -3.6976052432577955\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -10.11809960104837\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.825332909147996\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.36422723399115\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.60211084382969\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.64203920117913\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.768320650879716\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.65448909466369\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.204728528593584\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.194911204134144\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.20091743707888\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.220858021119504\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.18761232416536\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.216216689599435\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.19628192813611\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.2053035789044\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.162375381701004\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.20421886949156\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.156089145540875\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.185457343526046\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.163890760492684\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.14372023425162\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.15133623893279\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.142705679732174\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16052414239986\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18956434450712\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.1361816188004\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.17619081069281\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.18827599456865\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18626548856155\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.192188399204056\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15582067072806\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -25.651619097808776\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -78.21660434875078\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 8.92778256500092\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -156.30206524662742\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -83.86105147915231\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -17.5361687770327\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.07352752451426\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 73.94362447085305\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 77.29267001837938\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.90682058915993\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.93757706946873\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 72.8340144362265\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.118130115174125\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 46.148856911403854\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.13529396393792\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.32411201752073\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.188208131919104\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.234795406066375\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.20121544518083\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.23060572117862\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.194972519565255\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.20739325146985\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.192452973818696\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.238063331274724\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.16165150626694\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.13637809006417\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.18677508689986\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.16409065088205\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.15088705732878\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17419368533062\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.203391300563645\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.19867116029867\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.18742016237623\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.15141000028802\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15268775598387\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.17895926882509\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 40.92148552738391\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -17.31622774915582\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -34.626925694883035\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -39.40510477286261\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 13.727222229723823\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 20.704783669000935\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.42687132864287\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.4712330734405\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.34088129497918\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.4705120872436\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.22529877434758\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.798182074485\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 57.403003204403866\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 57.88601180317762\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 57.1616431579171\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 57.64705655661222\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.939300802952495\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 56.84827693106802\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.3535749993741\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.338819327185135\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.27962657464379\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.308867438800284\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.350944792825366\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.35138789032085\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.1619324664502\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.21200155899341\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.186878006155474\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.17990959276301\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.15321489549261\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.17629619526324\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.202019443491835\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.16309873762175\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.17132844017318\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.19763145188536\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.151101193942\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15666139606096\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 75.64494443936599\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 44.19239600225111\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 65.81533225779795\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 75.58519013686156\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 73.3246075663477\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 52.56923927216408\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.50137104858236\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.22634270056757\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.05203373787913\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.92305074983692\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.0495124197939\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.94802250146958\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.4319537279475\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.30037338760195\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.3799836320076\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.27856687261173\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.33790956606205\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.12785175398112\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.67458446610234\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.61391294648421\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.76730186126105\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.58267361021129\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.642869903276974\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.59154594960824\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.18033983461319\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.2390229692279\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.233829520534876\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.18975402969135\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.18332725743029\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.21246031390401\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18524194619964\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.15462784740879\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.133476350935176\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.186576089203015\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.161781118970865\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19607472784695\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.64968575396023\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.53572368073107\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.99867327111774\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.19061239423266\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.9043941987992\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 82.91363172161657\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.1546752316585\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1774192556916\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.18988255719184\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.15679386378157\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17174785659375\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.14544927401701\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.15828416646556\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.14889497760616\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.16121079064806\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.15201804241916\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.15358225943534\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.15221378434697\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.042824106068736\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.11881322918104\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.27275220403325\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 48.217486638903026\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.37385206174293\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 48.50050447433672\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.20052716867248\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.23893228798496\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.22255845122842\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.18993752345865\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.18895517161425\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.1852409660639\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19333570770488\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.150476320867114\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.178443133687416\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.13354702144261\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.18023388737318\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.149841422996026\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16603229655861\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.19027046888415\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16594145057482\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.16606246880933\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 77.89519482295451\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.24136142123464\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.89073096869043\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595743294187\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16593469035182\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.1661440963072\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16595498199521\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595537341504\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.1659242921192\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16591798733329\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16593013963893\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16592420981128\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16592727532365\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16591778430713\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 65.6738865632731\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 65.80159298318256\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 66.06013600722007\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 65.85260621926132\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 65.84817655684621\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 65.8377746228839\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.37043762491537\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.360154740524514\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.384061764475604\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.32229122326012\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.381223335654965\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.333380482531446\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.21007901020091\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.17152585191199\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.157355967152455\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.14687608400918\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.20612638107904\n",
            "Layers: 1, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.14959544218479\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 48.849882499258044\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 49.44348204899529\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 48.11428429525293\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 48.54717110691282\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 49.45001199961532\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 48.66434453180296\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.74936016485249\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.75791366841311\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.76051437802181\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.677553102785616\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.41247679314304\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.77736456703079\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.19433275819872\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.20483399009011\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.2459688247962\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.181661636397486\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.20152859338339\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.21336407343867\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16476812342872\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.15399048117029\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20219926157553\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.2085425583542\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.15888745332438\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.171485304593816\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.162022619845445\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.1735306049811\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.15925033377912\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.13603155663731\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.195354321034856\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.178120339633686\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.145205647125906\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.169835006748514\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14405221844522\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.20134370768672\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.182432375753976\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.19537103487058\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -85.64258779197257\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -72.82650296789595\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -85.85846131635341\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 21.54158599906192\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 3.289893190203197\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -24.582864921761516\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.81761765050211\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 74.46948228871909\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.4759478376979\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.07193156462955\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.36011900936546\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 73.12478242178369\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.058940377742296\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.972043275918864\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.17682524419833\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.26358250491136\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.33447020508614\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.25149634014064\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.22973255642647\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.1774771760872\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.21059696182843\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.23429032122037\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.19001413842446\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.19934302201622\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.13855076276015\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.201390704674495\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.18795999808728\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.167533437709636\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.150716347687045\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.19069563911097\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.1661654262606\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17210658828471\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15090237669204\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.152894623704775\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.169124072228286\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.188233963094895\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 12.284765155735167\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -112.94014466874299\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 2.8599811295325295\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 5.528187868830159\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -122.13890343367208\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 14.584104271286236\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.61404625702782\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.28719582537558\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.67526009972634\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.94299892927432\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.39492508274044\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.95574149602575\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 58.16012422986812\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 56.492220609808584\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 56.94729902664728\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 58.087765473170805\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 56.81552759659989\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 57.09821147917786\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.315480419553566\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.32760322740567\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.26668579142582\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.30023495414829\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.275902175724994\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.315643173785716\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18050439700436\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.17554557877248\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20054335289038\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.16224127377148\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.19713206972432\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.16329080055746\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19044611594448\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.15672900280292\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.156909636874495\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.15617931726427\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.14568983447157\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.20256672294951\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 72.8586099227361\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 73.18371536999678\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 46.013225574442195\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 55.18624165618744\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 77.69163441191608\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 69.0706484684507\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.9228689581853\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.15989706037965\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.13381324017418\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.09059327436124\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.10329991005186\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.99379902412757\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.40895256668752\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.40306101039218\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.32861045132115\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.33595594940947\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.2459223556665\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.2257161259331\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.6310411245431\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.66931048378857\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.64735247410253\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.58813641337219\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.65565196509317\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.622826196970635\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.22045997018941\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.187027411243164\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.17415900924663\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.22209292640649\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.19255624444941\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.173583196588275\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.2126106049967\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.12357850710526\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.148334821427504\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.17820556386219\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.17431769321419\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15229251416844\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.20318579810947\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.98012831131526\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.6653017666681\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 82.99813401164064\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.16981867624509\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.02048503552354\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.17635087707191\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16537978505558\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.17114563705275\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.1735514423881\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17975227522768\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.06540206423381\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.15274447099797\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.15532562693372\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.15879653137581\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.15111788013313\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.1655968063217\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.14783790626423\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.23946691557942\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.23298164057832\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.36679851606998\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 48.25915358697701\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.162750711369476\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 48.1399165482575\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.22314585015852\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.24838229899959\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.2249476868955\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.21674579265912\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.2386062706013\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.24988263303155\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.15371499904284\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.196763160358074\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.123030142058404\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.148167028468734\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.16301721317667\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.18670443428676\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.13939113199916\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 77.63726130179349\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16592048949376\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.1658806085678\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.15016292547146\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.16596491381635\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594807727611\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16627264662999\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595540633823\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595177381579\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16585256534836\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595614893846\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16593121878708\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16591975055164\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.1659242610251\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.165931244394\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.1659206687421\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.1659311968383\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 65.81857463844352\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 66.14903290540633\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 66.03218271899689\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 65.95418077160858\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 66.00618887699834\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 65.96217797949802\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.43322810435208\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.341593072241324\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.31294506797685\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.39603232700088\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.364244673701194\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.37252513762159\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.195521417223596\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19424242442628\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15787336722769\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.217569534794364\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.172995092936\n",
            "Layers: 1, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.228407197936626\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -103.53550300133563\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -199.72415930702624\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -12.89895956114786\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -12.117630472391738\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 49.76165457068159\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 48.810915577946645\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.461235239124406\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.15530459388468\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.538741945334436\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.64757153720498\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.75430815378551\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.63835130056339\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.205418942899854\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.20222433571932\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.24275598007975\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.18204957916425\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.18012528922901\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.181411345052425\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.20079168905819\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19293856698363\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.170027892595414\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17613785552867\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.16613877427703\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.198928112039816\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.184067646759566\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.15630896085331\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.14098880042317\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.1498036381996\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.146670381186325\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1725328473823\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17038363297651\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18456306047008\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16945185232301\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.1384107819335\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15243531012922\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18374330693542\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 23.315432387199454\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -80.21257963293029\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -84.3218411935624\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -162.3174484461077\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -31.114374524933375\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -149.03195810628938\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 73.59161959327314\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 74.23826910866876\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.52521883705177\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 73.4826597163263\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.70523722628135\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 71.15124694039916\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.078974836269104\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.88038760512827\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.304862512897884\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.1644743336061\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.14792135663082\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.825350095239635\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.22703734498741\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.23644164228487\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.22083158746625\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.20663562112519\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.22411916298148\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.190658734451894\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.15325332103325\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20074591458939\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.15122285698253\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.193168227885906\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.15298159268235\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.14161219166811\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.135421543934044\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16205790605915\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.17713659355438\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.19295070184259\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14811756288669\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.21036801110964\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -123.69743889302734\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -37.977754333305526\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 57.17124095217314\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -45.85639950138059\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 25.487714022295116\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -55.94604559039815\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.92175810374562\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.50267059317392\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 79.65337434672834\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.20688528771322\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.61981861098405\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.89285447033237\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 57.273552601576114\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 57.10347047209237\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 57.15508853950215\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 56.81996035436401\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 56.49789212179404\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 56.87690593956711\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.310753878180286\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.34300596485758\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.315221438079355\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.34183102676671\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.35501726062437\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.27918842184244\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.15949959086135\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18992712440796\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21199927245537\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.15818921680514\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.211834733477666\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.161527319837006\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18604844247712\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.14808773188514\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.147305961993375\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.144679623241224\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19811541280746\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15189316339496\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 70.54998030550964\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 78.8742807956779\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 68.24579577268564\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 51.99471999732845\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 72.38074901143298\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 58.48255683692607\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.95887314683411\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.14946217010156\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.15549861438255\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.77811013833745\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.07373929673061\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.23711699071312\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.29335840515675\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.19042565477824\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.4554248929801\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.27879209997647\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.2984170310105\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.2443329862676\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.61392849727773\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.597717926280566\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.586117956748765\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.67266329739987\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.53105447059808\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.580885696843254\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.18811605748905\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.19186699368994\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.18814905474443\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.216269501498154\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.1747988378054\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.191177695398025\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.17192356322728\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.173399261980215\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.18857326482162\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.151452659984734\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.166756572411906\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.1383178334113\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 83.20228335223621\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.97896722106583\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.69908798943545\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.15544203043808\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 83.02683556633157\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.17209297914728\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.1797907038763\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15016715792709\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.12927487896098\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17579481946493\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.16228029597158\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15894902781321\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.16175699416914\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.15761412703584\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.14576530346798\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.16315834212222\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.15838017955703\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.15483254951035\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.446598979314025\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.332366838416796\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.26599394336005\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 48.281792353339966\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.22555446117762\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 48.09604288502173\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.26920794951257\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.214910708785084\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.198022141507145\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.27960209822548\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.22031486984649\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.23008062860489\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.18574633201534\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17501544742658\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16010393598001\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.1379298201416\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.14496458190989\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.13681535792687\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.1660027608224\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.16995013843884\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16590061853523\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.16555175371539\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.16590104287825\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.24002471882858\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16608453464545\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16594808459237\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.1641306161972\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594961551948\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594834066142\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.1640084584554\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16592353488643\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.1659208498195\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16591849215514\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16593018353649\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16592080409289\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.1659196280043\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 65.8154922758065\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 65.73275673101358\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 65.7768517654885\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 65.7788863264833\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 66.06213627805371\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 66.0311433030446\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.40938952273257\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.36411007879124\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.38553361212039\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.35444272821394\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.35932822834942\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.38470974317057\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.141282284665905\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19074679659926\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.174702483007486\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.155852384001484\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.15231367540704\n",
            "Layers: 1, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.181902885554194\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -291.6191695520235\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -7.8705023146242725\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -198.9852504397335\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -583.917548823823\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -5.751228369100825\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -97.82029329518681\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.993491831436714\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.31328676684999\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.599465201161216\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.12764019543383\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.04194046124262\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.3242984423246\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.23447835868304\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.247323791709725\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.248324136243646\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.230694103980305\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.20182458920162\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.252915752232134\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.19500884679005\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.161949064515426\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.16939557385018\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.15818752470242\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.172962200760736\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18025430923919\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.15852050082333\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.165809136942194\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.1717382734645\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.17531582417797\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.156128982660384\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18199357732497\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15520422878927\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.17328813255526\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.15397240693913\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17494396558786\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15186616571103\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16602646756517\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -174.9651634880625\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -420.29866875749826\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -475.08628259548334\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -170.622908962061\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -174.06003350629908\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -301.4576239982784\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 74.37956787682968\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.73111428645637\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 73.7763799412021\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.63221818274998\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.23009787942881\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 72.8732553678863\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.412905039849484\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.49052259300831\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 47.552922889133\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.429916716349\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.822117580334535\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.02849098847408\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.251591799028375\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.260398709231296\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.2524782022612\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.2408425766356\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.233891488602694\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.267039635184744\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.15403109649786\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.16605903778373\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.17223917969279\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.1923774194371\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.174782542809396\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16862570411033\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.1558840639552\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18679076101414\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14782441929118\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.142523388338674\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.151759691727186\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.171475569330156\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 27.193997016024397\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -86.1838038835573\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -25.160682204129103\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -226.74877895081593\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -99.50562660201187\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -337.41410042894813\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.67545452125236\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.11673337360612\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.42831905893225\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.84257688474783\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 82.24543262726053\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.67085174952699\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 67.64149928166118\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 66.47564729603488\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 65.79169464936695\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 68.80022394700576\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 68.27411084063995\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 66.90176913186787\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.4229888621479\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.42286440925862\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.410187874299424\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.38837155811617\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.41546526447504\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.40106044452041\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.2018688853843\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18577893159027\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.15707076615416\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.19123325595158\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.149947862313375\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.17529007059784\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19593239841375\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18773866246843\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16095828774732\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.18709512211587\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.189233555104465\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.20011373784138\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 55.18748639082149\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 38.07153565699816\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 33.50526014780879\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -12.592029172797115\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 0.19040511035809393\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 31.081684183516533\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.01091256523937\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.88320066922462\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.3021528034042\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.87851710039672\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.99606510538409\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.05987864012548\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.65426434968045\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.57615185716757\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.73488602054772\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.49007645492861\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.65204444533342\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.70407090915955\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.120866272337004\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.08657633027653\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.176142302117775\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.90997181202621\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.15287501040574\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.026946761837216\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.19977750966715\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.19291139605745\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.19332955059452\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.18579014807034\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.18850422660078\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.18755207816193\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.15679653115052\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17161545602249\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.19384104841737\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.15996652546702\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16477081908601\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15603909207173\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.84534466600437\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.96872049901602\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 80.76740852022336\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 81.44240121435378\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 81.66074015328743\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 82.99224165787311\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.07412373960177\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15456452386152\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.12462771583618\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17324600687968\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.1745812716409\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.19236212138888\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.116067854161\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.11443999666888\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.11162185888433\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.1203334615932\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.11256673905783\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.11561656629257\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 51.39251901539157\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 51.4174648552367\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 51.75354048981262\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 51.78952298461134\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 51.30470910301417\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 51.66371101820015\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.289379666303255\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.228451266289106\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.22782756330743\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.28060851021344\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.234677587833374\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.22671440766648\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.163043964508454\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16341360321423\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.20329025079967\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16396221449443\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.16258036553265\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.17653216662493\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.17705597052594\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.164819524258\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16625404870048\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 65.70168463352498\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 63.28264074806606\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.16199845722639\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16324833210373\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16571490994166\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595105682244\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595236460368\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16185864901058\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.17515874210793\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16594139570287\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.1659443661039\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16594549646585\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.1659342367438\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16592779294896\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16593306980053\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 77.992314807199\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 77.93544228707047\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 77.96854505238974\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 78.00546563215808\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 77.92603747685567\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 77.9232314156992\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.56720422323626\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.556900606222385\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.54546711867082\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.5986915760551\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.580615673498556\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.52485502795483\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.19667909795135\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.18044241337162\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.19899543751084\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.191404769117796\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.179006368422684\n",
            "Layers: 1, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.18509026527502\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -196.6029380590558\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -97.85851538021262\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -9.081124138182762\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -99.74296953836344\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -292.79005761708066\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -95.99140546950152\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.03359203675629\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.68467159940221\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.76467324111441\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.16040037514082\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.067859246658095\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.078099321829754\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.24987984086074\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.22109052940123\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.253120401447646\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.25074026381256\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.24725100472441\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.25164956630625\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16516020311108\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.183327093746875\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.17298056079932\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.185504244596885\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.174247432102874\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.1964393048431\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.15408907953391\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16940700294383\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.13752151694681\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.17388002767638\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.14669239389563\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.162962012695\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16173025118748\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.174344610458576\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14610106587027\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17077782019548\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16381829634979\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.17103655400248\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -346.4199332135547\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -479.01232678724705\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -175.5502924491127\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -183.3310927364348\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -269.2153345509695\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -171.9809896222558\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 79.89109849216474\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 81.15099430632948\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 78.73806795764801\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 78.64645307897196\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.28078340999367\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 79.31424388585532\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.40355647291587\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.57005201253253\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.70512130720478\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.761363891015165\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.44170523994171\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 48.171567201934465\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.245534719825706\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.279800196931674\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.25640556468022\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.23677159859189\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.229945831403775\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.24845348187376\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.17708248256606\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.187779635217396\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.18692965343567\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.178896098920674\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.16706616200841\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.15726199708872\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.153160209052686\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16028161911297\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.19964879758421\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.153893923135435\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.18632591979619\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.186034993911214\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -51.490197790780186\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -220.3665047584478\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -146.30809420317493\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -150.87317812794873\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 8.340719068100677\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -75.22312733877376\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.26533074629549\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.68025109701153\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.76441227744995\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.91437075592614\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.94817912574011\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.44751151274757\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 67.03250267151721\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 67.95870487105586\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 66.77248673497098\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 67.15549950038148\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 67.78184845980712\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 65.43799173296938\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.37931867994119\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.34853450863222\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.392550970504495\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.36516273716457\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.342269298325085\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.39845869593868\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.195544350559715\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.189938286375096\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21214793889208\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.185894707605776\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.198406677176564\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.18502101533192\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19268231433815\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.15537206365576\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.17141880606857\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17693945185625\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.171187824957606\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.154435842129715\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 30.004245515156857\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 64.0238178027833\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 42.0562849897144\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 64.44949288225558\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -30.494405635252807\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -3.5876285083274784\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.14519792212167\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.99591352713716\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.16613570821664\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.1327493647117\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.04000547209324\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.02287979010745\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.77636722536164\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.75267090715573\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.77135544825569\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.691459027478\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.71046452015626\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.73803920273951\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.08116195551797\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.07587995920023\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.17245294459488\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.02224679058614\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.97752613364495\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.94619799481071\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.219202516028915\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.18218947843855\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.226268682065864\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.224759290073436\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.21664520920303\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.2390738562111\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.172817031779665\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.184539065431586\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.14155452221483\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16003261479264\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16867961346214\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.164983748111986\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.54933124947557\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.68795520444459\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.48964387244725\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 81.18097549660891\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.97632567124391\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 83.00701551799533\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16423411382128\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1424142265393\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16348717045318\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.16736740036215\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.13281886825445\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.13955371389912\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.11404535767673\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.11325633850507\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.11426941901274\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.11796870321253\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.11668629842012\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.11326746287637\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 51.69703994785111\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 51.680268013687666\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 51.650883341297416\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 51.51028735173768\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 51.52751338106945\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 51.714848756881615\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.227761627083815\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.25565227016373\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.28354653187302\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.22630392588778\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.235500120028064\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.26230511571004\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.16045603996888\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16856641244639\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.20760631039424\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.1414347682927\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.1713485104408\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.171165320115605\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16007619257122\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.06790452100418\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16379275685945\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.25017656546514\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 67.90360292347629\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: -170.0458407136824\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.14083101743019\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16598619315467\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16188522532016\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595756463452\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16309043626745\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16597685577955\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16594005316942\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16592842946346\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16593805217268\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16593024938281\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16594771146318\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.1659536211711\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 77.96628757459614\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 77.93462962998163\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 77.92044134171108\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 78.04481610421435\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 78.01389107449923\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 77.96546809189522\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.53871526348211\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.536516791200285\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.563815202311076\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.63156035547632\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.465416506527255\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.56106901154248\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.1707975673809\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19179125082494\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.18674139935833\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.19727326948503\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.21550447409626\n",
            "Layers: 1, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.21173564893293\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 51.167240117215364\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -195.44077704956032\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -102.51132244665047\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -100.67044768289537\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -479.2092460464782\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 51.542731626398684\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.10711606866013\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.39119771811118\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.11659100544283\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.88815906631293\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.11749197291742\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.08510773384599\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.23813067283445\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.265376509278354\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.23931083085796\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.21512832463118\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.268828627429436\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.25701153222479\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.1429364948591\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.172827143202795\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.175584808674664\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.155668361675126\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.1685352443039\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.163845688377556\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.165236086663576\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.17567379359141\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.189951977977984\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14160120226357\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.18543241598966\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.19719516135049\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.192581507065455\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18976422678001\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16929244735118\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.1872113860692\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.178273825802265\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16570975425635\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -477.7998761614941\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -339.36261989807866\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -475.36023192630677\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -173.41066778232005\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -425.74317341341026\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -318.1980330776245\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 72.42566413425786\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 77.07148838372444\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 81.30767038694322\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 80.76067338467193\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 72.00018640949116\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 78.02875713045496\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.18127573055465\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.79218868398557\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 47.5592774068733\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.73643445539369\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.581877488147086\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.45348575040212\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.25139615723115\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.22424770251923\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.23208425046627\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.22841190918566\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24390142536742\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.258976586054445\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18039446657771\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.177595739657974\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.16230103750558\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.1726488868742\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.17296028123954\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.216947646720875\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.19533941566538\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.187055165206026\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.1936628821077\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14771205182699\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.144102155753586\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16337050614063\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -116.07623927383801\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -29.62027949577186\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -189.0563284852379\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 30.154241430928618\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -187.76995642367203\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -166.30225673597985\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.47555683049654\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.57451903659995\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.92573427010826\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.6385058325097\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.7371939003216\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.76128226402028\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 66.69815248806539\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 66.25666905190234\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 65.5541525136981\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 67.05416797265542\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 67.46306862097661\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 66.62380490693545\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.35621487350129\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.40659469835629\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.40841344547485\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.39989898687905\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.361720397046724\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.39638365048373\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.17416306153851\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.177013025192906\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.17740169531474\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.161860959397536\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.193511515353514\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.18249783843204\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.180071016057674\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.158937425366204\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.18462332088814\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16652790824717\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19123242896852\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15580471154048\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -13.858476034470968\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 41.11347202144457\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 48.470291610325624\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 41.826444745841144\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 62.33311420480181\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 29.149086436047433\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.05324332394909\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.04589542978607\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.03015428710633\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.46628454374452\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.30642053145397\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.70179196889254\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.68184256361599\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.63908493916651\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.70382570680825\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.59101597694405\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.7589144352802\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.51858117756919\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.089557345612185\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.10685166901768\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.0468842715345\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.05828495402536\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.02168840683759\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.94678664777898\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.233054417137254\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.189228381645066\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.192172833072476\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.177818503274395\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.21343001772517\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.21148006506064\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.13815164552465\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.18358770006282\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17151205436447\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16846780163119\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.15835662613993\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.16921058290012\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 81.23160375047105\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 80.63590350807696\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 80.04799872831133\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 81.84200733157012\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 81.47495233194097\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 81.20834391922799\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16672380737845\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1364723352989\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.19363673253676\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.1680792796461\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.1656954450355\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15018148864874\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.11917063921953\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.11596136144354\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.11729199226944\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.11063351647626\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.1155638462472\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.11392382456052\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 51.53636039320793\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 51.49611481320003\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 51.61598013228825\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 51.36574633708163\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 51.35623048932377\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 51.608702401845996\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.250396737709174\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.2732245904641\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.25368862602135\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.23604579114844\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.262619691779804\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.28371566620065\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19469204090113\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.14894104603783\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16655935313929\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.17064653464358\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.19488398806811\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.171237976137036\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16469866240853\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 54.446943442758396\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: -31.819201850964273\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.24486177910504\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 70.20890844522147\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.9506879410709\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16594509772976\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595367421398\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595624039171\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594621528827\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.15009044146785\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.23276354565124\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16592636627853\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16593619933018\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16595513197852\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16593551708905\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16592716923792\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16595291515215\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 77.97502516739937\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 77.91072966908878\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 77.97294775736393\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 77.98786000497596\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 77.93330903180403\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 78.02490357562505\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.514391978754645\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.567800103417746\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.54706571357183\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.538289028412436\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.50595109907201\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.517823721939855\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.17226572421343\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.16788033212313\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.174509587457976\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.19704726245647\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19455565391684\n",
            "Layers: 1, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.17956973425758\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -291.4145783404989\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -102.11819275064764\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -287.72690507247154\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -288.0449205529521\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -382.4697397164015\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -7.176867765990713\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.88052344738889\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.62672861609186\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.86992261524762\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.05333847014392\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.70327042755973\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.78648328946887\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.23243174670456\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.25777775922874\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.276322545484426\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.24634244561333\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.24139491859059\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.22702949868581\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.167180364201286\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18010338899112\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.1624531793009\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.16671262693631\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.16896000024971\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.16977154511007\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.14690603725797\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16441001837782\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.158220839667585\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.184783300650665\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16909719996032\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1695722986122\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17043814599824\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18883627924409\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.15381675890454\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.16394725655276\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.19805275451779\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18505515153461\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -474.3696950942472\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -206.19268398816257\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -167.1946063917809\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -226.99151171685\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -249.1289571623808\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -476.4146399347088\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 80.94387215766862\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.6743439534271\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 80.55001293602326\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.00641279070598\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 80.54163647466204\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 81.8853717111837\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.99854908658939\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.2680233578413\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 47.7782171661135\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.517991936442314\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.87607671522207\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.38292022184892\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.24490635972834\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.23590926377451\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.22279869840064\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.24241005125166\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24318721082874\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.24550177863544\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.161026313559425\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17046860650169\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.16894774351413\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.190741460099474\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.16101394287705\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16059941676487\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17725590531799\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.1444798099423\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15921232417386\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.18677115557186\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.12887007488623\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.14944065042486\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -242.595157296803\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -15.37333326043413\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -36.80769959125292\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -233.88776222041705\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -210.31545371994648\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -67.06255587207252\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.49085018156495\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.45634899644226\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.65752710855276\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.28063309361846\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.82663042699875\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.01755722135157\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 67.04047151072801\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 66.76464171372112\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 67.54983968098227\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 66.0811416605308\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 66.99658771676033\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 66.43499648223492\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.40257680646741\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.41592219215005\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.33441604853482\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.38308619651236\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.386203080187094\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.39087409322913\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18476546482131\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.17560011824242\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.191326722546286\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.199227670384815\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17947072848052\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.17421123549009\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17681231347386\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.182267251199285\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19963973954157\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17649822328619\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.191801228495486\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15772742762988\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 56.38556452514478\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -11.379637259105447\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 25.668472839490086\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 40.218134454991294\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 50.325354904702316\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 33.49696244597206\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.91715995077433\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.78806813887174\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.88524002145431\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.24579630128534\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.90310582531153\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.12790606458114\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.64753528685235\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.40764916606182\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.67408736967519\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.5524576806821\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.69044386320842\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.62075913560587\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.07128901715063\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.06255770041127\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.20838833729175\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.810347115160134\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.11636160126604\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.21534757264671\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.21270893282665\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.2285961361011\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.222107470571416\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.19763077230925\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.20066301819665\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.1883891622897\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.15194128882632\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.14414830683825\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.162945761597896\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16943127965383\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20122993784972\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.166157452485336\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.96412455518903\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 82.87334844349992\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 83.1783511019503\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 81.68176193734811\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 81.36725299880567\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 81.80126905349832\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.14904254105615\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1450160070082\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.18279118203185\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17403737731395\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.16034614605424\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.1640408420453\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.11385692469133\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.11617648596959\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.11319907231946\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.11992721352762\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.1158451454183\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.11357819718513\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 51.36271009831124\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 51.577656996069535\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 51.51117483697657\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 51.269844307106574\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 51.598547739278644\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 51.44953256172149\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.255932081154945\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.235260324327584\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.25070557063556\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.277950045980276\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.28065512017542\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.262714244924005\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.20828112913063\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17747935322966\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.14821006595305\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.201022555261396\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.174349530068035\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14511828672671\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16601219879607\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.19198970553241\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16610323134445\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 75.52557239598316\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.07186268644402\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 81.11864740289762\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16593390568308\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16553109625906\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594359058048\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16595708176145\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594339669963\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595376749628\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16594679144364\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16594548914959\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16592840934376\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16594158226748\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16594411003484\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16593421662407\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 77.91876421058869\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 77.92498176793829\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 77.92945828710336\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 77.98015173203103\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 77.96820362134595\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 77.90845019221993\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.5331431590928\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.50446399550786\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.485891932760914\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.56148646745901\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.49569165656335\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.55653547262969\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.19111446140668\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.17339491765334\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.1723185472982\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.17890886436844\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.202601611795345\n",
            "Layers: 1, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.16366000717708\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -92.00177230732534\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -110.88697839889848\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -199.01536582117112\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -390.21555368864637\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -103.74769730269797\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -0.8059699781379104\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.139756823105046\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.68880720327401\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.04178583230445\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.55871252676148\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.14049691291069\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.68388627979962\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.29084321550131\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.28048852143512\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.31393914243065\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.331081225074854\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.30988194338126\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.296242462091016\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.1724260604712\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.20170563203363\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.16989999969454\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.211821008024586\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.20240839439653\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.20725680758451\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16646849990353\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16872286258634\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.18403260238072\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.17285128002955\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19354188288457\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.170138493381394\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.174194456306395\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18400117206575\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16619047325152\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.177676240139434\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.178764308007935\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.175218287088036\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -514.6515050499568\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -563.7475081792406\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -239.53016597739025\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -516.2950592823855\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1598.7798214068484\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -496.42461048765665\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 82.22716644069085\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 82.19162720759618\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 79.14603245500682\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.6601202174261\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 82.64746083580401\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.74336146069159\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 50.74467260351938\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 49.937673070145415\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 50.92703972344752\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.56906096268578\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 50.665364676295546\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.70006674366364\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.30718338295695\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.29552716721816\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.26649861328953\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.32083343505848\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.32825295666735\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.28918393608219\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18456008898843\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18978110932908\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20949256830925\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.18353176425741\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.178017838964166\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.18874683300383\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.156538489258246\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18067275407597\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16710988191426\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16215481041953\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.1962739402249\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.146600185489085\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -358.26944099516993\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -733.435798135808\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -40.92522853445997\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -157.4232567760466\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -457.9592139445642\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -166.4403210575205\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.97478819679931\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.28389273286093\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.99471691425705\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.87804523609952\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 82.02124419320256\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.11585564132298\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.44605523359286\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.3652661803311\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.4831377114196\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.29252982775188\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.05506145025882\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.17349758685938\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.63565752853035\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.53987885596346\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.6333936556908\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.58815350534561\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.59757856168438\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.54924285825946\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.21664764888236\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.215812915496535\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21319954168611\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.209716408998254\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17463947977301\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19217626106117\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17604619857798\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.163611180772754\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.152829861762115\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.190687446376664\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19283370195421\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.169830689250304\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -19.009358362535167\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -142.65152529002057\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -56.966212777100075\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 6.584498559463792\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -84.1496101128081\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 48.47118251114272\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.17681048809911\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.9392927480375\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.06138509802312\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.90069511030934\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.0987830577246\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.27058254552153\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.79977965162868\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.81995410867977\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.80566794487267\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.80349555379549\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.85552107039474\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.842772696363\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.779692973945856\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.35813266377272\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.417819846820166\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.47626471704377\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.38512950423157\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.31334714598362\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.21181854223823\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.242264655750574\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.21343896448781\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21464337719484\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.22224557401322\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.23888669292167\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.17314850347436\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.16977222136565\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.167651821247944\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18934251281343\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18323067060197\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.17758530669455\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 81.90224306218505\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 81.52502837654819\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 75.39780135167527\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 75.38721583197754\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.74889542492221\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 78.58215502500076\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.68278204804871\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16331362788321\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.19012851700455\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.11611328812728\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.14800803305593\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.15026954896705\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.1867441748768\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.18796322086921\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.18926975834083\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.18766137581471\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.18788146716587\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.180843780541\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.51032850959085\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 57.52730241624797\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 57.81251071677567\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.47782604229441\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 57.71306765379008\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 57.56836248449567\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.35365096670277\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.344872621767216\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.31184664263315\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.321403385283254\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.316639950864754\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.3123364537257\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.188521437596464\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.162006329443535\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.204248651365575\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.17218643558645\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.19752259025408\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.18621891747344\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: -18.722333509344956\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.16578397908154\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.01108308710904\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.25672403898191\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 24.975128495743505\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.02165975962099\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.70543714387233\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.142705229813\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16594059457256\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594130059151\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16639465987568\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.42610256151536\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16596359140259\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16596970596576\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16596263480177\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16596607344334\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16595876084281\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16596366456518\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.9950864359845\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.99320048192108\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.99512422354596\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.99467882709482\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.99716818959126\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.99397974943447\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.93230855625836\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 46.013919745107856\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.95897180491599\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.94459058686171\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.958202484927966\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.933905137730676\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.1866038019655\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.179447901280156\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.20633080653466\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.197038012124466\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.20555983598143\n",
            "Layers: 1, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19605995832839\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 54.18517021278699\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 52.940374702889656\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -568.978518216053\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -397.8286111551452\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 2.7931818287172394\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -10.204075286797188\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.19463114970298\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.158679457124876\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.702916038575914\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.54820263271456\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.47505585480043\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.464903940059784\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.30290169873581\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.30402467817234\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.338181366734595\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27742426258008\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.301456632004\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.26395855006366\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17069043396745\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19977639208855\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.16593254173098\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19084553961618\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.168401868145814\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.17504014435525\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.165606353403\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19421696984418\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.1725798197938\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.178217706034744\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16582968774744\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.17476010802201\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15517242836913\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.168035533404215\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16063201282814\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.16450889266853\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.14121341893444\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.17081105167251\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -607.2785940017561\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -607.9766193426657\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -1598.1177228101608\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -545.4721092226681\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -605.3679025054298\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -605.8487526337946\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 82.42731110181761\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 73.08774221249818\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 82.33108304906816\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 82.32771545522827\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.64129826846047\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 78.98025595734848\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 50.44875499214028\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 51.31206018546555\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 51.29652839684148\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.89315057978303\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 51.47391233308862\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.748341159375634\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.28798010054509\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.29165006542595\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.279873267722095\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.32231871325182\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.29036383995858\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.283864751546766\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.19462398540065\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.183049002785545\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20918132894821\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.18941899904453\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18321610249666\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17415311450077\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.15364824484149\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.159968074399124\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.178807567698584\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.179567374046115\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16065863303349\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.17242491780057\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -800.7625806638003\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -485.1477182098948\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -658.6046685942024\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -756.2646539393265\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -78.75257471579906\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -732.8571530580979\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.05363334418715\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.85782843985957\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.51587901940324\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.34331015640628\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 82.4307352672558\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.74371177298747\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.40793333935655\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 81.0852156684094\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.2198683270148\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 81.22986464560786\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.7094142912538\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.68627708537578\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.58459664397385\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.604236978132995\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.54470595486796\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.60320783878766\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.589446529044444\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.58809175364327\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20017753755313\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.19952548550793\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21182910649061\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.190031739997806\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.19195971746493\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19923113097516\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1550311074829\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.151662181624886\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.181457822212515\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16617820257379\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17874078126357\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.164025140183895\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 27.277835421421358\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 15.619282179240933\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -10.087558894857928\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -90.29297797580217\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -168.7606135318231\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -170.7086497108553\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.1713952495076\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.00115997232511\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.8212240280024\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.67164771384007\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.06416160538134\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.28370502180738\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.84491737528144\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.8280016621457\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.81336706151988\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.80168379208206\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.80465602033387\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.84960443213565\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.383298468906446\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.213305423588515\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.4836219665439\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.359137565974976\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.62381580945835\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.302247916798756\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.249030899015686\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.21996442066012\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.242935873265836\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.24609251049468\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.262785236289396\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.205793211739945\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.16920290838101\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.16205528870869\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.183118079682885\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.17544955113066\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18706538401348\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.166705460350144\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.81745034156744\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 83.14837330275675\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.4166434397403\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 83.08219273519278\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 75.65137227742115\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 81.41331714416859\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.00326032751953\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.15212202748897\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.16045212571905\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.17072620508907\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.12204687267051\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.6322726548564\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.18626085917155\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.18987070302374\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.18622737631378\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.18335060328874\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.1842529613669\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.18661707317321\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.71512425752906\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 57.13184769600801\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 57.655134785264494\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.481012053138954\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 57.63022747475226\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 57.532698076988574\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.32341776810098\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.316513960064306\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.331001113617944\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.31286433953034\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.3249174351102\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.30988175283345\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.18944900460298\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17481216630499\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.177564218579235\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.18436120299313\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.180102273840404\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.197457669361405\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.16604863010608\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.2980070600667\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.10798447480548\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.3044311375649\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.32128379552539\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.21404997397988\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.16545688013122\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16593895573061\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.15606627191764\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594965575891\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.16594410637673\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16594699995701\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16596319083743\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.1659582596791\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16596661484648\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16596664776965\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16596720380531\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16594855100385\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.99298876402797\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.99419192825188\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.99500406314277\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.99591597899834\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.99440853249126\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.99486715948166\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.89502787791224\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.94878482559105\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.88841033137058\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.91379128258617\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.86501539902391\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.91620112179504\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.211634845153135\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.22002614292595\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.22215672697919\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.221211798794535\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.20423763456258\n",
            "Layers: 1, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.205909361588404\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -205.03087407928984\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -91.02546928712678\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -107.48641169615345\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -105.56791593436769\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -300.5378073881447\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -105.47493830740477\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.752522740091536\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.26919955300974\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.94948457011694\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.673497925453546\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.77997129650284\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.452718965231945\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.325112143173484\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.30660067674231\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.2881565193621\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.317973013792944\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.292938057559006\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.320526650179026\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17260727460993\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.162621891136034\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.165025498878784\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.1718891679574\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.20411961511175\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.190292486142525\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.171896961812706\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.15869361286826\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.1705656481031\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14954075650277\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.160216115021065\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.192151540666345\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16737090464786\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.14986735250351\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.154312166324985\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.16713156147657\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15605202177271\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16152824571347\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -1129.6160783171424\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -499.8926293058874\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -561.3449394378252\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -1125.8616478963515\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -509.27495317345705\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -605.8946711755503\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 73.90195266927024\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 82.88314995794951\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.37063279643935\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 79.1793490425581\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 81.29703784702288\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 74.10426874383913\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 50.64828821169631\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 50.889696846714315\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 50.46878956975633\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.89612154803791\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 50.60769932010894\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.607216264102306\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.32206965949762\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.303843797595\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.28041501664908\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.31340618726746\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.28523506108159\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.312231333928466\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.17125210311088\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17429985225666\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.16632441112448\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.190161716744534\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.160262974680734\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16879456736309\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.163372968724346\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17641587193061\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.17618600126819\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.17631487419317\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.17121598429408\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.1607065126432\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -655.0904478549409\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -303.59039132513664\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -809.7778455004823\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -221.93955223908767\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -748.1696091276227\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -814.9925944829839\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.47641382941717\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.81970129243523\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.49925856742084\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.24550044532018\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.48166431519186\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.65415120755844\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.87997298429055\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.3264636635826\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.44809240843539\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.46765633764787\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.23596763530819\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.44830670793897\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.585321005530965\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.631385894158605\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.63048092126103\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.6121629933599\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.53150260456601\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.60905722294633\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.19005112171725\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.216431830508476\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.17829527061109\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.197731987882214\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17475291947457\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.219111423489345\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17358262999978\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.20320075541579\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16961479370561\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.15039683731972\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.14491282514035\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.18153156106345\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -159.24664862822607\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 16.480098037734358\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 26.592604310109238\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -51.76389494662912\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -26.42710051410355\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -47.83783774747214\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.03652033021179\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.89274888062414\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.92281932194297\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.86850238971249\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.7442261703544\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.07298236666945\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.7819077345801\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.81101136158195\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.79389199307013\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.82952805501911\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.7859942912141\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.79938936398136\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.54662665073721\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.27193998819482\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.39754279143037\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.353849406566695\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.45040137002099\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.312436423221214\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.21990010642926\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.2481032550813\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.24111552629218\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21378505178186\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.24800816655164\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.2255821050801\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18672762336663\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.1651213917146\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17794338926797\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.181943514417554\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.17250142169178\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.18530608569588\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 82.62594142909428\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 81.47436761654883\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 81.67423022218622\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 82.28131312906947\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.64884660469878\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 80.82125754113119\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.08181585641853\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.16830805631082\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.12037197986598\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.19931004666532\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.24181255271803\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.1658395213737\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.18696598189061\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.18354310685756\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.1847567369895\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.18264171818366\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.1867833863651\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.18642926847153\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.436284566084495\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 57.6658243343297\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 57.67490844880662\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.651450892643865\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 57.89965710033618\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 57.697802903634354\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.30368877112178\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.32256884620417\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.3538192583103\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.31111489048304\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.311975722752386\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.3505068086445\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.16576483324142\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.170152753716216\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.17480709177786\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.17716034761941\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.17103421226651\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20176435894617\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.04425065801844\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 83.35032479535313\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.05983246721708\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 83.0467057328687\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 82.30855210878221\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.16019092613955\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.1662116692722\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.1659523993559\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16595779143854\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16338055884651\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.35205120769643\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.1659610032761\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.16596299695658\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16595994973484\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16596718185653\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16596294574276\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16596246652783\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16596518817605\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.99469338644954\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.99590860238055\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.9958554049493\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.99696320814331\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.99597763036648\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.99204313851061\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.892781081756254\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.860922854217954\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.947792225849206\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.955511486585756\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.021120409796666\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.91029854835017\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.22287949177357\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.21314927885872\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.19489211144264\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.19652386900791\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.222982604461706\n",
            "Layers: 1, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19200495922403\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -87.45646891120207\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -393.75306356716857\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -105.88261469507016\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -11.771952347321935\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -377.73471919521774\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 52.56377273379591\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.14216954659172\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.598657467068506\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.25303473047795\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.85726779124313\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.83406421429566\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.7109447367858\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.28822915769661\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.29476174111948\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.30414493563026\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27862934940996\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.28891227860575\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.29826846568533\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.19365900818981\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19137776713985\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.18925405121907\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.181634388624694\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.185840097691845\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18255256580087\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.156330936929514\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.159657861696964\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.176125668815544\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.172403132005144\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16534695612112\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.16701177078738\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.19668922450223\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.14146106770083\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16049752944915\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18000657105583\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.13786249196406\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.17531470427777\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -534.3495849050412\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -503.15507125067995\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -554.8039398039718\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -462.4186229230254\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1183.844082504568\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -418.46507537214063\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 79.1897091172324\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 77.64643288957328\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.17275493909538\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.88558785102356\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 71.60176408487487\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 80.31199074678601\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 51.15545130314432\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 51.0764977460535\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 50.15246524492305\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 51.20679804435808\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 50.04140970861953\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 51.00588587837562\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.33949583762057\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.288454917374544\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.279692971648814\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.27438015295145\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.32061471558442\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.25410628824888\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18076690761062\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.1808227298872\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20944726046091\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.18491737406821\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.171433851372214\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.19412853412732\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17377009026751\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14771657256037\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.144533982095325\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16507552787704\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.149466025681996\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.195779242489586\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -596.0542954783994\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -45.789276587812225\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -749.2395021433477\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -466.90711788898875\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -137.69270315634873\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -103.71757193822533\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.49899880548615\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.0777474100854\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.06714387007679\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.37902455829229\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.17302435570217\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.88522766429344\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.32848258973266\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.6220282740551\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 81.06477734500255\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 79.92081964340619\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.84974235376271\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.16222625467849\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.64699142409676\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.6603883421045\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.65164130773377\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.631594821013366\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.57995793999447\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.603353748198764\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18102448149811\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18150906073516\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.184192867770264\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.174602125932104\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.2131225490256\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19385634642488\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18670780037819\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.16859099175853\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19623429693623\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.175207056867215\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.168335848642606\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.1729672640489\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -63.14701441013817\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -177.55092403190918\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -56.58807363828391\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -173.64413407555844\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -27.92831157696034\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 49.63493173498279\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.97647714980681\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.0857522066041\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.81195339499921\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.73671215322221\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.95382164610177\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.00116495744085\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.78846847430289\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.85349005869993\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.76480410882783\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.83355480123856\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.78393983639745\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.80753178920241\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.17255668625333\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.55750560639322\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.52921378919458\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.409330561448435\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.56214102526529\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.33767789253043\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.252873511534105\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.220659601758975\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.25360522803532\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.237336580269684\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.23152376764722\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.233249508532495\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.192212528314435\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.19048593875655\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.18077188691587\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18131008743062\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.167670997385166\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19347355799985\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 81.59460532285722\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 81.84748974804864\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 82.98763176376993\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 80.38267922849549\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 82.51203816468386\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 80.40647774123121\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.16709196700143\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.19294017166939\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.15873458095561\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.16209884086787\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.16370894545835\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.16710042093831\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.18296379269336\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.1870282652006\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.1857297134253\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.18851151958388\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.18497528278101\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.18809783637043\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.316531551509556\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 57.888494767548714\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 57.75404902141488\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.75786524126253\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 57.24252859694226\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 57.684666741772276\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.28759625303982\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.32817754428298\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.302781882632615\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.3187950694627\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.32613670755654\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.29830737713155\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.155429903299584\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18460310611011\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.183436550176935\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.17873160264555\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.18428199631236\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.174683353790066\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 83.07489244609579\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 82.83257146424884\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 83.16603654364675\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 82.81160879190004\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 83.13542701021026\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 83.06410967640645\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.21943080054876\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.16595305599013\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.16593936909923\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.16594668901602\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.1668888018147\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.16595620198134\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.1659605935656\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.16596480041434\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.16596820613275\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.16596497417548\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.16596126300327\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.16596269333183\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.99375356638345\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.99316950579644\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.99537915949351\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.99535786369361\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.99348483654472\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.99582064631893\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.8809766442864\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.90408506051226\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.97601930198939\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.837484068302615\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.87627472626748\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.95326565734765\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.187904602845485\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.20541346545939\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.177050013122646\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.195214572690546\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19113645494651\n",
            "Layers: 1, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19463058287084\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -20.26372578119382\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 51.433731674645024\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 71.77065127555282\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 50.18846931428255\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 50.66733745674452\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 73.35013599929839\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.147417600659246\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.879771598555095\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.95909028673793\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.91548937122438\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.929550348363435\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.50403283407828\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.15593397776526\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.167565268743395\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.19725472821365\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.231363919328174\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.19547087720181\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.228843634221505\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17169912090281\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.14506130686118\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.14362083416466\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.25339137494141\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.184390059421155\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.20814486108735\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.2139097369427\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.23457062941175\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.25819302779828\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.18504169708676\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.23034748331563\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.10854032772297\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17623652817305\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.207316564645375\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.141025672880716\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.09225271087773\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.133821318254\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.163762138994\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 43.8233441980945\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 62.59629388947481\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 65.23737059821102\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 70.56815757210062\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 35.6591153300307\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 77.2444845389076\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 71.44628320656656\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 73.45125878776312\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 78.60858563861035\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 73.31397768817371\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 74.01905809730967\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.63731720971568\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.29788652406509\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.260433490312\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.92346963381282\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.95815576736984\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.12784528160717\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.03725590713188\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.25515132926985\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.26518206504693\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.22562113642338\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.16438403819506\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.23902553660595\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.235214303531635\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.095083573236515\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.1869351296794\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.25210643522256\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.180234403546294\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.264839430821546\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17416563321592\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.188682244245385\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.09396533365674\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.178017391711414\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16024169912202\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.133879468213635\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.10257090910223\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 75.61982477127384\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 78.24592101605953\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 75.07364269413159\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 71.97157667750456\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 79.63718948393523\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 70.43850179920263\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.76580610081388\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.82692069700042\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.55801236057904\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.17072601376888\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.00763362625526\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.08335460901351\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 58.695936737089816\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 59.03656501627352\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 56.03178235091695\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 57.80343507686977\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 59.9812170569358\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 58.092046465323165\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.328290383215375\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.47924809589393\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.540354735763486\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.255777143202025\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.384509580765\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.48469331902839\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.16611144074986\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.22064886661406\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.15444136702315\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.24401536147783\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.1379119907479\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.132126138285855\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.10113351009887\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.1417290168201\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.158830708329326\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.21678528000751\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.21895701146396\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15588778939554\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 78.05157233071044\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 78.85831493411341\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 78.37327930083163\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 78.23764792245879\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 77.97191084322913\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 81.00283145355822\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.74950279876744\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.68483618826623\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.32001563256208\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.25236247698832\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.67168683753326\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.27391208228728\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.57513503238164\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.29065508416295\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.43344464245361\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.40769906111683\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.41546008245764\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.2208370666738\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.93246650092209\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.14279965513914\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.00374543877182\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.89893132802578\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.72838500033497\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.13361500726607\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.209161326957926\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.174255020267005\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.27711722955513\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.202020572017595\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.171861516852616\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.254900850324184\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.06959120052433\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.19807152007108\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.115861495235876\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16146429071698\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18311037944568\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.08144857105415\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 74.09955322125153\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 81.3253962842724\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 79.28956711113932\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 80.61252457180103\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 80.57043037516979\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 80.75786889079525\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.94742244461463\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.68468830034017\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.91799597526173\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.42787951201281\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.24905256937873\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.48645920309016\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.23323585267354\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.61649896616584\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.91107241385825\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.3730774086739\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.82929376866291\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.30027962169933\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 49.18357574944301\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.444657385189224\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 49.21343287380893\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 49.308017468973745\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 49.349677375691755\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 49.249653216747205\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.30666490241536\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.19208753801593\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.22541222050015\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.265748938227645\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.15543184027012\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.19958707716532\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.09559760862582\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16580561939719\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.19700201003914\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16042561033448\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20575907348581\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.12100488070354\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 81.27046442415964\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.93997054398174\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 81.75165920455107\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 81.20231106768726\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 81.38099445801015\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.61095090748297\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.04789831046634\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.38882184074352\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.09161349005016\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.73675547205812\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.82298941515303\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.18931100813876\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.05630272652027\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.99389807273475\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.43609808704888\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.96357873961256\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.1480978997585\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.57374902717171\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 70.09181045873164\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 69.52744030888437\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 69.86032686610308\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 69.45767466483282\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 69.91964655357803\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 69.99845325695311\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.409816019908654\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.4463739049277\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.52372525558647\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.60652467348953\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.45840864848898\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.344179820870934\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.25797604740032\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.159131191030724\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.18863749237732\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.195062673516006\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.102803981754\n",
            "Layers: 2, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.132051221954896\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -1603.2813225480209\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -2165.587673384795\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 47.832671208632235\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -1982.941417979426\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -53.40618723235369\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 49.386847497495964\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.04349990148493\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 50.831767014721876\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.56631376610595\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 49.358143387373076\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.0289797648576\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.78923221843014\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.225012690574786\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.2084300014443\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.23365239861039\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.25545181528642\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.16406890288521\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.25233389889035\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.208111347985714\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.12098236963307\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.10962495665339\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.147156425818544\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.244471616485725\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.10533578005109\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.13266516673101\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.179035645907106\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.17687306036879\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14093001011229\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19264442366049\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18153039072285\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.146425249591594\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.195022369361624\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.09321087154681\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.1540092407934\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.12462356081714\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.23467928709874\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 26.213972049972135\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -740.7102740291073\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -403.80730038179405\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -77.79695933012383\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -182.43209521707377\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -74.69912126231661\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 82.14818408126158\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 81.44443516352646\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 81.92329095797288\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.7421938333193\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.510731218965\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.7800827615326\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 49.330343181802526\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 49.295696081628634\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 49.90731644296628\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.5298283369458\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 49.67504976145097\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.72442843020534\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.30850183060103\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.27268141702441\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.26764386919755\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.28085268082938\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.227369109149365\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.20111962648463\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.2014169876657\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.14063118854098\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.16393470380164\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.2026404550647\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.12780942119255\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.13500044006593\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.1020666370029\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.11322146895337\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.18073081631195\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.23430298979558\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15046410787013\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.13013091842145\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -55.74130611214822\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 75.49974565173092\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 46.720624059655904\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -31.560350367974443\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -60.43471051762286\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 29.804792620004438\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.81299721367019\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.86009542577149\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.23111434324885\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.3406489668196\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.51998031610358\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.10935914749655\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 76.8248926582673\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 74.72434797093106\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 74.74241833326502\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 78.4046187938563\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 75.6510242155607\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 66.5410342168936\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.553739277233696\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.52614879018853\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.42031322590546\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.52534184515431\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.641003368250196\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.344563301782706\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.200799264593925\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.16314098495728\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.16205010376585\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.10676897903615\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.13413250444891\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.18902590477608\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19738328937866\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.1202251767296\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.203156476018016\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.13600111935083\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.11296760659532\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.1354504458672\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 62.80787924125271\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 68.560040864992\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 39.20229504105686\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 47.49521836404102\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 72.8478352466579\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 42.6367514024624\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.60945143368585\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.8688442557993\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.7247272904092\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.91372298096897\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.17670441698104\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.61830298727757\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.0761656660571\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.23142440190112\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.29366904618323\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.44963565529376\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.91327187783608\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.17667913546633\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.23049665661117\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.536538614365256\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.67699570365973\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.50505859137588\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.18025947536232\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.18493651637273\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.208498855997746\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.16928246366964\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.183544718113566\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.18813305577648\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.228369229919366\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.15747148401058\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.11973458748435\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17448971609935\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.153205977301965\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.10943192496358\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.181426108172815\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.157275635679085\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.84165354656633\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.96225194488396\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.85200236517669\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.79080578299559\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 63.03246590658582\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.9168244169094\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.54565579817128\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.36337647403298\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.89090236229234\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 84.53776730126606\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.27979787922382\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 84.23435332997347\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.92727346131295\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.09537995780707\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.38265434480235\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.29648260783088\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.8253035177707\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.7882627030648\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.16035101758506\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 62.49486045861181\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 62.36862043882632\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 51.28776732650716\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 56.91515816460636\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 63.015675970910834\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.24347116489793\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.22314763874798\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.16730567649019\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.167636498094375\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.222379854759886\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.212048596806476\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.22251803466516\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.189568279236035\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.24142181240008\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.23102872055074\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20675378369676\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.08410709030929\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.9344893771202\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93448223645174\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93441949221747\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93975652732014\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93446175092748\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.06887229740917\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.9086069949157\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 84.53940951600127\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 84.06089371541107\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 84.73587023695173\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 84.3395468774407\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 84.1399133566654\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.84738167812873\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.96728477214643\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.91415691485477\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.78854859867543\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.89386616770133\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.84264760882543\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.5950222871383\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.24705577326498\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.0387999941025\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 80.5975838259897\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 81.8971160933301\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.40719125240665\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.589010279767116\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.67616904338041\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.91528794707857\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.83846102358282\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.76140248102729\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.380405376829266\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.13822402061318\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.14785646136578\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.14905722143067\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.18971779862062\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.16741600749627\n",
            "Layers: 2, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.184296675438674\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 49.40635959901799\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 76.92915613183949\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 2.796644921224145\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 2.985411403642846\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 76.72682384261236\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -41.56038238209327\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.204805674112116\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.77944236721716\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.24535839768905\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.79402656952083\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.59985675320664\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.14321992961574\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.21134690939237\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.31112191254808\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.2003139878527\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.2662314238519\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.19317831161764\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.2153772058692\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.239132137781226\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19026026443379\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.149803877314085\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.12592957313285\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.19897327690139\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.20098897721011\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.15637652877677\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.201301342201475\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.22470466245293\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16527242184408\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.12493762814614\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1414756467448\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.12717975624332\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.13972044088258\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.13863311946895\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17480395249179\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15870247314866\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.14660181921357\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 61.333116766223974\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 41.84264271936976\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 68.19054842635411\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 57.96762866995826\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 58.60172368211677\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 57.742182216198\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.83478476784447\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.71034917915559\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.5991705223935\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.18220036561159\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 76.34758392146688\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 78.60320041131187\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.06914035332903\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 46.09573106656918\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 45.97677598682728\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.373456781926535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.40428355886051\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.336744418764276\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.23920731539693\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.26188524164857\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.13264539372569\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.25483499402212\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.29208387820872\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.19103760845704\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.088919162790255\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.19096037387237\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.165664616630686\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.16091819262432\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.15498655779794\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.232396011391174\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.237448261782816\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.131035459402554\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14771611686606\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16642903714392\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.152510590515504\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16451897579387\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 80.08834525731316\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 70.71004762931213\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 78.09191952183369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 74.9503677303781\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 77.83821816365747\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 77.10839470913459\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.74127934744499\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 79.39268962331873\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 75.81241187712769\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.14315731369037\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 78.05613625676045\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.13571301026046\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 52.14838928585544\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 50.71532060691211\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 51.37766331971538\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 52.768978306192714\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 51.82779061369024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 51.109948995975785\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.32926528279837\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.39281195603342\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.311257738890106\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.32236409441099\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.26649637302826\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.276845163102344\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20531577379294\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.262283963384554\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.18173731390871\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.1458098475886\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20330720752946\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.23555206651484\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.10987065365237\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.146078888254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.10511280323611\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.14225919319065\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.15589863783171\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.21915907145267\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 77.21610181579473\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 79.84521342732232\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 77.41025056059807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 78.51347241037688\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 80.52797553820037\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 79.83857556142979\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 80.91342767262577\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.30123934436678\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.47196140589273\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 81.73166929829564\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.78774610167497\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 81.0249282813502\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 69.89494930248313\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 70.05830015585622\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 69.46501708826344\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 70.89926896353174\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 68.59170705182918\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 70.78712098297599\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.55471387947937\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.44840587993942\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.59145104795934\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.56278565113326\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.828055221777475\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.60809239385043\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.1756120886365\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.11660195095446\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.157775278937315\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.22802347525251\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.107957918109754\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.228864010702075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.11467785343362\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.10919340049164\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.183015898156576\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.2346022960083\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.21835925255441\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.139500323781625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 80.04532384195683\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 80.63512587757644\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 81.00950489110036\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.29277418727018\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 80.98141849841201\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 81.09251931749819\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.10598944227586\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.33653882915232\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.02676513783082\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.07287661454026\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.64405518397547\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.86436812997458\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 81.03655462846658\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.4341127010063\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 81.62004474483324\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 80.48642341048955\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.45131410745812\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 81.48057993225638\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 47.03130900196797\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 46.67653586108153\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 47.0313694424953\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 47.20383253552971\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 46.72147555327479\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 47.12672457138972\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.192131142896116\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.17792704252053\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.174735674629574\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.15949064722255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.23589511622368\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.295822824646926\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.20405068847101\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.158224552830504\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.1158678181981\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.202020371656424\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.073562499848784\n",
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.19668974142378\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 81.01366318913772\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.68282963262922\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 80.98855438203157\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 81.09428264923739\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 80.86480609292403\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 81.0807464696184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 81.22350163783523\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.34209108224626\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.00581342157935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.5675515839548\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 80.50916674679434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.52918235799294\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.25161201760922\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.96254251229155\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.89578993767671\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.27503390471324\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 85.27873128503961\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 82.99025636182053\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 57.72911960702771\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 57.53333537661804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 57.68669973811657\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 57.78699282626791\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 57.713860682614424\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 57.768009362844474\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.39549069331211\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.39705711417972\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.18773777293214\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.32057796803913\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.44267625748356\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.4617301115461\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.13812871514825\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.142260582922546\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.134662007709366\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.14306462247326\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.11315819570502\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19546158113783\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -23.80619101566421\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 48.31403875515568\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -10.218728324852089\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 48.8545638508023\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 65.44285409515463\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 22.132072795795487\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.67135707804263\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.1897441748977\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.50247393822848\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.21889929907231\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.57814677027642\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.4967366851006\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.20065117060801\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.24327727679862\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.159632884741896\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.18229716861464\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.21039327973801\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.290743725472225\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.22733490966978\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.15535591065878\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.1310207419266\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17208136681726\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.149420408039845\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.10169710275922\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.209388248650995\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.210383172912536\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.158543500547\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.13469526774275\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.11279281377393\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.114932068329594\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.12820877331997\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.134645286585965\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.24588063872256\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.132784556581754\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.19299494441618\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.160234260437484\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 65.23073965715732\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 74.91549340801583\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 64.69703897005029\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 58.39313590485426\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 71.52284254184572\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 51.400452731988146\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.61268551046929\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 73.67762262399597\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 71.74531315129335\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 72.11940010274375\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 72.24336721451748\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.68921084371068\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.102137204401096\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.02065392877116\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.3518408515591\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.65794295310334\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.60575701351773\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.06641626866169\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.29583145147329\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.175922815318216\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.21773029910955\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.23695921850447\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.15372633734681\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.29148488853871\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.13271623515779\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.209085782957715\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.17144928668101\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.200964079603224\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.23112215124583\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.15226316192813\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.13889598471549\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17127328852209\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.11645242318906\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.20683449191818\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.1349726239863\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.13001623776025\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 75.67050737396346\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 70.27177102455107\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 76.07613464402293\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 77.94386813592125\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 77.50393046242641\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 73.15841473607574\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.37987853468356\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.24163657694334\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.60846173374598\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.58864351520604\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.80839280456613\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.31638406198236\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 58.433076701586764\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 57.69466335763409\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 59.893749236694724\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 59.49123719847884\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.35097531756542\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 58.055911640303314\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.349812976913086\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.40201023204226\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.300192676726084\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.365558982702446\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.247436715507874\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.32167814480156\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.228940707198795\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.10696354264619\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.14803869367192\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.162277641139234\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20407500800058\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.214174301109885\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17157022021813\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.19756394102998\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.11694105652114\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.1564447270827\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.15250051739823\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.162234028571255\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 78.88413264278621\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 79.60519996236282\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 77.54002265652159\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 75.94109733853708\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 79.35525128152757\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 78.36837199349765\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.46403876583742\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.25834908026317\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.29281346094668\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.57007333662577\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.14393790947051\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 80.17314115336995\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.51579434724434\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 80.9195582437753\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.00573410967733\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.320781887012\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 80.83103883755724\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 79.24388118754139\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.65002124477928\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.098985972863915\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.062504400152285\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.59175274799438\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.166668001792424\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.68533036756388\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.255162983769814\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.160753793191375\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.23964161097598\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.16691572628655\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.2874069648613\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.22831558079488\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.23353150022249\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.165833384238006\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.10855970587002\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.13141375663788\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20406534487334\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.1017572321955\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 80.48031347118945\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 79.08460756074345\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 80.10173947590724\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 78.645896201944\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.50601632045428\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 77.14909150767792\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.37048105284497\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.70895105968457\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.37211557819232\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.6807113811345\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.80348764674893\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.30694961931954\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.47549932049662\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.78503540808732\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.19082531930846\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.36819311828964\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.03237289096552\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 82.387938211373\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 49.68296559916907\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 49.9361414547208\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 49.82576767969369\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 49.89685471116422\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 50.23796979567992\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 49.29854171665684\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.17131447715962\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.25045754703238\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.22439133484841\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.221321844221826\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.20187657744073\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.21675416804625\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17952074110651\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16432567441705\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.111571708912024\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.1833726365568\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.11612247739246\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.17321161564981\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 82.02952726983823\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.61165407602935\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 81.05088485992866\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 80.32331617389733\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 78.70189602364971\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.54682679756144\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.38808946770455\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.7951308770979\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.79701309072409\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.93035596899479\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 81.84629895378906\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.5051970750745\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.41943332707238\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.01143761215239\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.20136795749964\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 85.25851521036567\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.62483225995479\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.84718640325259\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 70.15162570384163\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 69.71151760541757\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 69.91874720065312\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 70.01790833555513\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 70.00718917524765\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 70.07268929417489\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.38094718612253\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.40467915879323\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.343736149259925\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.31025844834042\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.42042368400655\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.31582090627126\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.2336473745913\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.23024660052526\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.165681268621164\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.20793841755226\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.18181851933206\n",
            "Layers: 2, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.126927779508044\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -336.0091868094388\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 55.69415096858017\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 54.47349055955636\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 58.11021934673162\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 29.336356282645724\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -204.33433611730442\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 44.02843246365764\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.30840979776576\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 49.685861404983754\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.19682383871096\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 49.55242572545914\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.03204116521322\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.318702772391184\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.23190053616407\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.361305204519134\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27798671571171\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.26964286651632\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.33227411086275\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.195081987724386\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18586065969564\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.1823435811448\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19186756298901\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.182473331709126\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.227048990167006\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.185792145976954\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.20735214464009\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.196603025467994\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.13395667293313\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.11986107318611\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.197023747872535\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16104334226042\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.156141010895055\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.202865659901924\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.142497448375686\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.20127489506244\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15576076508426\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 0.8815533638366291\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -87.29751708363955\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -50.15863834874219\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -84.50051682636479\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -39.91142316846772\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 27.23980685052261\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.16647160643126\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.33500154785695\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.31124224295138\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 75.6222390817599\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.30712517276321\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.54916987987967\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 51.73330387211321\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 50.55775349973306\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 49.048595156850695\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.46028774910427\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 49.86359321664108\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.39382529935481\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.34412344102435\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.21628761468981\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.301909384128\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.35304173572817\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.40559735719298\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.36863384884827\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.25199732342738\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.15621848089144\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.12636688125284\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.20219629178895\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.15635335257782\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.14790578205874\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.23567256299435\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.1541875158467\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.1923502469739\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14897386531664\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14633283183396\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.181103709768635\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 66.07101892574472\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 60.028003373272256\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 52.9064346795618\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 71.85309632143667\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 74.26285488741765\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 24.52566431453048\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.5605199306826\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.96825343455032\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.28755079561327\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.70851145862481\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.93155430631707\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.17516544050854\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 79.07688189522261\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 79.24680317687402\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 79.83696627900387\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 78.86021707905724\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.0757751190484\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.42909976921185\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.64419225950007\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.647723697580325\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.65621411087049\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.57608028075729\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.60175965761242\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.79223558472453\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.192250793075985\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.1850095356331\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.18508109512357\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.209823397467275\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.18125429890956\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19372118594811\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.14172496254108\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.180083329603214\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.172629120642135\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.12247689094039\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19842347194483\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.176749444181915\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.15594280785787\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.87468983087221\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 75.75218341887387\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 74.53191759397026\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.09095880342343\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 74.32731307234214\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.75562671212158\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.71831430509074\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.5395177834602\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.23271553864825\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.51764564821444\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.20919936033424\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.20051445282417\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.21915484224648\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.11493268614763\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.12887372849863\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.41160317330603\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.05777952503692\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.1680382974403\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.29206453190026\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.564939127761285\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.25319622518187\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.87386679036616\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.13928471157548\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.230733893317286\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.2553743647435\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.17980438837511\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.24698927428389\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.27046610117187\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.270368516870995\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.188204520365915\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.13778093009816\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.194741485345325\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.143872341194694\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.163932912113125\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19220596207231\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 79.20205502936243\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 78.64406748114301\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 79.07576038959066\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 79.74909346726464\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 79.98313412156011\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 78.68556320049957\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.43618804666792\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.9142978552616\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 84.03780301411948\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.44823644204207\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.31415117129907\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.49511470655668\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.50224434574947\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.94754185912524\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.12294433543998\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.28523490197549\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.76499068842202\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.23859950725173\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 57.24341059810227\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 56.78919776079524\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 57.068880568256674\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.722892913500104\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 58.23811874931051\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 57.066022064844034\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.37317274709199\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.3310670542749\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.328523954373004\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.37235381213879\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.28894374707924\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.39008677671662\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.23439699794638\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.19022979465998\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.25843644498247\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.18650011475279\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20040758468545\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.17414728256202\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 80.32831380776075\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.17072097140343\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 80.69719268848579\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.24156936686478\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 82.43243522922062\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 80.76038462291699\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.71382752094043\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.65816400569656\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.1208436974722\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.63564072599071\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.78525896368696\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.67478124840213\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.39380571857741\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.1960733459057\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.1178516249662\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.65526397904081\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.21991528429471\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.2311965326354\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.94712140069046\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.0735444316564\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 81.86333516387589\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 81.92523533419077\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.13793070507451\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.9395558057638\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 46.098681238244495\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.866578670470794\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.11073563650599\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 46.075631299945286\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.72517394985236\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.13264138036913\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.13397065190993\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.16269295241314\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.205051055528855\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.16430750173792\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.245550494679655\n",
            "Layers: 2, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.194536083934125\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -6134.867010142184\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -14035.45013837445\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -3533.8523034204413\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -7666.051330741996\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -3616.2430156068326\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -6804.78352501697\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 53.5770754417241\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.36297082368286\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.20755400767855\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 44.34361676723736\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 44.103937017583824\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 51.005366976616884\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.30215233752999\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.30402992410841\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.320097126229584\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27876297932934\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.23205861950805\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.23224617277766\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.219978898246374\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.171202011345244\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.178761721435414\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.15639066863355\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.2337611202575\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.14578420844602\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.2119806139881\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.15692784139972\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.19896870737986\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.19978844329906\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.13410832267433\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.157882134845586\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.1256783001445\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.1994157672903\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.156936036538234\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18442345662726\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.235551239434656\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15145181289917\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -1050.1519914399541\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -69.91337039071021\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -1191.2775322080438\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -355.99382746571723\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1528.7777994828612\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -1870.3626770675069\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 81.08279839118048\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 79.37313919374665\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.56096063265639\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 80.06443333891008\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 79.40478680460308\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 80.8380970700655\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 58.987711448069504\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 58.49290578174189\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 58.6360229657535\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 60.90852774190958\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 60.230000289317786\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 63.194218854315224\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.20442238092526\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.31807652674258\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.282037033430456\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.26125969174474\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.31292266846503\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.33063444898738\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18963618013232\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.192400683678535\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.19019491714042\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.20592764183582\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.194229776190475\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.149920255331224\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.15478227322629\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.19736826184052\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14457695299918\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16492876418201\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.155223011373934\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.17608173680821\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -278.10651331968495\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -101.57176308582514\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -160.80643759589566\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -173.4775061803071\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -144.26611579877462\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -173.50338296776857\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.59939716618592\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.89225724706442\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 84.10777758491145\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.78404112268235\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 83.37485031373755\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.82455856607113\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.45127662699663\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 78.22095841340833\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 77.93096308238121\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 79.84093360622862\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 77.94767682745304\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 77.8320481020873\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.93592662622885\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.75732978804613\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.596945989015325\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.86800794321137\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.04232864878075\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.945595586990365\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.21405116295193\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.20341715228854\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.16924989441004\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.154107178365\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.22934916604626\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.176070834116\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18285259838088\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.172715619082105\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.178808506698246\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17127004710959\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.11816523900583\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.18441513972018\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 4.033501793883698\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 39.65958759525886\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 40.42995533359704\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 24.315091547583723\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.78583084416655\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 40.825460482978905\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 84.1679594751789\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.5023570600304\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.26898411767986\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.23076169884978\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.27636583765027\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 84.49385537285433\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 84.03456612021063\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.80494687856273\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.41071299131136\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.78448815925577\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 80.47247778011274\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.4244193727137\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 50.452588736027835\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 50.73974233959477\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 50.295535152700154\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 50.21560667504925\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 49.85753872726537\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 52.07263759648521\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.267867544800964\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.22628125434903\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.232168846675435\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.242078133373255\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.19437280222737\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.23829159032844\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.14768016472098\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.16136467423594\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.119047375488066\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.19212600592335\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16746099573701\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.13370246132374\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.72441173066204\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 69.08860857019398\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 48.977993668200284\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.65252084150413\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 63.394408904596176\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 75.29241777719278\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.18822137871418\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.31638889565156\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.37754702357711\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.90904268909333\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.03978147312833\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.25652575739782\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.92554954225916\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.89122605076868\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.27334604457187\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.83396014977014\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.58319526528965\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.03166487064222\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 75.25461075906786\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 79.71711009134953\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 65.53045063719331\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 78.76943152390726\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 77.26271553427313\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 79.20319634010657\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.34245805938467\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.427499539524284\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.324508372478476\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.31186717692055\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.25888117903355\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.31287652083486\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.16735084953164\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18815539654032\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16924526788563\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.215566247661634\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.16203918538224\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.208247002429616\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.52885473593012\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 67.0621915955538\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.9344611656268\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.1455694827075\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 61.013090311452075\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93452320750023\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 84.37684438491078\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 84.33821567320302\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.8363045113362\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 84.26422570553784\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 84.59718104016063\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.70515652163805\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.84563264601708\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.72385331026932\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.92701269064912\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.01743074921204\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.695739849253\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.77895546544774\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.82580261830401\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.73744025009091\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.89589228847724\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.74538762934326\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.12882234568244\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.98670128309091\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 47.05500974873953\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 47.28350312946672\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.15358439678884\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 47.39317087844546\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 47.853606337210564\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.90765531117037\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.147087449285785\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.220499849457774\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.19212437393148\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.16958715479876\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19625379541395\n",
            "Layers: 2, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.195445186309556\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 49.83491758574093\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 49.9502450037972\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 4.747312258247394\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 5.95609601265542\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 4.7130310064446785\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 76.95526892717285\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 44.70943271645101\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.798984844884885\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.3030384446544\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.25739261020999\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.294521351266646\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.118619356865985\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.220185861343\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.24187488337792\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.20743373800019\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.28997092881809\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.27311368825041\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.213204257513176\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.14491334122416\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.154773255622224\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20680514060312\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.1775430065698\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.1610530921307\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.208110671617604\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18977627456102\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.166172117339954\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.15648625711051\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.173318217861095\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.161008725533506\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.21347381330321\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.20991083481758\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.176071282018924\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.19738698750449\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.21433958182869\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.186885510998906\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.162794779287566\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 42.92262967171503\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 23.755027645156446\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 65.60814278858304\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 67.67431814459306\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 59.10405886598505\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 70.25591721880421\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 73.58907596425715\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 73.48353861116293\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.38194945071636\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.67583989420292\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 74.01160444662769\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.30277375403061\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.6953829256657\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 46.54372393650527\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.8135584676499\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.69805223291571\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.99339659130797\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.89621165926356\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.29065245045846\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.30847584162125\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.25658029158384\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.22217663118596\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24751160761087\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.24600948022388\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.190431487685444\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18335081651196\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.18050418677624\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.21054584031412\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.142270883021574\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.18436313859724\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.18417698129337\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14643182227117\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.207124930268094\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.17450503420869\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14577714389095\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.135104208055765\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 59.002261778198985\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 72.76713252387653\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 77.90024854325681\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 70.28062664232448\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 73.96056221690176\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 72.74191718472802\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.78178988135907\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 79.41440447656231\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 79.98281696174718\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.59475410675061\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.64976156722736\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.00728372325831\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 56.43563103026524\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 56.173592981909906\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 56.671846632670295\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 57.297607860118895\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.63175032995741\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 57.60113156797513\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.41601644989674\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.32004498113926\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.42388832210168\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.39176483937594\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.35814562816021\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.38457786312129\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.221647118994525\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.139606066654025\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.18890800281764\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.14220055818592\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17268693572933\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.21281388261376\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.165830574344554\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.1598344537875\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19087405896045\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.19224789847042\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.18687594799153\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.199931717624565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 77.31425113773089\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 80.22842108723209\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 80.16985598770101\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 73.63529041438072\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 77.94256802212473\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 76.20581868694438\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 80.82036691101567\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.78911171793526\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.4453835408716\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.80010432779994\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 80.8543003917792\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.12190413685379\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 80.91634462837246\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 80.08972017253822\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 80.68242418752304\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 80.37584251400882\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 79.04411252998145\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 80.64543978300047\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.79556118906984\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.1165681658552\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.79034763572314\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.05710464256405\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.924478580451435\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.07277286180738\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.16880072013836\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.170828940051756\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.2286879069986\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21209650361817\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.1689540463763\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.18197757224465\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.142691362889266\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.13745144208215\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.142422262693515\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.17011802980899\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.1685439998882\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.17057647304499\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 74.98227270526542\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 77.9099273608478\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 81.20262494615818\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 81.04585420778989\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 78.84645035498252\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 72.70171073281449\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.89478016375215\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.56021558163859\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.56901909009132\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.08318792561896\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.39998506576018\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.50289752669018\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 82.02481243132081\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.78258828125469\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 81.63168086647119\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.22513532839747\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.09690201835794\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.38466742324975\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.729984288385744\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 49.068080575977376\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.67777012754411\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 49.01045309797667\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.9751840561434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 48.95320829505613\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.30552207143269\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.2456823157416\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.26810157005249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.23687774994946\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.24657525659453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.265483759175005\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.14936847528668\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17580203045378\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.140933376670134\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.20061992651111\n",
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.1549557184834\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.185744493276644\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 80.52673922949565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 81.38479086462912\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 81.34659896282844\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.25846050980908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 81.19471419026763\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.15059666986616\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 81.37015377464631\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.88904029837411\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 82.66878310226599\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.6499661576213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.43825007088577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.75935723583132\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.65855291532655\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.6229673765334\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.36654842992738\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 85.28021814999656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.50735359938153\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.06047784829441\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 66.24023129119698\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 66.53955195111976\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 66.1530607396906\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 66.43622294851404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 66.04592847513156\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 66.26973554189057\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.491844610345744\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.44394997467609\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.41315178939516\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.41663349240641\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.43570222722464\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.505680239973366\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.18634120760451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19392240800373\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.21107352007571\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.22629934964301\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.16051574509075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.1241830152527\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 10.900420952355406\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -13.772680165898453\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 47.20399985607895\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 6.502882522049658\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 56.595867623028106\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -53.46734672110338\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.63982684641083\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.74086106720902\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.023800201648314\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.76032181943817\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 46.47075641447322\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.97040906242703\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.21449119920403\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.266602188620276\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.23801563467686\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.26092079879254\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.23550744699675\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.211272472951535\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.14062788182963\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.16544777009205\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.13207773464734\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.21599890920718\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.226070066413115\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21358275756748\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16296967223149\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.14449707840973\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.12923491076292\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.155195920543456\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.12497653813171\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.13740369477775\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.106948536470966\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.13410295875286\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.23031295959148\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18524360044216\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15098272111355\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15707513483582\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -14.853135029566356\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -15.353604178799962\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 8.790442397597186\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -6.857703283549177\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -9.231862279250326\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 42.8318829511517\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.14757843222432\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 78.65486928590833\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.69719779322236\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.31907598351353\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 78.412446664767\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.96219986775482\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 51.151135366311415\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.608309943253246\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 48.246472821434104\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 49.46113795476441\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 48.91619049689987\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 49.000352717675966\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.30330614371516\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.235003239697036\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.23028481712027\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.21399704913427\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.2725757996287\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.281830886939744\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.21616725291804\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18817888533159\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.170379008120406\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19171635607078\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.121066276574204\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16590223279876\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.205005043170885\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.130539363872444\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14501514859243\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.128031291987845\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.135955292450014\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.198830929079314\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 75.63385488980568\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 66.21136407080309\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 74.77908244570142\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 65.12526415571382\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 55.09110379411106\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 62.01353450273251\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.43585706241844\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.50432490095308\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.73807451630809\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.57453465946433\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.32590676524605\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.5056001047179\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 75.40504046587547\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.9009002726518\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 77.28708710577189\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 75.70292692692928\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 77.66929736208724\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 78.06516129347753\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.56077647708445\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.43751915630294\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.52465315944632\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.64450873971545\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.51979427858201\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.69954175804286\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.171694594646524\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.165206117546205\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20346201419698\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.18044802377206\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.20937137206425\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19932797459416\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18858964947565\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18428648575872\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.165808126946615\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.19727322230446\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.12945620370954\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19178562545216\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 78.59766288145686\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 78.54612593363655\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 74.72293562938484\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 76.77395919021355\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 77.2221270035545\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 81.0533385684676\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.41718236751652\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.49627620980893\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.08604224321712\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.5285696070634\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.44041697824417\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.85056206838904\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.31112195634275\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.57319712007828\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.83185514838567\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.87114610770767\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.58589623374046\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.2365210693734\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.3559761025929\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.67172334017678\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.91758723838786\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.35948729075529\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.966917462079664\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.778417153471864\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.189517924951836\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.253088227135606\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.17299701297877\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.19802015766682\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.17678593591179\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.19200728559601\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.11714178795872\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.19851181769188\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1850373532743\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.1306890633413\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.195641488081186\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15632100681955\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 80.52797355549428\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 78.77766661503144\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 80.30985493105767\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.94321344783125\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 80.4608669288322\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 78.38474140525733\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.41076209328392\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.25096654654189\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.10696255077015\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.73434827370711\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.34827595027782\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.37979371400459\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 82.88092672390457\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.53780306917432\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.9059643849679\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.09881911956354\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.05915492313504\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 82.80839753416154\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 59.637323671435325\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 59.64958505151279\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 59.207539596821\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 59.39029003131604\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 59.88271302814725\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 58.72677789687379\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.29674704244848\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.26082650540767\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.29254851067396\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.267479073432725\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.26096627616339\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.2959661376793\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.20935583012311\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17239368859518\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.14712317359084\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.186773621599365\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.21754852403562\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14069581260718\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 79.49231249096296\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.18021562705792\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 80.77079114485939\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.48484162878249\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 80.95816700404569\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.71072372184844\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.96166913464556\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.28708205316366\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.41925479828922\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 82.49165245249301\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.84671644529804\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.5496833158902\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.78526502886538\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.67976710674722\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.27197601953411\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.51569032614195\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.07569160857418\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.5585226725014\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.21891198781975\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 81.20901565420064\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 81.42233572748826\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 81.23922169871562\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 81.20274466575678\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.24255193893966\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.90158861179775\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.99126121885918\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.93504660345948\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.85805702913092\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.85068613382954\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.68386255194389\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.1452448225384\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.12212918990581\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.180967847405284\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.232504444631935\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.22478610144998\n",
            "Layers: 2, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.1341752711102\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -134.90820056138298\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -84.50926976410604\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -126.55757194375643\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -545.4514122228766\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -1631.4437907181987\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -79.26592558702382\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 50.041760939220204\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 51.427186952041495\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 43.69037381067192\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 55.63008872494187\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 48.175038987757105\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 39.770947073353724\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.439896229861745\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.50812699532298\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.464276111087074\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.42483727663104\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.47721490029589\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.528735520456046\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.192850426969734\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.16722783703434\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.17613156505378\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.148567426428166\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.24041177977146\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.202772008104375\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.17091569513772\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.2029956023287\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.177919101362995\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.165597213450766\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16118281951733\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1486196517437\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16263827500063\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.153662101251314\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.141082169830305\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.19481387034367\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.10786452155244\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.10288371967089\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -324.07280044350813\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -216.49040946477575\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -301.1325452809687\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 44.01239120278909\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -305.2182517657117\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -316.3532339900126\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.42874760439354\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 81.8036930360783\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 77.42751840709222\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 80.86839762966798\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.54874771565919\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.88180572720475\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 61.67258080139351\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 61.37268710465646\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 59.98562186304801\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 61.11172532687756\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 63.06069093323047\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 61.807281285993064\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.481840906343926\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.616990048902714\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.440513220586034\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.533553338114864\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.60469737953046\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.49677925362274\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.19725720473505\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.157293251629504\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.21675587180569\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.23301271836752\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18783542527637\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.236847465054076\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.13404355616972\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.15291377342952\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.14774759825551\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.17893077997118\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.1442060006213\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.17300993224123\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -143.02695420312386\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -254.98932671995055\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -11.170394110597325\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -101.92346849430555\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -161.00818751824968\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -149.41511782641425\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 84.06912045685505\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.93043869027335\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.2440467749604\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 83.72229891807402\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.15379654387198\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 83.40516736052894\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.15674247200296\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.0624655909039\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 79.71235033100393\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.23880540432945\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.73993621770569\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.57273673499093\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 46.63811610343587\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 46.84735139805465\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 46.709345090444124\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 46.99444822689834\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.74395765376174\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 46.937328945369515\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18315496529753\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.181313638518574\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.230463863628025\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.221409582238934\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.29875251402743\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.26266871008051\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.207331245790094\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18213040970257\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.14257053077638\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.177137490524835\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.188200356521634\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.13505917398304\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.11740153275737\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 55.37981440840802\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 73.15585320417479\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 41.21623777821918\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 32.19900185457354\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 26.50194158945497\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.64476299395632\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.9697558348509\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.96260149596887\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.54952226162506\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.81932658439493\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.32569142068351\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.07468993754327\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.59876069960394\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.12946609486828\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.27626697121792\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.59762172457538\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.73877273540549\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 53.50975809378051\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 52.95537933953018\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 53.52080066724441\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 52.92658809756794\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 53.318651816055706\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 53.60795369150998\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.3181045742489\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.337587213097265\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.33432072030661\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.334704947834005\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.32715394281317\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.312303266176144\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.198627416777526\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.18970446918684\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.234811207930456\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.21472319106078\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20287629921385\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.187073384629905\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.74190512735083\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 80.04933709882437\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 77.1927452672703\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.84678902898567\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 79.26793657777682\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 78.59081780448653\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 84.54036291229986\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.54178091919098\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.77595149012055\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.7822462882308\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.21240426591406\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.93960999605707\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.74614025287167\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.0800358099548\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.83405383629135\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.42162343337246\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.90715006077293\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.15850004657824\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 77.56473608386439\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 77.6221646209032\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 77.56116363981194\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 77.49368833955971\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 77.97372173888041\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 77.75969368221662\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.657601952667115\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.56144802379206\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.652493802486916\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.631068171465316\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.61803255597431\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.668724820469365\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.21088353123503\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.200028341094914\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.18428555656172\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16922438768353\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.190720394336616\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.19480356310345\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 71.15047238272729\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.47075445440751\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 79.48089448894447\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 78.8832509824038\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 78.77031525296181\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.66010830143561\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.85679120435813\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.82208069878882\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.10380871157835\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.50422489675105\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.86926711250366\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.59295605561448\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.88245154213621\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.30053314396281\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.05321866376765\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.87635799418202\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.56338150271704\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.25552162577195\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.39311858727002\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.35130337055588\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.37652213371345\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.27388571010131\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.2390241033966\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.25231505299968\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 48.071464126623134\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 48.32631245809905\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 48.548246953146275\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 48.425275269805766\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 48.28046195834909\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 48.48167770808347\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.24306449809086\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.28018277448555\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.2423215136475\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.211365970696846\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.26695928041578\n",
            "Layers: 2, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.26941687943242\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -31873.736172856436\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -10451.069237606982\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -17387.775668198003\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -10975.43478008427\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -542.7774924632\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -888.4872930517222\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 26.53931299479565\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.80963006389292\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 60.11995742272545\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 57.87233520020549\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 67.08122168732275\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 71.17237429258509\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.461536481845044\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.33104187056942\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.322261818502085\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.337413720226415\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.24084339924313\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.29933956139129\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17245740954105\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.174915880626784\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.15745138901307\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.212536187042986\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.22186383297778\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.19943842340523\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.17740182606785\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.168306559290684\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.19829085814053\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.11701266913263\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.13819136958151\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.15457662834675\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.153406396287444\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16147221394646\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14270490569844\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.172810081610805\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.147148955914886\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1680336129332\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -4661.046266621632\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -18919.596791152166\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -1276.0952802101312\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -526.8446161374765\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1086.1749583055198\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -10259.92878930341\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 70.241084695091\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 69.20765386946502\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 51.247878421253745\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 71.53369558237044\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 54.34632823894341\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 63.44142816329212\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 76.01745043854444\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 78.34274855801371\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 76.7324789374278\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 73.98589941382545\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 75.14198668395416\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 77.30081218478777\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.505889958413015\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.414596623674996\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.42391408382017\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.340797031947744\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.35446638845144\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.46060329636808\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.1840993907886\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20101989301848\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.186990382948956\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.153670095615595\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.17275523372912\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.20549901444958\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.12207693395344\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16676006455809\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16853672980271\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.190791096156055\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16944301528147\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.202749551005596\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -736.0588441555374\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -643.2826872643814\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -1085.2305756961425\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -854.0486824553635\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -2533.8113252824874\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -630.6264095010088\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.46083532991106\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 75.9728671217678\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.09712371709112\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 72.42292343918555\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 75.49247485081446\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 77.04537594688775\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 83.13567657053676\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 83.4659489535444\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 82.92311875903383\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 82.2265447452693\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 83.06120154406771\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 82.02559003072723\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 47.2527168036696\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 48.16827880888091\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 47.774374772179016\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 47.834504888370475\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 47.3729775935743\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 47.43099066355606\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.24061944410719\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.206114521502336\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.18593424706108\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20586309366866\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.22682608442833\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.21011693338465\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17233312508022\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.19964958613972\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.13245316208735\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.177324456411924\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17943127410709\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.1242424076831\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -96.07665866373323\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 1.419761885982107\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -121.57961730898501\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -49.9483093284399\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -115.60604239314918\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -206.0161006921637\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.49427037537212\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 83.94669210586139\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.72925161135622\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.30299634315247\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.45471025546667\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.3067643473237\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.10067922162699\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.28848403854575\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.13592422955055\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.26829901864623\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.44777133577301\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.08329212743293\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 65.43882784964164\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 64.73470831356335\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 62.771555593199416\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 65.2305902939077\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 66.63870735784852\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 66.35573558072089\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.255280781450125\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.22339463914131\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.266302602789295\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.33734365811877\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.28355138345632\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.328074388454645\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.157330348994115\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.206191032927734\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.18865319731955\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16915992115981\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.13262044801296\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15683172945481\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 75.57740984449543\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 58.93280919685554\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 51.454599592298855\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -74.4828964660668\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 70.57392097315113\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 50.70920555720533\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 84.45354500857633\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 84.41844600711144\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.89747230111296\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.36718567313022\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.27451971439643\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 82.98328297230582\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.87317430849592\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.83142836394784\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.51554590758575\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.99057735827951\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.97355263288972\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.772193252423\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 82.48021470846506\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 82.12895779304226\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 82.21177771761813\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 81.4909314377627\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 82.39996917045661\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 82.02306046447491\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.50404157996818\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.91225057143017\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.709831806769266\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.573545896291364\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.7160860030277\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.67729084066243\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19306411904701\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.20086886749382\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.1847967303472\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.208651050467985\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20654143726273\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14888261631703\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 60.99990223942016\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 32.654332701255406\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 75.21765600642712\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 63.055858429154085\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 77.09245038663559\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.2766989300797\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 84.21770150148416\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 84.64040881790875\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 84.96141836292954\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 84.96564261429512\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 85.1833896724758\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 85.17083406265458\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.02154740676325\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.08532569542852\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.0389738743178\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.81620042023185\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.90928046384167\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.28968463220218\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.85671174338736\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.97232820213092\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.9911380150696\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.97985025617324\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.67410644060082\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.79331361026566\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 53.46379231460075\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 53.19877051145673\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 53.357783780502174\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 52.682663604794314\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 52.51236704422864\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 53.76014866474994\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.19748961758033\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.22672746655587\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.16817365485445\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.243599786211306\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19891628180675\n",
            "Layers: 2, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.222042244675066\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -131.41144986988533\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -223.49017167758979\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -85.98146948541863\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 54.074884611480215\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -85.18152117683462\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 54.15679852625031\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.042059553872136\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.5703282146377\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.06390707466307\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.54255545226194\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.047272783428674\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.88744353164326\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.23672631329785\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.20516214970146\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.279565508001184\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.266955208746374\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.26032091004436\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.26842453440213\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.1905936229864\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.164019751253\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.159777839942095\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.188943698643605\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.176479139373825\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18741249868101\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.177691913332026\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16934156856537\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.1850490994097\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16104928847998\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.20028146466088\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.17402341875123\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17343842541448\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15197343076177\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.19827986556907\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.152466527718985\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16140266420911\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.138365132359084\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -15.592385977220836\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -44.08536758100936\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 19.762799431640254\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 3.4520321382523367\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 40.728213445958296\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 56.13780947540015\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.1423611832982\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 79.56098466528574\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 77.6403513748526\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.9511268419647\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.97267583123468\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 79.47140072148377\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 48.64305486247418\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 48.245979347675295\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 48.200421272953854\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 48.32114120987489\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 48.21723974020855\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 48.46310854689756\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.30141306657374\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.29867142553204\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.258575529431674\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.268883807766834\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.30946420792149\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.25356376244275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.17473952538228\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17612784904813\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20535022283268\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.18928435504991\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.20342129366605\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17199164723722\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.19672544625215\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.20509183486852\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.19374868342039\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.144738041687546\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16400331728829\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.14657789434041\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 70.35487423008962\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 55.54492978607048\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 75.37451206700392\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 57.04435468497384\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 66.52112494769753\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 55.87787098476702\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.22952883024988\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.97876754719772\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.73722705102122\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.84249332575767\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.98151406777238\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.17723693525741\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 64.3615908123407\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 65.92461080810874\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 65.34336350546698\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 65.51449614372298\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 66.49232222743998\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 64.79818239493711\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.53021695223356\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.47934812949816\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.56165877994012\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.55929890070375\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.537746683774806\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.51211626126385\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.17872222141932\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.20315447998689\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.19319204841486\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.18747873457115\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17893167630576\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.2213439790194\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17065709323551\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.17708882700908\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.18120643329111\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.174880518698025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.15520993745549\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15721687630868\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 80.27575107657869\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 77.17156467284103\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 71.34420666330657\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 78.03980517487989\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 47.46073671259732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 72.76735411871884\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.38333184901077\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.95848252062693\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.57831553000804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.31741833147255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.02000616590868\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.09201558869965\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.3118347750343\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.62486097498967\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.57162868908003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.53771625719264\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.05736214711284\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.72452255003198\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.657472737319274\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.706400304449716\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.793845650515976\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.68725773069106\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.60548064824855\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.80928405007047\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.18815755637632\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.177618454338706\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.200427510094364\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.22960071010772\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.20542893930269\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.1995668481531\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.15176154244413\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17069588964758\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1571594781875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18794733650145\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.14537873271729\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15624075939919\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 79.83260748438511\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 77.70814598223969\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 79.54531316607219\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 79.82353643205919\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 80.92643360274022\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 64.2228090749374\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.25861860391596\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.9105181531244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.00702785192708\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.23386851150869\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.44805205037505\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.12679872414049\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.19384204980098\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.42746923378614\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.52331803866534\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.39722631532752\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.40103971944443\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.46150055487676\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 52.10666390365232\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 51.96608948533819\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 52.21363829615293\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 52.22760403892479\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 52.33198745524147\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 51.86713279231006\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.22395471640369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.270708413943964\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.27856826565026\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.27033686074869\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.288369339523896\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.220344969649105\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.179160197498\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.206055668310206\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.195999654637966\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.158766274550146\n",
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.15154511125141\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.18326283870161\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 79.27884459266912\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 78.09525100947981\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 80.15051098038147\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.30511629128758\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 80.67571757806414\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 80.69732364585768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 81.60299601114066\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 84.64795877158893\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 82.42067548476008\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.83461354105327\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.41251072592785\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.59905798297117\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.10841536473899\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.36133622414407\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.88697285097955\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 85.27785061966837\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.33710075952013\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.29399660386296\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 76.92542641291821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 76.88000258697053\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 77.09158469032957\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 76.54582064957467\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 76.44077875351695\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 77.062033260742\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.69808122112141\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.8148780699475\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.81612852601013\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.73019873149376\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.798457410082214\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.79049367922039\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.190775907406596\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.168516118432464\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.193599486808566\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.209240997913916\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.181835127775074\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.208893235134106\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 49.21965798839647\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -27.308249131536687\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -128.5225014765234\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 17.46396890032277\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -111.21317939187634\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -155.64972139017584\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.75188732419861\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.44152273033079\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.67826624316963\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.467380065443784\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.419680174507306\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 44.78667108912919\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.28591329442272\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.25499629813466\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.195263371407044\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.3257371517885\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.23503681905314\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.25509340294015\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18811540478625\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.16445252592103\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.21206663592101\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.18773656332022\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.158357147351026\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.196612969328086\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18759263246411\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19516510591354\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.14690508469754\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.1733174793155\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16066349182945\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.147201954658634\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.1804477546977\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16322904437624\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.156180562143724\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.1347418589461\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.178987367513066\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.20771768304909\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -457.6432219162544\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -126.28397692104936\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -36.76725793259783\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -386.4251392373281\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -214.04244167026448\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -428.35912376699383\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 77.33216396546885\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 78.08555742767884\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.22190306458292\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 79.23832711196741\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 76.1269357664447\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.11898189657347\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 60.51156835076815\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 57.66208506654769\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 57.12968705975785\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 57.56316582156087\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 62.41418848629422\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 57.86706793843259\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.32610990082409\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.333089253589755\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.33659174097012\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.30032878141817\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.35422003996249\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.334885358999045\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.17091371351407\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.16561680279227\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.14220428718794\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.210939433067\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.1770008243072\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.1898679543643\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.13996843851983\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.190067004957555\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.187628455599395\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.134542540693175\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.165105351619495\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.14519967578886\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -62.64305927033711\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 3.1058791774783323\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -0.15825633029622477\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 10.214074840479814\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 14.5371324585277\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 54.45127192703206\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 82.52375315010296\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.65790777605157\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.98447655588903\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.4695827632651\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.85344986162013\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 82.73813238215254\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.50563369073491\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.31623694189764\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 79.5321448717165\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 79.06413162262173\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.71780950555775\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.85979821308618\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 46.24682246894216\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 46.062290178066014\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.90591743049016\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 46.34250043548064\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.1755387535541\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 46.096651917937926\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18475573194757\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.20416440029326\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20273227671516\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.18083003101299\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.178298897386426\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.18377712815585\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.180809636629604\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.16789105968741\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19749159054809\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.159900211340606\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.12947723178009\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15866474304555\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 50.686977003834286\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 64.19068390261317\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 63.31911016068423\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 58.67418151090188\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 64.90604724013397\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 77.80629877756965\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.61968400407945\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.65799491122895\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.0352779298072\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.81469712140324\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.68392118129138\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.25702075927157\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 82.01179826675725\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.26040175649733\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.26726965470748\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.32210255876372\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.04004344420731\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.10421802899734\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 51.1466828509908\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 52.98585569954005\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 52.309049142758234\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 52.2397058233933\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 52.77716249030328\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 52.18303999751749\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.24621309184976\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.24779396750016\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.24027938044644\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.23819223428013\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.22808922496154\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.225908215299505\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.141988886358796\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.1438570413617\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.15370782282551\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.14186584808313\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.14592901805696\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.17584282705256\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 80.26523715195914\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 80.32902998900478\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 79.89793597485857\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 79.01751637138715\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.7829599179296\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 47.76349864062743\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.88484206453886\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.45929905240637\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.06339738860092\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.3657003480628\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.17616513555517\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.78169516653772\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.27779655416303\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.13798206813682\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.81580506058822\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.84580325202371\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.24936444977942\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.65969950456842\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 77.8055610590892\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 78.16485774805138\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 78.4876686099118\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 78.0083249929448\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 77.6524599512098\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 78.4380000479888\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.47918382489436\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.33763853397962\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.475692646202845\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.36838875905945\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.47044725632091\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.39554339140156\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.178242195335095\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.17369946692346\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.170804031440106\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.20960884065859\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.167696690708425\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14932763781706\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 80.86064843505929\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.72384907086324\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 81.31214483514982\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 80.23719361059776\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 79.52112915966765\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.46590339447009\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.75481891325711\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.82266415057165\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.1660850171527\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.41414517994168\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.85792454065471\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.00984176813893\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.11717343815086\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 85.08602638010508\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.64750093749807\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.09447595036985\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.71094669763457\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.99421519139374\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.54177469195741\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.54606750306837\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.47825012748434\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.17089995050229\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.58469190549978\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.31397373446752\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 48.084776810844446\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 48.098695972880066\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 47.94145745415659\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 48.075444989598004\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 48.1165342062346\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 48.02115165717446\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.23200485173729\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.21878884263597\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.17335694767485\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.22345379339677\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19696376634309\n",
            "Layers: 2, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.2044164601054\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 66.04810052600271\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -274.9469722254687\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -275.3063132546165\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -663.1687652740069\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -684.526428058772\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -1128.4471497354627\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 61.734419340917235\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 67.35915702377922\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.06511988089284\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 44.14529898498631\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 43.50941467614388\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 40.28805484157163\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.88604500799468\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 46.100017282758856\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.7931702967846\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.89276484319155\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.68189066262094\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.714343013002534\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.23404870815304\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.22535807045486\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.248514788191706\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.20833025028765\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.245005110393755\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.273520519901936\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.15832574074601\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.172644274698314\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.17733318049576\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.20249824157885\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.18401946435212\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18477679959041\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16828114145064\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16210565124511\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.186137204745435\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.134733094557646\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17579779776594\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.167594588557094\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -1821.1676896903355\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -1633.684370968595\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -236.8965495277466\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -163.02983763211523\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -234.00565299212266\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -1490.4163332647595\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.07147287581677\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 72.41968412439532\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 78.21847497891045\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.0375822240656\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 80.99336945420717\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 73.12561840904961\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 76.35228350284139\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 80.64461497543505\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 77.05884416852365\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 80.75527299625645\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 78.33171595821081\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 79.45787757212145\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.09624685228763\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 46.22605307398512\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 46.31913116644021\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 46.13207585861\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 46.27877558814093\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 46.36349553274478\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.25038691444053\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.24883852712167\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.238529999519294\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.221433449232194\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.214127120649415\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.21655208050724\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16722315347984\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17520341431754\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.225975477762915\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.176503874699456\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.178633953369996\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.164597408658835\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -164.56338776543444\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -370.0690103610631\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -233.94447733764224\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -519.672294175163\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -605.6014259610544\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -86.28618748470609\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.43553199446151\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.87240837925368\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.50577915807679\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.30258927091036\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.63787954467259\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 78.89207522440195\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.96205276596076\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 81.22416321052461\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.87514426637803\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.73400809474956\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 81.07738893243331\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.92154243690189\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 50.98016465229232\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 50.25078425976248\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 50.6422321888785\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 50.82493981320053\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 51.03386054242027\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 50.92938331109059\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.37120562068201\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.27988927313821\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.31339630164495\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.348377152963025\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.341006539644845\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.346987450607536\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.186731271995185\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.161750748497056\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.17504798396307\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.224375393017304\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.18980100340097\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17558294828549\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -15.642735693799526\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -90.88678627028793\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 5.110186498925106\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -146.2122653716819\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 62.639250065752684\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -113.93351698635085\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.54438825343182\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.97189554493227\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.13095959457438\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.69110618828356\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.39585623322294\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.09403780551993\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.87130713526847\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.53186550659835\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.74265306792499\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.57067863192191\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.50083534956254\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.59891692843068\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 71.4478550155729\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 71.50383779816944\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 70.79564626344008\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 71.0376601014322\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 71.31267963948267\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 71.13806183282378\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.570920408229334\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.59908274299668\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.531254158675736\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.57074128655487\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.62486133860774\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.57280120027216\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.17813391698432\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.192732747406986\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.241162647454956\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18032162898589\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.23643176108142\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19118804752178\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 66.40066616097127\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.49246733719973\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 71.70480364531359\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 75.42943331854161\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 75.86282405260654\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 79.44258924366461\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 84.36737934108159\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.06619699814331\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.31616243275364\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.40403300937584\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.30505641722158\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.72979684858245\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.25889233953889\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.21480714748955\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.02999400177997\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.16005211037422\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.02584618794127\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.0674016335754\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 82.39092653082119\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 82.50056656283988\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 82.57206428174457\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 82.45241700162548\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 82.54225221501487\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 82.431568186723\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 46.908083145971446\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 46.805665864768876\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 46.740342653890565\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 46.60094882388384\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 46.79282157687241\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 46.86067682598394\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.258059137063924\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.202573809285454\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.26914299211718\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.20930773774708\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.242812406110225\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.22608909597331\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 7.444730904260544\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 74.45078041345899\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 78.54164610797983\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 71.13198251435252\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: -838.0781357929449\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.81882706134769\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 81.31349418256582\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 79.81401941810114\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.64413494262651\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.92527030041998\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 80.85764037314438\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.07109206638256\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.0944535852073\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.91027092723313\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.11773857047926\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.66803509749165\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.56953991497436\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.54658004576372\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.8564267687108\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.94512298329379\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.89156400448616\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.9387252004292\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.87735832501153\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.85142090039825\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 56.362594899245956\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 56.10065401949369\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 56.46835368233803\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 55.93035310632569\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 56.34657771641329\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 56.47428464743201\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.31706647165077\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.367668773428804\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.30006779748611\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.35405586334677\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.32960956020972\n",
            "Layers: 2, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.293748988289465\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -14688.849496435378\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -48336.905810228476\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -52292.714274369486\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -39010.12418114092\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -15655.593841400556\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -33670.49065616609\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 37.93169445537908\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 68.12029936599878\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 0.7064252838029361\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 62.44438406970247\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 72.41271438569368\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: -17.23316371143664\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.58098065136355\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.71611737205829\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.53408328107961\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.61849579958107\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.801731149126944\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.483838496854766\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.2183603287089\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.21089460748977\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.23331925915972\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19928559157205\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.21769558157062\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.195486084942104\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16115599599466\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.15241720932662\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.16007738624233\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.123421257553964\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19468930452037\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18373390523046\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.156377846343666\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16675528664196\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.165113095468456\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.15827858267909\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.179338324119755\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18006509709011\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -4737.891458833999\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -17161.329468967448\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -26434.63878822546\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -53838.481404765786\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -17688.822893260494\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -3744.1741231121596\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: -1.3234244048663912\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 38.87608580814269\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 29.232452723730283\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 37.655975327758796\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 62.58918490042209\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: -62.132095293748144\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 80.88196440953882\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 80.05465802169643\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 80.44172407705572\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 79.84031170229369\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 81.05537469788534\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 82.4426721273641\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.018401313248745\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.97202194619554\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.90733719640278\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.965303295432356\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.95848771704537\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.970703544724614\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.206499714747295\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.174500284570726\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.21156860313814\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.17315733594099\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18836272147916\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.181157516835725\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.20404488674934\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17401790226005\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.140702270745145\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.1543912305852\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.168597577875644\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16177904253056\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -2433.6316918869516\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -9662.996356299796\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -2471.835655941102\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -2373.666989612177\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -6120.418699942744\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -4304.331105952524\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 74.50522920078915\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 57.036391705628965\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 64.98084598658502\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.0593816950077\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 49.91000142187097\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.81841152821823\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 81.37630665855569\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 81.8145294211402\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 81.95118746448395\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 82.59186257244208\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 83.31839077491891\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 81.724651502749\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 53.18407076777834\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 52.94522595853699\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 53.84078340371268\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 54.77593707201534\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 54.3950904942949\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 53.1442623973853\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.32250539211419\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.292299261747495\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.296226130947794\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.26630086714983\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.3213950844448\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.268654149835896\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.154756348955935\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.170894469616165\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.18251364036651\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.15684666744379\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17826166238916\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19522161533792\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -1378.8027168701194\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -846.6195316702459\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -1320.7193871334591\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -1094.3685111150826\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -1171.021849607022\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -603.5314307745086\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 79.11462036823467\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.7108070628141\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.75675499773044\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 78.61775126226752\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.06007902293906\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 78.77947475085278\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.57311766038784\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.33181391102535\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.32860232222615\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.3191562165325\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.42540976001607\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.27949612452511\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 80.57236389863249\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 79.08604405325951\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 80.05856743250307\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 79.38462278419497\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 79.4266506004114\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 79.4224553580114\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.52311809311254\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.45023466510063\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.49137511133782\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.45313651431645\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.56003889191137\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.45377046601453\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.195845708820436\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.21349953553248\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17379559790855\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18255302490771\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.203416129287675\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.20591875293343\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -662.6932967975764\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -141.23523835071347\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -527.3664500258826\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -61.21322741396911\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: -213.6418605875045\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -478.9883484637367\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.86047045828788\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.67013128315025\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.25679242478324\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.11871726164472\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.33421345803305\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.71533437537929\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.98755445944929\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.91689407354485\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.97528742207481\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.95138918507543\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.92975647512596\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.82577216803547\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 83.85563847476328\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 83.68488687091647\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 83.80295450220082\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.88141782795006\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 83.97045446896307\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 84.02475951135867\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 46.88977056300453\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 47.349071245473326\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 47.10508431540534\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 46.94764407283737\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 46.88881498646087\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 46.896686445583\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.21556019667632\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.22180110544801\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.21216910091662\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19461898818701\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.205888685934305\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.18689189938594\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 65.80719850691608\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: -79.5484631446572\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: -149.14846411874385\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 37.46681457650492\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: -252.48811650468235\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: -307.2934338339545\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 85.12494687152449\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.5639783107873\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 84.92556062840299\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 84.40769037525455\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 84.30518562052046\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.7005765598123\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.28771138236951\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.94328579758152\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.12625049968582\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.98936807034708\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.18271079390817\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.13814912319732\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.3428748284074\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.0502028068364\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.96592190314126\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.83442560831706\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.8342891418024\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.86241849190347\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 70.9417600123234\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 71.64688849888134\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 70.2957783712022\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 70.49046527758658\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 69.20807925960686\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 71.11047834923865\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.260652027249705\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.32030350602606\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.29144296420458\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.28700154541917\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.27143148768952\n",
            "Layers: 2, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.26101405707684\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 9.05948855477272\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 10.471690214132956\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -82.28261481586159\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -177.24751169945847\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 76.96618806277833\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -82.03658735701806\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.359757586004726\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.86495329586538\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.2715555787498\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 47.353309826874664\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.26567108602784\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 48.025252148709086\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.30303826592048\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.29463951546294\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.26947345251684\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27101825881288\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.28912954029563\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.293943168881945\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.183458095827255\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.183680636917586\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.16954333382459\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.164821913878825\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.182457330594175\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18376083477994\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18487401743175\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.1827818198729\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.15987059259183\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.18606593389397\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.156282313728205\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.1889495905253\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17228906010901\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15298738995281\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16216374425904\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.15867498191556\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17194653900789\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.16336643640566\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -73.8654351682557\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -234.09345269066003\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -54.09200651507567\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -199.78448514422556\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -8.956118050689765\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -111.97092089197578\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.86208757167124\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 79.28959172303347\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 80.08045849875101\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 78.61402008238085\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 74.89616283933276\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 79.97659964903498\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 50.05392499659589\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 50.94910592560938\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 51.034030369245365\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.53624596136669\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 50.900484631241724\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.52347919974787\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.28053551146387\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.305460832232846\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.305569800532176\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.325755727876405\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.293907286920074\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.300391549676775\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18251358067362\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18530580677781\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.19258598500158\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.1953059529845\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18089655148724\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.1725609132757\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.164862301966544\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18335124407369\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.17211792029011\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.167098130621895\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.18610147389218\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.15967476927244\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 50.040710765281126\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 43.53430316692391\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 58.94737139029844\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 62.03937187385047\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -22.688667800721873\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 34.434763890461404\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.50375453713391\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.35238420217942\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.74083207528803\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.69251567868379\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.83598086053858\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.78786271991152\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 76.6019093638231\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.14468029096842\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 75.9695654643373\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 77.2491063072986\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 76.35146004604022\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 75.99393223454132\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.729137025301966\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.82351555895613\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.83288094048121\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.74069867268531\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.82699356611359\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.85363584960335\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.1982334030397\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.17868868177839\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20918832879654\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.21069528364248\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.217971113629396\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.21219629521428\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17581057247156\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.15337316953089\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.1838531384997\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17638888570155\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19031302678342\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.164784129836185\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 66.30026995472676\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 75.7076092519932\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 78.02464679963845\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 78.87048536212133\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 71.64807666797587\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 79.02722667998448\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.2980032417113\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.75725256770244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.37666165915586\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.17997543282284\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.66643539607227\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.69739802147997\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.10608477166296\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.11718046029591\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.34432079255191\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.33389050301163\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.46852758427346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.7143965217406\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 47.825191374889584\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 47.81503721905889\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.92970636131483\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 47.9995313319933\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 48.06050124093404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 48.01529383540474\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.21820107972915\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.25068508164609\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.19963686114751\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.2322799987065\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.235842754775\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.23978566578695\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.170899211601615\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17212162957029\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.171663845525956\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.160835005130764\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16392671579218\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.16016387477828\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 30.73760340302485\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 77.40295440152912\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 65.75694234516595\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 79.01439299471862\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.72513316059002\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 78.0888955658041\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.96987784224358\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.98174999370215\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.88663654159119\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.85993479914167\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.80274156249251\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 82.6349418868404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.93477310748406\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.29230300228761\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.95687251718373\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.79141683545568\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.00089578149326\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.28311709384604\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 58.08136388722352\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 58.29260182280078\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 58.06113744998371\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 57.982697122893576\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 58.16005133558244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 58.20250803453967\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.350630248660636\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.336188990915694\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.337249164694235\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.3533476261307\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.35385649969946\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.3523959550402\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19674635341867\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16536299925544\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.169529557821285\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16466902663069\n",
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.16402677519267\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.18200865806672\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 78.58635868384182\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.30533300764861\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 79.20425586239516\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.08206922869871\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 53.19238063518142\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.55768025730778\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 80.99642941994735\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.80420163311535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.315196957312\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 82.60944214276897\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.38724839883972\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.14873369800024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.3338401773948\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 85.30767242159476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.49506658313372\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.42235556881593\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 85.18363330023107\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.81319570596254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.83483987097819\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.14532304550892\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 81.97264897032264\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.31553731405465\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.07046525250645\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.99747244699915\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 46.318512633700095\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 46.283464284980255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.23745140532767\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 46.268804243436755\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.32677687462784\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.25693877217959\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.177722653687724\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.204146411962945\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.220675396425406\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.19129066025366\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.175074356117605\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.20091786807725\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -65.26446417597091\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 23.886680756668778\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -4.244147053064862\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 56.31042601918643\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -54.44945397759289\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -200.47094089023722\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.60309032584841\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.90007698829712\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.49619579135708\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 43.987253752558864\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 48.288149338717155\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.026739332974984\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.32115595381296\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.39936093442031\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.38156370987804\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.396900408350064\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.41695104605412\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.364050201051384\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18895485728485\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18172960753771\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.180266481948536\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.186172930069525\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.202032281509176\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.196040546804475\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.1444013271478\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16662862503027\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.19794942200252\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15774934464372\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.15542653858509\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.160952041250354\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.14355856869341\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16151103410601\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.17233861076221\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.174880404286824\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16432520728751\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.173733504948125\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -1153.5680339507412\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -1392.3732107116382\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -2021.0346268601559\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -1128.3296382422643\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -419.6446102114532\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -791.9468301998778\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 75.79696669540141\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 75.90352238573514\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.61094898120352\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 65.71499269768613\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 76.2195388718405\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 64.73694876322217\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 77.3320838067099\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 78.81653963662126\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 77.73354330600611\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 79.02225721371691\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 78.17368113541905\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 79.22647822935507\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.56680538321838\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.49732935577776\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.574171874998335\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.50062060835197\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.489248679847236\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.5234411729719\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.197567729787394\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17443766641689\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20434447122994\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19494248930462\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.21607020700162\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.18389827995137\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.184878329365965\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16663387648391\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.196499139530275\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16140053573158\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.175395903299375\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.15154428082212\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -248.6228096453129\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -435.58252702602533\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -314.08614971100894\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -226.27766488839436\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -239.93235759639995\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -175.7627163547915\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 79.92166588669495\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.04868901859629\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.7462391727127\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.30198635925129\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 82.42785919842063\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.72331633748888\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.76477910693139\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 80.44990037838343\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.74274434227571\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 81.0410278127895\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.62525856206825\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.37437846297544\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 48.620673829698916\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 47.72559849417468\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 48.404344423282154\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 48.67205117779487\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 48.675137518690406\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 49.027767000376244\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.247635624504\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.248294280806945\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21591291544079\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.2199831699501\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.208417949574674\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.22983920771706\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.169960348500716\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.182106933364665\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16541243792667\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.18569619381443\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19655278171897\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.18772641849166\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 20.06242248533694\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 50.64375064178221\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -14.78714829895933\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -62.75716317399598\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -14.319328272255794\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -9.181994954699245\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 83.31233414370848\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.22072501817242\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.87870887855848\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.02558863076113\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.89661052339507\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.52400204739352\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.20275209578327\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.25761660790599\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.78281910482566\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.25748544910834\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.65122364239078\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.33162959983717\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 69.84914912841857\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 69.47255380042998\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 68.8605843630584\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 68.12813877144855\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 70.0367176011462\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 69.52322573172758\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.30388765866212\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.30733649075206\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.35302515287062\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.34277685148956\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.3320914468116\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.33715914157879\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.16299246266122\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.188164158399516\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17647459276741\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16602002927541\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20979173353216\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.190613946674155\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 74.84212643806624\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 75.19354266044871\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 77.15198208831214\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.42302711210263\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 79.2746024868888\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 74.7364090940346\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.53328967725449\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.41879219271738\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 80.94066507294116\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.0204455581807\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 80.72959480406364\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 82.86728735851702\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.36674614404416\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.97364453058569\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.08979914313677\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.36716699216116\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.16266132736278\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.89471380617776\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 81.9118303277733\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 81.75011347663508\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 81.67344661064426\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 81.66470713481895\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 81.77973223409299\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 81.67370882169652\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 46.050825048085144\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 46.13292833473669\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 46.105404636295\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 46.08106970533051\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.996821251010864\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 46.11975775373758\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.189251913999875\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.1706270305258\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.20784655029306\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.208907020299705\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20092141949923\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.200377695732556\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 80.98550438377927\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.43589559861604\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 79.86731956871158\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 81.84333239380865\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 81.36181994146618\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.43360607131122\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.46141234576405\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.05192419025785\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.43973107442208\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.45172365515636\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 80.98093529199338\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.69728032272347\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.18200838186296\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 82.88632663459123\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.8653742622314\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 82.44002518911768\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.43119461800708\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 82.48453437813315\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.77615629780736\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.70150112127223\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.66191100927894\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.85002517041733\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.03235009496679\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.63732656940537\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 55.47756943397301\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 56.04329474558993\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 55.5225284311293\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 55.48106786136569\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 55.86273631843333\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 55.37324700988957\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.22761488896353\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.25199102978288\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.20722673474095\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.226164968167396\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.25157190102418\n",
            "Layers: 2, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.226623153429735\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -2826.242419185426\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -4331.666172204642\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -2042.9790864102358\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -577.7409348160451\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -4554.117861017213\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -1283.0404174945159\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 65.57565206213842\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 23.738845778371253\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 22.65231058978925\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 76.82061353770536\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 24.294719096111706\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 55.28812402228446\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 46.91178755463884\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 46.60719350420474\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 47.205141015925264\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 46.692542685921\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 46.52397447406079\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 46.934829091129515\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.38208039552488\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.34652874616768\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.29586841771326\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.32042749794818\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.37013391024956\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.36694002816812\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.194672518911716\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.201221148068704\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.181385325269694\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.17843275922906\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.20380023721139\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.227244353406974\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.18135554224002\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16759082779115\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.18942197756203\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.148344791312475\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.21336529238166\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18499286816747\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -839.7024420828577\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -799.7101805445147\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -1550.4145748239391\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -999.3083750611756\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1191.8079028981924\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -1403.5954026129677\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 66.03852044498048\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 34.38703335938802\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 47.02224785859986\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 70.38415116462664\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 68.89706694398477\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 60.50129663089157\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 82.23249990728837\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 82.12627883244622\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 82.3467185317121\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 82.0737510519463\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 82.31509744595145\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 81.86416005009109\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 48.58763250483376\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 48.69700368263733\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 48.530058394367316\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 48.27288762472304\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 48.59092360353122\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 48.54264658303109\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.3144464795851\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.3577454487597\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.33380553783656\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.39084286023522\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.30727933199184\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.316012649756544\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17743892813999\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.197628693863045\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.190079570696106\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.191735516623446\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.18515573307946\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.20541102503611\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -2159.476412241606\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -2451.4809864016383\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -2105.585732878878\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -2698.149370282008\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -1252.016928065175\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -950.878508421119\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 70.89210931487499\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 67.72847886189767\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 64.58872807368287\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 73.06869194431694\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 74.2993443324093\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 68.33891976871938\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 83.1472943631177\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 83.14759286647104\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 82.94506773895627\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 83.13002273228875\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 82.9300419580996\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 83.02985196832408\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 65.46940832152694\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 64.69218897371398\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 64.0701995885459\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 63.72408265623042\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 64.33997503450011\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 65.02223783953234\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.66326738707078\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.73378872107764\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.73651323647256\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.64016590973039\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.6327295270193\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.63208873056113\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.25186779395484\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18466886458757\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.2229351914153\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.216186539333215\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.20625362157718\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.206891458890944\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -478.3824946992828\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -850.3957885227121\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -149.79183654104742\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -1005.6655751876917\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -105.03363392752706\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -655.2669333808029\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 80.740800346046\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.7106536672085\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.72907678642902\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.68748701510414\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 80.65758264700389\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 81.84491448581653\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.92179351041361\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.88079181410684\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 84.08012674190462\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 84.04863533134153\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 84.032104387565\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 84.04556162292398\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 81.50153572412117\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 81.4651547312325\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 81.39444499716339\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 81.44535577292638\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 81.32630891803585\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 81.35633308904858\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 46.62559562777485\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 46.59561049974783\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 46.64780869765623\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 46.585389916175245\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 46.70039540696153\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 46.6292940326518\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.2638080428627\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.30976889268461\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.2785232955805\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.2564634024914\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.249359315960724\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.26077919085569\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 13.09718604923713\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 15.272119197984468\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 58.11855089723904\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 6.333547143310614\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.70659936722733\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 50.74357911092731\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 84.14321706630648\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 84.18660690284206\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.12041365400694\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.69737070330176\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.99583573994346\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 82.66333560400372\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.45424337061512\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.40856633078427\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.37881106840133\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.34865406111003\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.39745024332473\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.46425452559764\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 83.20518403662362\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 83.2182428413394\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 83.20827850309605\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.19826645552997\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 83.19756940996048\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 83.23569971983763\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 50.65616136133144\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 50.5948828407115\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 50.71652637663864\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 50.81859292314571\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 50.76666792050879\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 50.956172340438435\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.36103898828681\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.3705767800765\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.32707437432768\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.342779049859175\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.412754472423714\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.348274878868885\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 81.31260851401683\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 80.81830280331025\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 77.69538384083819\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.01097217738331\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 81.13647508127369\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 75.07817002974299\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 78.38769763376597\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.18839958475887\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.54188986742776\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 82.55098292870589\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.69150390696333\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.50723106905926\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.1630289606267\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.87084691260206\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.73996689550987\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.83202349513164\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.53704111503832\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.86729034391637\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 84.29248113672442\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.27379628639822\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.19909785248376\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 84.37101697025511\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.28414110301507\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 84.22113921571885\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 75.53358806930926\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 75.50541594470192\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 75.62529321689921\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 75.65566702459707\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 75.71916925344698\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 75.5295567902103\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.807019648855565\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.730154212402766\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.80562483134927\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.78017563119353\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.716245231451914\n",
            "Layers: 2, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.77032046661918\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -7423.819865236622\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -154021.14971411103\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -7498.839752349444\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -43522.88708884733\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -60790.22235058257\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -142355.68333153529\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: -130.51182116591522\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: -3.392618927042923\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: -99.58747960344148\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 65.95635688117422\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: -62.81381476094634\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 16.456961110491473\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 46.26632551535089\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 46.47387854706341\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 46.384626832280894\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 46.56959457648043\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.92469773989415\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 46.31887412180971\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.23703755728692\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.257070233676004\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.24929024970733\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.256766406129756\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.26650380426078\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.243769551512656\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.179636764645906\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.14372838868809\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.1753095233034\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.19364814033234\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.169244865602145\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.15765274790736\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.13822413661946\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.17596848131403\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14387797094142\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.163873660297426\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.18475199933118\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.184359828363505\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -132765.2992847883\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -9368.192729810942\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -202013.2320603933\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -124969.84835772116\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -64592.9106407034\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -155836.46150139038\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 27.81224351317707\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 17.729477581961383\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: -332.45354804217146\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: -29.378412635929063\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: -7.484060086004107\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 33.09603120272855\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 73.828122567344\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 74.04026138311517\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 73.510967422181\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 74.63866517330706\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 76.45550076331405\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 76.9742230871513\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 48.09271693824121\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 47.99988423310161\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 47.826300398984266\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 47.73589023877983\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 47.49473661630146\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 47.90239472749547\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.264856635060944\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.25730889271995\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.29363354990251\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.22354842833446\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.25571245981625\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.27074374199931\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16319983717684\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.1785239317902\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.18298081736209\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16324641878384\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14912513138742\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.1707728578908\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -19225.38653231417\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -14157.411947802742\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -18496.82185832595\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -8850.517620705625\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -20529.99223136609\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -23515.517883037395\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: -46.87828149930884\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 65.20933827267416\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 56.63780480457486\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 65.4831878379656\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 58.61281573612387\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 67.1069867998693\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 81.8453410214101\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 81.19348643061115\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 82.72060187968616\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 81.50733555602442\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 82.67395014409868\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 82.4962989388639\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 73.4037099435131\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 71.63271201813774\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 72.27859845278498\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 72.76548889161619\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 71.38945543130421\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 70.75409512068057\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.47898983739324\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.497071192634486\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.4685119485199\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.48297558615708\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.44091437424925\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.43151251940284\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18340087446266\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.20057824416366\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19683057759536\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.1819250155344\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.180569828910734\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.212992235830995\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -4451.394059357169\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -3316.0526324800026\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -1730.2599231557551\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -4218.821005881587\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -7805.003178160814\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -6821.456030138526\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 68.82051048125167\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 72.72519661953132\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 78.8011515565791\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 72.87563457313423\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 66.8034413416906\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 71.02939551846936\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.3157237937819\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.38275831871486\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.35447895238808\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.48756575428942\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.55425941473138\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.40360234329698\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 81.95951528004133\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 81.86680016409724\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 81.91173545601819\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 81.75970946320365\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 81.96148836983087\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 82.05501210520356\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 46.11447601790258\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 46.2028168090971\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 46.29811395229761\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 46.25596713301867\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 46.22905280885452\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 46.232672281589004\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18850145839982\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.21162037212348\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.20017344315972\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.213258506563726\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.189830070610846\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.222598081822596\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -1001.1582099042452\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -1198.1843211690175\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -3340.343978934513\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -1795.7568967676182\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: -2157.9652216025684\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -1267.3478502526702\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.35815222052078\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.50310983043272\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.11860070431602\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.39228019670458\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.10862396483134\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 80.40026888062683\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.91521223532455\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.90159486215326\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.03910649888215\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.75542090503384\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.6625604699021\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.92832461105803\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 84.1309939758687\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 83.8238188513495\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 83.86886173799937\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.83418712754975\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 83.91639544491024\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 83.96965911025328\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 51.807207014078735\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 51.80325627921412\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 52.44242468326541\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 52.307560327732574\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 51.8574232638088\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 51.62437461096525\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.25959711946263\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.274191905466076\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.26032110686886\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.249749643690265\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.26990628315084\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.28360265168162\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: -249.2583551871443\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: -695.8161338481311\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: -413.5813555554681\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: -25.656564791722893\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: -927.2992482018917\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: -1044.8505456611304\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 72.13474313921063\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.79198469126138\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.75586190999218\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 82.97954831490995\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.85422123315283\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 84.03042201022787\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 82.14713256478903\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.74166216391185\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 82.96146053822278\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.38133704858947\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.43754766103541\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.19079390420829\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 84.03207337394454\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.94207354107715\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.06387855506374\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.93877710745534\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.90997326963927\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.95541991499772\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 81.28557202822411\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 81.00626902146773\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 80.88071705053993\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 81.04019386355887\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 80.90591541139568\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 81.0257950092487\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.50598761486193\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.51242681758176\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.55290679822811\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.591425039402864\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.491554861753244\n",
            "Layers: 2, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.60247271372799\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -89.87714538457159\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -32.69048195485187\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -123.62286800710827\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 76.54775797291921\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -182.40440892496466\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -307.8946188578077\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.202852541978764\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 47.623798772206825\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.704572885942774\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 48.929741154880325\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.21818503722589\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.964468511073676\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.30589089654943\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.322429192937285\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.29214833470474\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.3194496979259\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.290222101265044\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.29694023588904\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18238365771996\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19762324794448\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.18133277861215\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17733296815993\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.198168348168046\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18029530468355\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16977393477591\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16150395783528\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.151818917069676\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.1643885734645\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.168212752044056\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.16950360827804\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.17144378535196\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.17844330299735\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16448661802879\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.15995409593307\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17917692441225\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1604668149138\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -266.3931571161514\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -305.1775088820551\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -318.1319987330347\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -459.9157817417696\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -279.5805181833259\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -360.41272732961846\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 79.26151511218295\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 80.32121304949902\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 78.95262172041974\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 79.97004954166508\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 79.71505169855666\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 79.61455971059736\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 55.478867441930205\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 55.238664346862485\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 55.20093422901782\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 53.72818530909073\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 55.74050775285786\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 55.77918297768005\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.37405885455935\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.367091309192034\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.358018691815495\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.37016939976273\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.390188946811286\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.363153744507535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18206877282406\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.1900900566774\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.19733571588541\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19755315977264\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.19333573674843\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.1924836692605\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.15586656734409\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.165875363369956\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.15723294622076\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.189412811926076\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.1898509499913\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.15919762504215\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 49.24572211917808\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -79.27188611368956\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 6.121412261857529\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -69.34192500902769\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -7.184981008485036\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 31.954829252950535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.90058714117087\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.37238768009469\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.23538732729884\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 82.42302561336703\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.88416811712958\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.65583932084236\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 77.59113067404539\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.64978406325949\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 76.98472880841582\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 77.43282037170903\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 77.37560995063153\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 77.83058646450475\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 46.314770141505726\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 46.29867600352087\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 46.28278399455763\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 46.314240368929774\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.34447601502729\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 46.27308995640502\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.21978566996785\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.21952900855498\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.1980427758626\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20801220596476\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.193851109498375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.207526240759336\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1620064470449\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.176959969853534\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.17372527574148\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16466992796661\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.16791148203482\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.179733181326554\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 66.1112591092688\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 72.40562918911412\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 57.464730994748024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 67.80961405787377\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 10.757926396588324\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 32.77984054325456\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.27004826366101\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.7585690377268\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.39266484095577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.12738968834915\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.94173988383257\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.87045178488383\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 84.15500437842964\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.95052795717439\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.68993722208167\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.28130969335442\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.65832015002053\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.61973142825099\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 50.35662076186988\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 50.31234693106536\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 50.38783222311156\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 50.230951263477486\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 50.2929333795475\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 50.410590261438124\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.25894230739637\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.24725945138432\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.25926176517184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.259657994421396\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.24762680633319\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.26692106342425\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.17373906162157\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.176016510043425\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.17317227675931\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16208807596459\n",
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16049528115268\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.167308870283584\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 78.2742205862346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 64.19458761679793\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 78.20914872450805\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.58219490300023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 63.04785877249001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 79.72423088884372\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.41683925135546\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.48039380956254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.23935425030028\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.77011568078969\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.21529148454451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.40616616152876\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.6904365889268\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.38671921055848\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 85.01919005079017\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 85.64511856815116\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.26301821923413\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.88637657516708\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 67.66510712858083\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 67.7815766402209\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 67.38296657309112\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 67.45792495733392\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 67.54739928163235\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 67.67558218923436\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.486888642461075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.43825841997151\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.493281705538614\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.4886155319159\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.467298911319276\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.481229907692644\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.191650888066285\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18412078722546\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.168020059685055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.18804017855208\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.19667585071364\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.19338873764096\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 67.74089276219036\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 77.55759562941222\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 77.37488470454302\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.2726791375983\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 81.55082925050982\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.85021824575175\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.83976865738435\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.94503260628218\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.91767659862242\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.36392823699612\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.24988159220659\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.27314501939078\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.58358350840848\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.65327649908501\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 85.23211371455103\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.06243914336046\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.73151913172082\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.31798303177588\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.68087932438881\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.63096955273404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.71543332265628\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.71534054152208\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.74924636682789\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.08764021287807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 47.469489817697976\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 47.34260762016756\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 47.36535263555371\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 47.377617874043075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 47.39105565455576\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 47.392236155753956\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.2247756624135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.21977759460448\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.209390212752076\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.22007591287935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.19930638161412\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 2, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.21130255947556\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 55.21288193182146\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -67.62573672017697\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -397.5177452561495\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -399.4429611803061\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -290.9767177345472\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -585.3148313989848\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.72945482803114\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.6938114632557\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.54254380093711\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.155980259700215\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 48.26261333832578\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 47.856367420638065\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.50147815945793\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.54626248089585\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.547212624531575\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.76077406525778\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.634599401743095\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.613230934822425\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.22965797763654\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.20573235736843\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20328900593719\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.228414129412954\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.21540997624197\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.192432625451076\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.17830802713413\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.15968231082043\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.17552641423481\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.17305492472774\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19186432279917\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.17425688617552\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.1619334481703\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15662982808208\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.1787382137051\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.19239700267696\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15930868343278\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.174922209145876\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -2229.777141200842\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -1505.4519063629864\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -4734.671341047737\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -3262.4435168497853\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -2358.018931742596\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -3011.8118169072673\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 53.860032024811545\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 9.66355349764222\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 26.234694717828454\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 47.077658296225856\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 7.243295506494551\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 24.923750760800356\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 76.05436808312128\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 75.77280181186794\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 76.95836984338456\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 76.16762816060033\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 76.01582949674582\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 76.05881188077674\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.593507073953965\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 46.372797915059685\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 46.35052968240991\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 46.45605772657417\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 46.317949168352634\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 46.40435667898568\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.24295421165036\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.19703570577038\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.21993952194757\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.22106031873301\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.228395459514026\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.19346673770171\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17159862110858\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17413016226668\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.18052433452904\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.176133378898264\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.178672107058745\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.172984057329714\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -1356.6702631053038\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -1905.2177645028853\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -856.4035689716465\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -1330.970291222564\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -2063.7384239997173\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -1249.541198977758\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.24550435567753\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 65.13486415845112\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 67.21471350221375\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 78.5902985897291\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 70.4160003345224\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 66.743125622877\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 82.53451823959598\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 83.35143369324234\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 82.47780406433914\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 82.34349925866931\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 82.54303435690338\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 82.4815289545297\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 57.81257522417111\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 56.725074388901305\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 58.2828502131231\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 58.10705895802506\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 56.53215868123473\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 57.7070360361821\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.29763534327695\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.31857982588669\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.334059677919505\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.29629627153196\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.297331886732536\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.29143875592153\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18650186762191\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.185234436710296\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.17997803302729\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16532820522446\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17490982211088\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.16852160046033\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -219.75874206322018\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -417.9628340000845\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -409.86198208731713\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -284.91848204480675\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -503.09738895926944\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -357.95903848239095\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 80.43807224336601\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 79.4925035262501\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.24595094833695\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.00979498148789\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 78.74116006874242\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 79.578899510482\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 84.91597940381523\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 84.14841631842519\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 84.74386296969152\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 85.26646289644361\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 84.38037586248738\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 84.98311523006572\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 80.22949798929312\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 80.14711047115114\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 79.98600149502185\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 80.16279338372819\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 80.13441750863342\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 80.26943447492019\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.63140307069411\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.6988143072884\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.725247659866056\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.734026902667836\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.664876690641684\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.698588666258544\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.20154457031326\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.18296222811215\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1845114988496\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18989209476523\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.17425153124399\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.18935335312242\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 52.893206969869524\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 42.39961087132122\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 60.51689987124087\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 39.14208086779807\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 51.83939646796609\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 49.570529875190125\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.90964702266726\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 80.37109804830196\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 80.28745877866228\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.34130965854\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.91789680849698\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.4592511689567\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 85.33688659543593\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 85.0399821326794\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.11253882588487\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.10276168052108\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.92657827334153\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.97352405873852\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 82.75327381431073\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 82.55128740486991\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 82.35612963642433\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 82.25653932978926\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 82.94300922178701\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 82.97912139292644\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 48.35530765229421\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 48.230790282200594\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 48.45565390116155\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 48.34699390186722\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 48.6335282681612\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 48.53351326969405\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.197390521105596\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.23259069329028\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.229897485793835\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.212006902788005\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.2345960339089\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.213678025264194\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 71.79560904581518\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 78.35978231441028\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 72.20907245324312\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.03702933952002\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.86640680179224\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 77.52526841448969\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 79.4741206211197\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 80.92483376377517\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 79.27155917020973\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.42255356543174\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 81.11846524726215\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 80.28884928940721\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 79.32228016771025\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 82.01727875334393\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 80.07874143091618\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 79.39872607788823\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 81.20948004388791\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 79.78223776927067\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 85.12186123554763\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 85.48764805341983\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 85.27457182371712\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 85.30145242564468\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 85.18463943120753\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 84.91122539033367\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 75.02564132968081\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 75.3721223062238\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 74.99886628078875\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 74.78076972424596\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 75.4099474266017\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 75.4331943071524\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.34971619362821\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.35533255694908\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.37756653610112\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.30896450577594\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.336317875862655\n",
            "Layers: 2, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.35632335887397\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 72.17620053600433\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -34.675612308808\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 58.017907707820136\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 52.464270685018136\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 72.32354565420323\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 11.462990453886167\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.674581628308516\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.637495255900106\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.79290432128862\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.89289257340747\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.14722501118155\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.77786069264758\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.254736544530786\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.213748388761154\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.23171064382798\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.17251562843\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.16216750056663\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.25495513850672\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16637727471034\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.238591062843135\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.164059859509024\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.118796199325814\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.20107546190353\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.19437330140863\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.06849091507189\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.12711694587692\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.25890713679777\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.19983625571745\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19611415212833\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.17754532211029\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16363913602741\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16960964156983\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.10439820321849\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.146771648496475\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15933455309913\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.19317245317354\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 58.388637371524155\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 44.33954598485714\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 62.6852084541394\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 48.706523396294834\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 75.0958730143791\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 75.41387501648207\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 70.87510378551886\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 74.87151209952322\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 73.56419743639233\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 71.90882402953399\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.9390665573044\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 73.65320818059327\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.843237838664635\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.46478530150699\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 48.283976869286825\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 46.921700449981216\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.03973923600127\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.02111542567834\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.23923395518078\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.21705214104229\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.20032490015967\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.29626699845408\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.19082326976432\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.25195998364383\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18294868599959\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.205783637667075\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.18117692691995\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.13357417194275\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.268217036618466\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.12623370545414\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.1406949244391\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.21871180755508\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.187804010001166\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.130046201411865\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.17491550652413\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.21053579357629\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 71.27696878296413\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 76.93712756146313\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 67.48709470502344\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 75.4013531519894\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 53.945203692601204\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 68.89745492060888\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.84997344958997\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 76.95027277119368\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.83233795926907\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.30866310813211\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 76.7496700308814\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 75.65984450129197\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 61.27973762837049\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 60.61680512452061\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 59.74060550246332\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 60.51943835692703\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 59.292181195726236\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 59.21522261952823\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.493266649021244\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.35233807017348\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.419158855525296\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.250744610670466\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.350209692908194\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.438630568555915\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.18354975672365\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.17201450037851\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.19281817373641\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.143882081771395\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.16651643277941\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.166149441392875\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.13090255827077\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.145256724401506\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.12348399637963\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.20992605182109\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.24930190007761\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.171285433097054\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.96071942281485\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.15950197747351\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 76.93281761201625\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 76.9426154578194\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.61262281013258\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 75.90371521487747\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.95822844159616\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.95696929884816\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.73611385226936\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.81260865814711\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.96069841051998\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 81.42910787965037\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 76.83514279335178\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 77.426778093169\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 76.817604155663\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 76.77623062514235\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 76.80485531156165\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 78.27439921634792\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.79222410448583\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.48209946709755\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.032119569396166\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.32251185112454\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.3624288466958\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.08052137646128\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.19746333422117\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.31030063341988\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.17959808267005\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.11989438098236\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.210263055342345\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.155074815235466\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.22355654594493\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.11668180347369\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.098006726997454\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16088609714425\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.23300220709013\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.08835596602951\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.94405869226782\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.92824898892817\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.9435121970109\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.93674547717085\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.9338149936619\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.94230507286197\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.71800973655897\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.62280872346433\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 77.3038928226168\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 80.9038101945474\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.93957640103193\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 78.55961953628827\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.09123976092528\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.4494738225787\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.62257288825617\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.19864646639456\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 82.41152915179066\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 82.1137711861877\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 49.41075866232442\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 49.658926303330894\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 50.49399909270784\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 50.169785585172974\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 50.29548986140091\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 50.20951076261741\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.177264826641625\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.19233394972908\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.165814018090614\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.20411610806244\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.2381131436217\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.22030991662507\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.161667438663365\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.112179102805506\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.139051953835775\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.13558799741732\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.14867771079669\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.191573004770945\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93445461025904\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93491348600223\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 75.84336222658497\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93100531621594\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93446069738624\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93445835618347\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 80.9200188534718\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.47339493459242\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93446173629496\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.7048421361504\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.1677152782187\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.9465587163822\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.44875423781114\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.82910027922823\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.89673822722673\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.88996599385416\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 85.26923414112555\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.32082472882784\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 69.31286293471688\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 68.98307457160438\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 69.3530121771458\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 69.68838719879022\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 69.77908286272816\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 69.63626282856205\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.39417959992621\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.43317371977212\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.47927092753656\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.59809405661013\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.574898118596906\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.554868892709734\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20446763715788\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.16816292374277\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.188447753361146\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.18841107323406\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.23574684656896\n",
            "Layers: 3, Neurons: 4, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19259113238251\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -25723.498237004078\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 67.69915783775689\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -103659.01252936803\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -62556.324596807426\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -2583.0501078205816\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 65.064117475904\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 49.029323698964944\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 48.42844805911349\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 47.48379934414804\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 43.940745223103505\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.32768023347873\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 59.46599863203814\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.21999918620306\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.180874631469\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.23573427496454\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.189900756851095\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.27618514117401\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.22543179703642\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18255738017414\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18645986713151\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.13472257347046\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.11148837321064\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.15151908255379\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18028110582246\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.12361820976528\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.169753046453756\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.120695539170356\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15359142396525\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.14076950004961\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.118214890223655\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.210617396438955\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.15128462732918\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.15436211896694\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.143327526133724\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.12006467258994\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.13166478212378\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -133.95178733038455\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -11.201673828874181\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 63.02304211182296\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -170.10668205492422\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 68.45505967924866\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 57.37782365296324\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 80.78050112239602\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.13683394010286\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 51.59786400321256\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 67.54346322958433\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 54.15452383558641\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 50.44316121157348\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 48.05062080508775\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 49.84157883393888\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 51.847262623943855\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 58.3485391234181\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 49.0318143344904\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.28597867712921\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.288596653022616\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.18658921788445\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.13316573853786\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.256165348516376\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24093687045628\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.26917641818821\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.125412108497365\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20003178941767\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.142605309376414\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.10968297602781\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.12593329925909\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17851184991427\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.25229836932352\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.19749461586391\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.194315524428156\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.065814271414915\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.23063333089902\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.081355574376104\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 72.58546513892885\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 76.89781010448132\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 28.40391813862768\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -23.67932470721854\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 18.72841481280867\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -469.1915620804565\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.07744183264155\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 81.6808684173104\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 75.14643333677958\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 68.59473838996375\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.47298494389472\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.08408706600656\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 78.47048953341303\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 78.25787518094863\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 77.13914462419868\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 73.07407543034586\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 78.22388427669992\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.36451552515062\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.914417513380755\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.36360920550131\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.33863162606851\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.35993286417415\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.5764360074847\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.41555927121118\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.1689821251214\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.14315646124621\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.19060338678366\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.18108195090458\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.2729460609613\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.20568763870799\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.10478263074267\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.106111732842045\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.1184313012252\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.13521204941549\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.15754154132766\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.175054200639494\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 73.03900276421129\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 74.42152429367785\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 76.7706081681568\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 72.86466966518552\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 70.82223418849979\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 62.526008421587306\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.8071175865346\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.72839677969432\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 83.16748300456524\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 83.83164055008152\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.56912540484078\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.94597887654483\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.04252117129181\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 84.08596603267715\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 84.33504435674962\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.77236481685954\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 84.37154443226513\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.67023782982514\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.768199247009775\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 48.25922946478153\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 48.807507747656366\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 49.888886495207196\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.58052404915915\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.74037209291498\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.191012187973435\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.15869490204983\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.27138666984653\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.27752145578978\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.183213969587555\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.21327437657905\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.11191405664585\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.221446329419834\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.12167861794472\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.146975162591595\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.102328711189685\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.17445970494207\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.93856687439691\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.95041751605503\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.87912080952799\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.9496366664007\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.91424160201201\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.86558848191918\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.77608010628967\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 84.25701206106311\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.10461048701279\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.49447532042169\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.57394640666111\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.82401059219835\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.11846227466626\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.48347126427034\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.0065824569392\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.75193966088918\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.21990238390163\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.98168336672941\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 77.26150600581869\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 69.1584416171075\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 79.66941098760817\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 78.9087865377323\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 79.89394899433401\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 73.84089898825333\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.24099632914742\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.192576829897035\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.2903689327748\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.2664436604849\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.15910850274103\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.241905255296764\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.1436656064525\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.210125334725205\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.23987421040728\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.11568643781455\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.2653657287605\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20776081413365\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93446198504776\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93436295217055\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93448094879022\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93444126540324\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93445285435696\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.9344560149807\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.21689886657224\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.98861411489378\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 84.2914249652823\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.93445542968\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93445250317654\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.73868629124873\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.75981962630517\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.87779841808967\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.32432452700446\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.07342130487615\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.25029918888679\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 76.93452964580786\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.38460106478371\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.0313153473591\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.23685038734497\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.92239748036948\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.29378244855828\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.47409901328136\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.5765710361259\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 47.145108679358785\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.128912129688416\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.635114821795085\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.198919834672694\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.46357103181974\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.163132398541826\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.229895418394804\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.187636393797206\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.20858813335852\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.13772290913759\n",
            "Layers: 3, Neurons: 4, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.22633503656655\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -43.10013876777068\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 49.0972651564668\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 48.85715614482732\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 76.83817358276694\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 76.74154272105075\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 50.92907300158209\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.10730600302184\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.99269969021172\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.73082728910867\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.13956234763307\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.58456880519525\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.51908833037678\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.14074309617938\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.1351221217722\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.276021579066864\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.2183730724526\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.15393438037753\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.24063662203236\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17747808299962\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18377831537958\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.175471482812746\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19259328346258\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.16533944249584\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18120137061516\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.20337193635605\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.13017336493211\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.19150667200469\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.19351614329794\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.11819619373899\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.21815139298746\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.13482366361675\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.19995095201856\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.17766628341159\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.14564241368022\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.18673187821223\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.145280181369905\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 77.1318678715058\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 62.053080837892125\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 62.04945015916432\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 76.94015813139055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 57.35334028354494\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 64.14933638139205\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 71.19882132205004\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 62.22884996485152\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.51920431273899\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.95704513918544\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 72.54941629366624\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.90621710024806\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.25123154124767\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 46.164529555869095\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.080031592040214\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 45.83229270345368\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.43095115294711\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.274739447159476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.20903428012629\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.24202351214376\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.218392904743546\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.25160677047996\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.17919320246618\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.215558023786784\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.20051000111797\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18097933009195\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.131040419561565\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.10915736294496\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.198826427386784\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17747653722206\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.25019274888068\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.216830386771264\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.142079707532325\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.11800137511731\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.191924452394424\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.12743398424283\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 76.95237967809773\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 73.88436200749521\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 65.55516373576174\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 74.7154404991744\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 76.22650281103535\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 77.61700097930171\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 75.7636848920874\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 74.2906673301167\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.12270669823732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 75.26385750311474\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 74.91830138808976\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 75.53400306716293\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 51.68532786352862\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 51.99954766535786\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 51.2912148262378\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 52.36201512022498\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 53.37648284540444\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 50.58183289049776\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.39314930186127\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.24934314139899\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.43921429396682\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.3963827703321\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.23686742109268\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.161592562100275\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20921011364262\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.24368835240888\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.169834282212065\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.12355756536155\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.22670096525938\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.19642442575572\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.12085584531377\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18142810105667\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.190015341922226\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.127701626516846\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.13364267604455\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19548914376339\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.91483603339566\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.93293183344646\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 76.96016259700072\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 75.95357689310593\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 79.40100357822408\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 76.9297487341585\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.96209727917588\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.9490062097594\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.96135023063657\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.70262729248233\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.95218287073976\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.82468123119546\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 69.99493611009815\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 69.41115455056773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 69.47205539494124\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 70.05124862138274\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 70.90505619729484\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 71.0803389905923\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.6481396916455\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.56831476889125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 45.61916888000227\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.513615825759715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.80952633076534\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.707712731326644\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.20843975486017\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.21545229307481\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.157435085798795\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.24239227630121\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.18264083468255\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.15306758537881\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18233135681261\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.138461181660304\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.254523103372954\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.17194192402499\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.23515355434965\n",
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.183125163553875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.91503685006337\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.94241645558382\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.92078734157549\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.19119418385297\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.91637312081019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.78896548055715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.93240629195435\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.92978417411544\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.92044236534711\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.93198042717025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.93923446836718\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.92569874601433\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 81.39874360640935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 79.73872313153757\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 81.36625059454845\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 77.90148267903271\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.85334432797274\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 79.51130320114059\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 47.43360971005246\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 47.1614186737242\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 47.262514746870444\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 47.14424132203342\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 47.39790583004383\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 47.21726724092742\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.212207092211585\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.23433993683037\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.26493880001804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.185009578266474\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.285918248205014\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.16999863205305\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.2031865033225\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.149598108439505\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.10588027542585\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.248006673427575\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.10802844339945\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.116481113154336\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93445519555972\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93445835618347\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93611668863652\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.47434624722052\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93445461025904\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.9344632727093\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93446303858902\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445578086042\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 80.92779912388914\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.93445578086042\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93446315564915\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.9344611656268\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 81.47453979006387\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 80.4876277690927\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.56735991114715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.18193004009684\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.81089292086537\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 80.88964723470707\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 57.728500933220985\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 57.59402800027008\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 57.8057635584964\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 57.839318686259574\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 57.81136689809807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 57.61722949532471\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.24067736133264\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.35586250865189\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.38872617377968\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.304666123984184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.33301862466138\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.400689427176786\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.22341163323832\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.14115246964492\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.23906711413227\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.18650910098678\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.190071112618824\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 4, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.16486399224379\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 52.045327205972555\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 49.80506775385582\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 48.08914644031357\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 43.68893197179984\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 49.65098293567463\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 46.97289230176942\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.91689100531175\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.312719724492226\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.55601615465872\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.38940816957647\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.33885984143902\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.66433994441733\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.309840916278354\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.148488878817986\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.19614518821914\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.165064110537465\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.267819332299695\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.34703889096497\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.15123385403735\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.219707273026856\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.14801168197706\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.09564210581092\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.1974753078572\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.168103808687164\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.148570179774225\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.173135770175435\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.21102715882365\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.215808114556346\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16555665898737\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.14861442741702\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.11483417896073\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.22024164188512\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.098278330982666\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.15618368999702\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.17469587347512\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.21701373678597\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 71.78625436774789\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 34.76438331384516\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 54.03532684283737\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 76.25301876148593\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 34.22426859171948\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 68.39525094743793\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 75.10099673664428\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 72.21702718167731\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.42206505518286\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.1617709541394\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 75.02085538307368\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.52459888289978\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.70943111078374\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.543123041390004\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 46.79375989299008\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.55259407186215\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 47.766805477969065\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 47.20379180294552\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.21658855902283\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.21232276642642\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.27133081012591\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.17628602354884\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.22829110566977\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.14879625239904\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.10481498712523\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17933910217109\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.179305836947016\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.25771118680498\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.16423510079912\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.15868462857407\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.154014395515595\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18722788055292\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.24484635182483\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14227111777058\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.16645596791516\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18503661285679\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 75.65482851825183\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 77.53673171558096\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 77.33290197842872\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 75.31265173920463\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 54.45039311267428\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 77.06042276198808\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.88700076104308\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 77.78992065763308\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.48454587853728\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.62983098119714\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.24716493221\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.41221317222487\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 59.386688674323906\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 58.44635667314372\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 59.468133485221195\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 59.25918017487257\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.1651148384542\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 60.5072795251891\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.41192717981672\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.35246704067208\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.47053361960658\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.42863689908864\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.39707889925442\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.38696539450021\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.10405656384711\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.176737676070964\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21290194754759\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.1779785559807\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.184456646691764\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.209026195067445\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1552274842569\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.10859067830081\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19308346035377\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.23593560911466\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.214813959145\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.10664503199756\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 77.17755761419076\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 77.02658208796564\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 78.73438503351713\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 77.34436545831105\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.93173512764987\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 78.00062232974118\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.46685255319166\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 77.12055800379032\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.30623198854178\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.76697726701474\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.99952379789305\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 80.9111893291349\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 76.17429123832021\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 77.1365146102876\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 79.1448485956459\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 76.71084352412076\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 76.11349981591123\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.71863267281799\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.698206846952715\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.14484727485545\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.24181396451874\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 45.99690288182392\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.45246931360653\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 45.902551357253195\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.16681160612048\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.124571405488346\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.15303080309558\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21824241390414\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.200056444387705\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.298182207320814\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.16741794753373\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.156058468796964\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.113515998949524\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.13480116952229\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.1312666165735\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.153083115000534\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 78.11482770807862\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 78.69420315325833\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 79.63365619924272\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 80.09032800730773\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 78.48380335729883\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 77.38960650948488\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.57144190938618\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 79.7164562958431\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.36914178243212\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.65488059830747\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.09279250220682\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.75612235645431\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 81.98215400116354\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 81.52551030582242\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.0087232286097\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.24825093423355\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 82.04373054958003\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.00111930124321\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 49.605571917295\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 50.01678071444549\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 50.134426590854005\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 50.54084814779139\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 49.92788464528278\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 49.70302851391241\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.29262802217443\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.227654790923744\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.30656888150036\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.27581921553916\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.27021657762671\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.26260612500346\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.22712647553361\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.22701757238041\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.2558340302711\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.10072402593204\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.19894698568297\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14696291740377\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 77.99052857785651\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.12290270958634\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 79.3124567433523\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.48060098405904\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 78.67978406583481\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 77.08769661830584\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.02776556596658\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.12040226259221\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.823186856099\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.15291167457852\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.6836374192847\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.02385291863\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 82.84511399534318\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.67472502295432\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 82.57020518658122\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.45991504114617\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.05291602014351\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.54893547207357\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 69.01851747472212\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 69.033265748516\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 69.81630283614709\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 69.23739819404125\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 68.90201017506332\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 68.99787291604346\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.664506804556694\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.45583592859958\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.37592784119009\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.57684863475624\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.57199951917173\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.556586047896694\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.19890732238713\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.17372053167288\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.14510164759684\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.17796944345104\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.20261289278209\n",
            "Layers: 3, Neurons: 4, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.22882133453662\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -81.77193650807921\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 1.9797426025484177\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -50.26976462109225\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -131.99503440257004\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 57.57304693118888\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -101.18774648182777\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 48.48070717242013\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.5799806695996\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 43.92160543226675\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 43.97428176574063\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.2535041734822\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.23662493628563\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.28930021815022\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.33087372962753\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.249030193207496\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.42125543495865\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.239656925012795\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.250188813276516\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.167082661372014\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.12224489323563\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.189857754419094\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.144166425270605\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.15890194740436\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.171512396992576\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.152854267943624\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.22595369004464\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.212665308748434\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14366498736413\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.18751462700011\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.16259352043542\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.20869215059415\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.1201155125569\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14479427310839\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.1690818017508\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.1116462727765\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.19330952715202\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -67.88806793833311\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -63.38090282772344\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -146.4610865877925\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 65.0173843377936\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 70.45422166620217\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -109.65518765583045\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 74.63997548596815\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.25213908380964\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.56229791908278\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 77.93529067132772\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 77.72903338491939\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 73.96160345220181\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 51.467556318744734\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 51.64020787631593\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 49.971285334573565\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 52.89049999257546\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 51.6596978772496\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 51.49317008304926\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.4122675269968\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.331389680819164\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.34361221476401\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.33165575599918\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.26870770362039\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.271414546163825\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.219093674803126\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.121922481106324\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.170243785491174\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.20843153841591\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.180098562293104\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.160688530249324\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.153699623900245\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16880275720375\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16602040856853\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.11700342507554\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15017718946941\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.14295047446285\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 59.33591689741562\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 45.682451564962946\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 62.492239498113555\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 43.841235033158924\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 53.23024010813735\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 72.79672796075994\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.54475960916241\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 76.81027648363893\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.93809207847487\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 78.19114322172165\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 76.95303360529681\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.04774875297005\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 77.00587519567773\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.95404769727723\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 77.06280720384815\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 77.10170062511051\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 76.58530498310027\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 76.98573697422673\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.89958777544734\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.95614855208067\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.799979993242566\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.782810546540645\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.910975930788624\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.87597275242112\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.124028482337565\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18880641210066\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20222241711265\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.2527086782556\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.2431618252224\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.24220351296958\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1630267611943\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.12498335519146\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.258986411003896\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.182486277989575\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.1553082281147\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.213505845512636\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 70.74478834215874\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 74.79539286646752\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 71.01568786129442\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 35.49030346389379\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 74.32722559183733\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 72.9766479890192\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.81392574566372\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.67977232762942\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.99583868327431\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.454471990529\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.69896223758161\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.91238132179767\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 80.6595451858339\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 80.39107102107673\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.05979740916882\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.31850349797935\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 80.23299505850525\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.13868463162859\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 48.447722927747584\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 48.07005969418115\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 48.884855012670535\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 48.17896898651563\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.97061173450366\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 48.12924079799908\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.18305321721311\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.26340366924545\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.16799322672054\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21805334506156\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.260058666526206\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.24117248659395\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18229994638514\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.200974671237994\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.204610737204256\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.20986337844879\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.19105456459383\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.1583554716813\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.94961053272476\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.94836381298398\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 74.82530108804121\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 73.57745148278673\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.9512070281596\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.83352278346159\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.31368078122799\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.87103344939612\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.23620698531559\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.0245872837033\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 83.16006566447503\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.93933860799294\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.52714990719004\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.30463468685522\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.74749072813466\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.36970016514611\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.24173269209663\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.04950951889735\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 60.3234939004527\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 60.556929825730464\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 59.31135475018219\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 60.34675591925711\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 60.522724626436506\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 60.44320729485773\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.43844866498842\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.33886560659368\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.34192500183717\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.399784669594425\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.31627413575614\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.304197805323135\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17001987117338\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.1145122436743\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.18103503305243\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19928341334894\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.20082103553623\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.199249592585765\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93443997774172\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93448352411326\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93447568108398\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.8168338706862\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.94655295117037\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93445753676251\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.2166189611484\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.54805575123133\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.4056755932163\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.0362311569173\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 81.67992566251004\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.94386331011702\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.97613226155646\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.82397119707197\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.11895627210914\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.25100031766024\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.72943724459235\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.27215284545805\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.96356930310213\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 81.91288126077421\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 81.97128382961915\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 81.89197713678533\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 81.92685789013896\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.94702150632378\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 46.526956098217774\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 46.182632604731786\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.6229872809638\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 46.494274838106996\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.60289047868215\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.42428072147274\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20033737456646\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.17792481744728\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15310938579024\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.16076133520377\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.14740934387552\n",
            "Layers: 3, Neurons: 8, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.181007455015575\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -13284.948111454736\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -6411.168171685456\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -11500.809501762084\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -230711.92371200133\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -13014.345073809696\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -452286.2891261807\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 35.74099789737603\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 59.34238970028197\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 59.151717545568225\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 66.8757017865051\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 31.502229272061765\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 62.617922987021565\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.22149523719431\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.317929044828865\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.26525659460216\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.27067839187661\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.334017113319845\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.230617441085506\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.143381798987136\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.160366748086325\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.19714885122295\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.16355292625099\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.18634705159115\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.22851987079923\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.14899747558353\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.17627743588929\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.121259593848166\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.11964257281398\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.117258000090054\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.15400309163875\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.176808217309606\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.14011924374759\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.20191559917427\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17707645768242\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.133811033691316\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.14970696217223\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 75.36508809665085\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 66.48373236910429\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -86.12162904260165\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 76.90376448548979\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 74.0735730111576\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -73.94682210754699\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 53.83726286074189\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 51.46639187033257\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 50.674403940163494\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 73.97866244328904\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 70.76043233955596\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 45.78009354825856\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 73.51447475976217\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 73.35418769578894\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 73.18141361832116\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 75.85634420326856\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 76.10682977677124\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 77.4284189836614\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.333734116776725\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.23515761617356\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.184006737970236\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.25147683769865\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.23039603899517\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.265480728643496\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.1726727602817\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.14480982329461\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.169011936556956\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.19809396554887\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.132456143448465\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.1516973201184\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.15837703693957\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.134113034284184\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.197720362140004\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.14166935218772\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.18874531698967\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16796440303259\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -86.81668532137034\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -77.36879806021048\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -93.84176859327053\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -116.98159777162446\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 47.53370490255235\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 25.902026095607976\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 75.80838151118925\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 74.9171760889777\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.9788989969538\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.37114202560115\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 72.67308435999757\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 76.82891389168584\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 80.86287731131146\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 82.00521592578076\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 82.20065549639024\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.93840110699793\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 82.09595629571726\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.70111681503221\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 46.597907921570126\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 46.76335391616016\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 46.0619670303562\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 46.109070501818614\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.21261967122441\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 46.3628063076842\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.160659498390054\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.18196709048235\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20604848804536\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20936343443087\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.18234222100481\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.173176356800624\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.153636315956824\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.20999199108838\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.21449929455493\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.189582919519744\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17268281433221\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.157083695108525\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -3.1245523620301086\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -111.84140842263619\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 74.53463450125638\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 59.30530174966512\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 52.81230726048185\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -66.99991861979167\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.56936019236072\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.20046140775398\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.77174822123607\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.70507274901313\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.01209567194017\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 83.53498143531557\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.88178719292667\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.8278102198002\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 84.07177493450601\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.31212865012242\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.8152922018305\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.42884007751324\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 62.30276209895621\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 64.95989245658731\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 65.71529018225047\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 65.67738917489412\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 63.08041421490059\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 62.56149344852155\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.20666014530996\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.25465588923794\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.21525686300253\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.199448521037155\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.24590473783556\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.23650922951488\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.21738698170886\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.20473438762714\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.19310225097096\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.19276057217466\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.18737542709835\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.166886211899836\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.74242248652217\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.78102889095726\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.9557781095106\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.20812729578344\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.89322140557948\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.95993830977521\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.91540251667124\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.91378805250513\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.92130419135232\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.91603306110764\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.19619668741815\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.98180454957973\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.47740290034169\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.71180531436289\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.93259710949135\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.50356661683533\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.77298321630656\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.94054569226826\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 79.46069910922269\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 82.50369035013343\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 82.75165487259433\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.06888196950311\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 82.1953882108395\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 82.27853011938636\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.449307927991136\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.4669961244108\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.481324344358995\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.39815163247696\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.41616287795483\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.294617924126214\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.21372386461926\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.15039983607748\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.17833984377471\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.1675617321638\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.210125357266996\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.14610325838652\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93442581346494\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93443307119354\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93410647340694\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93431905461857\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93436400571179\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93433076063243\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93446701863373\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445800500307\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 84.20861401135409\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.73238539759757\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93445332259752\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93445859030375\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.21379790253049\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.89357597378876\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.66105310844652\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.35186635227005\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.10211866735085\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.86714496875935\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.94977805163607\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.00268230577241\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 83.52512366020866\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.98991139034773\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.7912761145366\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 84.02865841510335\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 54.4880296681455\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 51.566264688351346\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 50.37703808761439\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 56.00522219380246\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 54.13043874088357\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 56.44915993066414\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.2402183832047\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.16870973508671\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.16875838227127\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.21083103993706\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.22201235855865\n",
            "Layers: 3, Neurons: 8, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19234632823142\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 50.21828480593583\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 76.87165272386768\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 5.10935904470311\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 76.80355780235323\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 51.970944036960056\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -40.84481669880668\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.34650237038075\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.78198417014073\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.7629981830196\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.300916398124855\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.773328462687765\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.27757344247739\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.21298027042575\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.282225748263286\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.25421628006171\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.21026849526645\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.239734537427935\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.215148673213136\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16501892049053\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18333014785975\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.17156280105626\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.201888916536014\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.17702080394057\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.1785607488971\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.13515808282682\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.171904327370385\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.13450782443138\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.18710667801157\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.11292520417511\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.2179387019462\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15210783456306\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.192186416887346\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14898528683949\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.168067769164956\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.202415740273906\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.165710514407756\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 50.814261280991914\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 70.78083771419196\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 72.3471163569349\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: 70.40723688720422\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 67.24712645454247\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 54.424590493784955\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 71.28618085983388\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 71.9317676703684\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 71.78887783177841\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 71.25006043229654\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 71.83942545315385\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 72.19177307550324\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 46.73352729830652\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 47.11797551080829\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 47.2912557124177\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.12407081210389\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.8068868417963\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 46.55787429774818\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.213796435760656\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.215377280760826\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.283360697606255\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.213861454536875\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.21607421810114\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.232227879308816\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.189191892787406\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.16922972974196\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.179772385323545\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.163125998716005\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18575439523973\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17110713508936\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.176524164728136\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.15955112588359\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.12264924854795\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.15586363815063\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.128835717756985\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.164667993889694\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 76.18066633346669\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 75.89237916958848\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 45.50737548523769\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 76.75331680537825\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 68.47457948662013\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 74.43631024757565\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.68959882097496\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.84352485238249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.58913627364052\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.90657348983993\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 76.91703780290949\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.8638603465055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 56.98401648649825\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 57.760916056192194\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 57.4968387982742\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 57.18392518822333\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 57.80300992724994\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 55.87934891657213\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.2990358256841\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.32330132892272\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.399996813659605\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.46909020442729\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.31781378351823\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.38006564164281\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.17240354637313\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.11169857253711\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.194046645127685\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.19000612122143\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17397575783442\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.20153496242948\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.12991760301897\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.12888827658747\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.199042086342665\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.2133020978974\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.128459423483214\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19177946407458\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.90561848396445\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.89663019681461\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 75.7216625264837\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 76.88582010272823\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.83446675641902\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 76.95140333801199\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.95381266978403\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.95256397465339\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.77504641551555\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.94358730572232\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.82849013398423\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.74629144116201\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 77.10844652287841\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 76.95154247861919\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 77.3779980359884\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 76.959435902293\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 77.07036849864379\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 79.00159210275314\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 45.99412934689582\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 45.911455501941234\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.009674687974524\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.09558782222676\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 45.96651531633034\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.02229964929748\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.2344818711338\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.18599642967247\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.25539764920824\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.16189028058483\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.17328240836163\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.2337152989582\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.16845525792894\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.15542165926038\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.14366985953513\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.14448080103991\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.1625465168816\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.2187630328831\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.92691025991846\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.93641668450664\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 78.2917929043201\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 74.23844911423872\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.94194043053034\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 77.017303572151\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.50674756294839\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 80.813134430833\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 80.87496142575785\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 80.75542603592604\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 80.60223650008635\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.92301243532194\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 81.39745089666661\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 81.15336794573363\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.25859079774331\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 81.84458980260337\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.63060681775104\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 81.54844625726896\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 48.76978232753565\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 48.73581873670958\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 48.975086091439934\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 48.88189020584862\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 48.77651197955121\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 49.055779103716986\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.196471604506804\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.246426706935736\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.265120458748086\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.231852515628844\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.24115043533319\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.20952459143229\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.13694717882806\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.15869887818993\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.21848822840624\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.20458573048936\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.16638239239755\n",
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.16668794152413\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 77.37233948058262\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.94603455034672\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.90834545842249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93446444331067\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93445496143946\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93439888963309\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93446221916804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445929266458\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 77.11421915338924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 80.99885445241046\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93446046326596\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.21872488913702\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.59369256736952\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.89671061566656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.21152888196704\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.8152709846804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.83833255628814\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.92223886571078\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 66.9346895572734\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 67.28501962618577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 66.90522080140869\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 66.74623314521301\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 66.77466100232556\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 67.23192612238665\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.47466674068672\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.478155024444\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.442198569010074\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.45687507727981\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.60869943606337\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.46243047298121\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.170510142304266\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.21050711711465\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.23211807604146\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.181688911581254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.200413303956324\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 8, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.150110035796054\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -3.7594900361687777\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 37.89674264332048\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 47.91699424483193\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 68.80233272995855\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 47.40713707231039\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 27.37023656835581\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.807214308731574\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.03511946788112\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.673090917171535\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.434702889422915\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.270908890805764\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.32543430285615\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.254826143294316\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.21993706865207\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.252926859634556\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.2346762508514\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.294785130273176\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.281158256382724\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.103749761941636\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.224141369623425\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20661975480373\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.16136792944948\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.144636793559364\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.19903396462395\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.141155975447454\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.18762003868857\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.21991373256626\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.18178977433765\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.15697464601385\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.182185696409725\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.18264080166651\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.160110122086586\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14101227886166\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18316344221249\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15482988545052\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18230216441541\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -3.693079145465905\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -75.84588538470925\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 39.33738359876369\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -28.71953316378695\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 21.826451107328037\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 59.97575074720072\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.08520548051922\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 78.4281673219225\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 79.70056871093001\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 75.93168194009556\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 78.0176042970863\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 74.96347183370938\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 52.379730705287564\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 51.09907500717345\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 52.59735785639052\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.14513853458688\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 52.15242003041023\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 51.12278761057092\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.21971196280939\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.290570902664214\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.30059714665144\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.25248406584241\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.26066533059431\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.30295423966145\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.1862162249938\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.15279389831041\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.19758326618061\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.13566047218273\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.177368098421766\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.156486838510155\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.20918820502031\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16740810520011\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.17245982455302\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.13925664484827\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.22722411700705\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18636876710423\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 75.76063832881856\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 5.045296047490178\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 47.4390210435909\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 51.26189467090274\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 70.24334125057874\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 74.59070395911202\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.91432624380846\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 82.07297474385953\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.25915167091173\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.7637487996068\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 79.97092041788221\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.57681041903362\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 75.83279771003679\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.10141250791796\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 76.21046924481321\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 76.8239857366728\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 75.95148682822932\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 75.99950753092355\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.624252614684146\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.8608735768581\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.85383533448335\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.63925791486789\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.67150619785517\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.758109147098494\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.197536271132634\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.23000845619155\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.19695436269028\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.15208426484282\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.18003568574637\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.15827472512812\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.13639170779354\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.161825415576864\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19172449237916\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.20762347187925\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.16765109386876\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15487426618277\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 75.50856303107844\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 76.71634040484265\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 77.04909773498466\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 77.33553082722561\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 76.76847901982643\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 76.07034812725317\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.48062449302718\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.11343185079477\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.71391434005544\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 81.5694192967563\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.35047161858064\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.28579997101826\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 75.53591644439288\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 75.5125114841849\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 76.48173620109134\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 76.51392689051187\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 76.45810102383021\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 75.49950202031752\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 48.775426105013466\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 48.441124084035145\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 48.24504026746585\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 48.41049936661832\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 48.307320476264216\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 48.63611923765762\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.18601608320808\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.24050469858566\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.20354056247879\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21488450405427\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.17112944395888\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.21708763467086\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.16080315080859\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.19574445139332\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.13651723175738\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.195702980877996\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.22059420418776\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.15996088324506\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.96843479801788\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 78.52422505720027\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 77.73333942748415\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.68805911433985\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 78.91101989336383\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.81156048703386\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 83.0820699866631\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.28335054907527\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 82.30487957373862\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.95271094292573\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.6426228580687\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.0131419554587\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 76.23616088703304\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.62764596298915\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.49759990354127\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 76.44395553887371\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.6288846911806\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 85.7320690118448\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 59.00799142569018\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 59.07804931037766\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 59.54202245193558\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 60.20001152076618\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 59.41149393351635\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 58.32086019512332\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.37846128459867\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.291715984128764\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.25912704007678\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.33087011946113\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.278280176055134\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.29678227240238\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.20281175143608\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18462083240814\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16651407500342\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.14215569383752\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.21667092766343\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.13879730057516\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 77.46153358485812\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 78.36359050386383\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 80.45843021230401\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 79.79547455981174\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 77.62615508213781\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 77.9515355969463\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.89987760162646\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.34188848041144\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 82.21172401536504\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.17666302890579\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.1305015675172\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.41343075578862\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.81886917190346\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.19470225525487\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.79819645385976\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.54225418345426\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 85.89640754916267\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 76.46750492202766\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.00276734849983\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 80.96971655605303\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 80.88556619636117\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 81.3413500118219\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 80.74034034039812\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.13635403377415\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 46.86441952028994\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 46.72299225119095\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.938971397654505\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 46.78936337391261\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.91143440889315\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.864595137933776\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.2010042714916\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.2028373474956\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15953475751245\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.21826723804122\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.18755959094679\n",
            "Layers: 3, Neurons: 8, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.161913019013035\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 64.09436856918423\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -286.5645658773255\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -708.3124985016303\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -578.1647517500959\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -116.74776405770521\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -480.097086869486\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 43.74589800148756\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 53.21736128704274\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 53.60577473569108\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 40.146753651729064\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.72664722062913\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 41.87722304336495\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.42625775888682\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.65717871293374\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.463190119774985\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.41011445781579\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.60331143064429\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.492010243582826\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.18241399181432\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.152817673446755\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.21621102272127\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19438856007337\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.16773107846378\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21744057807627\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.19128600345502\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.19894397954375\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.14577619476358\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.189794657554\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.176549272673896\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.13969011163953\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.193129542952114\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.23235651788398\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.20966844119512\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17991413959985\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.174753936689946\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18231970204114\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -176.64852702759396\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -675.4894349947257\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 43.4505855602737\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -769.4723940352895\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -674.3277863684311\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -908.8228582302454\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 74.02809596674443\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.41292242789845\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.52793367456832\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 75.48544857023684\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 72.99482034640062\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 73.7503889323104\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 67.87804198621743\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 65.90284940882758\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 65.89901858652786\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 68.78198379294187\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 68.75600284390583\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 66.17320179619183\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.52136989482574\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.66067615916183\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.65851081867167\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.55400982859219\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.57538577078585\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.6018281948946\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.12745149723691\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.23175855454787\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.17084209673538\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.16513490272056\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.18710044095822\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.196593573004705\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.14246080454409\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.123987977043235\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.186958084147996\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.13422119760929\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15745807041481\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18917459595876\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -131.37180154242185\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -251.86735082230535\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -301.8059363912064\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -172.81068697257385\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -249.97086419975068\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -299.97297128224494\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.96113756162983\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 75.5662274113733\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 82.26033129178111\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 75.59454001546317\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 75.41693262730149\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 76.53097652410428\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 79.77139700131258\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 79.83726914101833\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 79.76980018396235\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 79.51520376201786\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.89715687379007\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.95366512425886\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 47.71112657114605\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 47.89889292740758\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 47.6176116404334\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 47.89937792859591\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 47.65673445612707\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 47.69860318670065\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.23863253602705\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.24695248349124\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.283203035490146\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.24233783073337\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.300934638357916\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.21080265192463\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.18262225647979\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.09750530392355\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.182629010594\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.13102981287274\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.15846117678637\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17905255275202\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -65.8779641500048\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -35.83939236610569\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 76.93769278634221\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 51.03057845963982\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -40.848327800602945\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 3.6378376576285065\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 77.16507181885328\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.70159123709342\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.71315597692211\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.8503196666558\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.50325467058832\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 79.82644109283319\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.60468049190214\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.27924394186859\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.17905854904474\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.47362305553744\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.58486001556479\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.36239129737002\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 57.30922016733489\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 57.43896430600499\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 56.887634867765826\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 56.49578045463488\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 57.444011434358245\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 56.230378825864804\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.3071957368205\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.39772669039881\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.27864970023897\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.311752287338734\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.353773280858164\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.340683501675336\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.21037677449424\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.182360648968846\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.2109073357543\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.290774638036865\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.1460120496299\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.22410226107718\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.88206253081023\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.94604953404446\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.90976463701243\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.94910184789262\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.29979351996278\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.64897369981773\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.92412443347575\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 83.1855777510748\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.93914567825206\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.92542055259503\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.93370355240998\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.92457040333866\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 82.48122382447507\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.74525979308731\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.64134528127538\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.3547424518335\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 82.23738378796213\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 82.45897272605296\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 78.06944002444871\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 78.08324311215804\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 78.05402834021272\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 78.00911205776085\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 77.98622555690659\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 77.98356120962269\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.807813202278524\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.956278655736625\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.794690112798754\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 46.021418369360525\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.63425792088945\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.97473820484961\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.23641720231255\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.16292273939674\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.21907367190903\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.18446185617873\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.153043653875294\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.21425522342296\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.95351352259992\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.91821046748014\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.95158097677228\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.92267774575325\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.951195527001\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.9372164101083\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93445752212999\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 83.79847862458018\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 81.85227851735984\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.93446233622818\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 81.56758831888952\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93446257034846\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.37684846189592\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 85.09957739618211\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.39988914924332\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 85.1002933442204\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.32161792101054\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.09191264308728\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 82.82468469471232\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 82.76714623454525\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.83104490127242\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.89610365592156\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.82189163980604\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 82.93204269145794\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 50.29610934548329\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 50.30120226172684\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 50.317426923138\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 50.478580131538074\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 50.11824185335183\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 50.3213899529753\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20631061939042\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.24070688706616\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.15998066818441\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.26167697004194\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.201246074240444\n",
            "Layers: 3, Neurons: 16, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.27896983754054\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -1590021.052694668\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -636776.413442175\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -845684.6426088415\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -786465.3291019371\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -801042.1014336401\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -1146280.2218785963\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: -38.312459081628546\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: -112.18597526243013\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 25.624002183036964\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 5.829603226650337\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 52.72368774406746\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: -119.10869456453254\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.44506856895152\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.401813171641045\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.49850522825943\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.32373380228895\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.441502207539344\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.458086723905446\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.153657722416916\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.1945998117372\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.14746788630172\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.20441814338968\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.18506114659413\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.181604202311256\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.13580767169887\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.18113870595235\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.16400353905256\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.173397151859405\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.172149570110264\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.198726806300925\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.157651633226415\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.182876133656116\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.175780016069034\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.153938428792465\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16856796997711\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1679557537725\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -109.44406829119919\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -134.80901497920263\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -246.22393773074896\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -20.82670351532534\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -172.08633850121072\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -91.56388599204067\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 63.03251599369262\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 61.941500898975036\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 60.10482081335351\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 67.31497535954318\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 63.42255443460327\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 60.903832567666385\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 78.90930190779581\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 77.49235782030675\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 75.72225998771296\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 77.92978046586974\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 80.76592106264945\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 79.95946263553633\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.436488164450914\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.40605788692507\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.5077275243421\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.42259549712787\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.729369547090904\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.647405733264726\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.170989353250825\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18856189323366\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.172942295237526\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.15884679292215\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.17410613068351\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16327851410315\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.18463128413198\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18352208716926\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.195097195604006\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.172842367436836\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.134247000753234\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.15935359326934\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -117.5597212732547\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -436.1175590957039\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -1108.0763928845417\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -436.35141341220753\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -651.7310965669352\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -372.2486259949175\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.57638526392661\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 74.27638485187491\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 72.85947365828478\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.40855468000815\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 74.10602096949769\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 76.95427008227549\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 79.41795446976053\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 82.20672138083881\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 84.15461320002782\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.46907058016386\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 79.24086090061552\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 83.49009741390626\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 52.29097269758394\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 52.739769970725945\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 52.196814745927526\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 54.42225436529622\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 50.94076530385575\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 49.80231403527152\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20287152111108\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.181617219076784\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.22804370275004\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.19875791352521\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.19281632752426\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.197253109494994\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.186937706827926\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.172119337613836\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16946721188013\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16273681480204\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19916681186976\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.168571120405254\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -439.36083291378975\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -769.8294742819812\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -119.70949325883437\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -14.490632941359816\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -276.0577572850738\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -286.6925116723005\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.8102350736149\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.81252266284282\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 75.96128041596992\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.80544637746631\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.8238844613623\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.73872455674477\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.85032861956152\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.60498563329699\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.38018832921891\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.26858814255598\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.35475088308998\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.62994898586471\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 79.75186313392103\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 78.9841846748472\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 77.78147764026684\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 80.82829182020859\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 82.14766517012887\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 80.63952389648695\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.32581473630326\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.33127238644601\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.32177665405545\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.31531984341592\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.338327242500895\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.32004995602367\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.14738250948563\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.16852921684278\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.19300160485156\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18013639793359\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.171554601623285\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19350804228838\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.1876497023582\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 73.63490998356288\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -116.96274640690928\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -269.79694092827003\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 30.812927117327604\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -44.42980287447256\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.93858958406379\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.94226931098964\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.94419424790829\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.94245215892609\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 77.05474770110277\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.94034929059683\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.99285918961186\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.61200820636054\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.8283052159124\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.82557518142917\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.80613254229594\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.88160224888213\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 83.80787924235015\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 84.00672126831476\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 84.20756254975348\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 84.11067105912595\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 84.30603749824293\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 84.01926139784719\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 46.21983660287129\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 46.301868134501525\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 46.229442445354806\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 46.339375901185655\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 46.57800960389874\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 46.06030840811073\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.1567779481286\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.169505681656794\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.15592631034404\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.15894664111031\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.195580326023574\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.207156361951306\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93436704927538\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93428288303576\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93482346675566\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93422411884619\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93365134358817\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93441012740638\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93445320553738\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.9344529714171\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93445367377794\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.934459643845\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93445496143946\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93445496143946\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.06423067287507\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.07518696318562\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.90289615478194\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.0361280920499\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.25064498727973\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.49511060067233\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 84.16039755484314\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.05057817480323\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.11918391018112\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 84.04033930858617\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.74228933006582\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.95961644505924\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 77.79998017857255\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 78.84707042070747\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 78.82760843738633\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 79.72496090147474\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 78.9183024021232\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 77.63865758963915\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.18077293549864\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.18182778537722\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.183984098143625\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.18540049252206\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.23228820984934\n",
            "Layers: 3, Neurons: 16, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.17495970276377\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 52.07084806476647\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 53.77815311696733\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -39.22560583187054\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -37.97766980696877\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 7.121757387337569\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 52.78363917401616\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.05622974930936\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.10987360650294\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 44.557536478512674\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.506944438439014\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.30243000986021\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 47.81599910313204\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.267024631410166\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.25714332378592\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.27781433171132\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.19929200608039\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.25874662368427\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.267657661916985\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.16149157281332\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.16338846603202\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.159559895387716\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.16511296898435\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.18336265456141\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21391938030034\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.19465006614982\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16708963770508\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.167510527695875\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.131449615650695\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.14329852702079\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.170633700509434\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15051079317385\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16830860294536\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.14661572090524\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17902781072637\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.14954899618846\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.138414246746414\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 12.238583171509399\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: 71.7324289746974\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -7.380981679432774\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -25.77007028852607\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: 5.666434606647963\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: 24.705670542949278\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 74.03892247851525\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 76.51369089727251\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 76.09566109455726\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.74242236360902\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 73.37087396373683\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.29873509145487\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 47.91752086181407\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 48.015898215071104\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 48.129066047331904\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 47.96610918912021\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 49.01140352936142\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 48.70720350015287\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.25461908728513\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.248483815067885\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.31535429930751\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.27784732386533\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.24180535059873\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.22044020868897\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.16017156799575\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17155822798021\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.14389641592253\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.198937871310854\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.19018882437598\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.163507493935164\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.181524514845464\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17121945341708\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.19257007831923\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.138195851509785\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15713471589924\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.15027897235965\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 57.27524962606309\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 67.32366233389635\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 63.47029234926775\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: 18.830105423424314\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 74.57664966766157\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 33.46743574533876\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.27614525464654\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 76.61800163570473\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.9616478706714\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.4470266408741\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 76.85401897481634\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.03106041232608\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 65.83348082179167\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 67.00950441115378\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 66.08861313773427\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 64.66293075233756\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 65.54136653764064\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 65.71197247843565\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.47795193687819\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.52417207893669\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.488006355298815\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.53968843196973\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.50982657364149\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.583975912512244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.19426670445592\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.22441151632995\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.16552414562543\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20239750251294\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.1806368708459\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.173449199193016\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19645044898045\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.166532272279355\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19320093303216\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.18441708561277\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.13904960673856\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19723201118406\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.39908875822792\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 65.77365701117184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 64.10183167978865\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 73.71012832179214\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 69.65006592358765\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 76.0857738733383\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.02582934445059\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 79.80700217401748\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.818929598347\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.79780649142015\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 80.88630733336348\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 80.77999720776452\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 79.38636905624806\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 79.5789475581817\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 80.8761929130481\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 79.08327903791456\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 77.93899158613866\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 80.9580770799105\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 46.34255032041862\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 46.642135666118165\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 46.57278769447011\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 46.43384066540756\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 46.70632638234401\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 46.74831716868901\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.209191257500606\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.19368416307859\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.19805656336634\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.21560246257042\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.18212671079797\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.21623377002687\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.20437697483792\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17743248314932\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.144856139541666\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.171460278982764\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.15882574195077\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.16406089532941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.87958521636273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.9369415236379\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.96458614599581\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.94762504644946\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 76.95526579581416\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -5.069944135155935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.93244609240146\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 80.48402807472033\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.20439983026982\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.11619718147047\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.93848540054046\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.93448434353424\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 82.63024171010535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.46880748238105\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 81.62786977612109\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 81.2494940514486\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 82.7172350545107\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 82.11316329288812\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 52.30705880551765\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 52.12730864066477\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 52.28320316377617\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 52.253747878972725\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 52.23822123802253\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 52.21148466030483\n",
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.28155026233627\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.31294278374658\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.256688085509346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.28182221364372\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.25727942260143\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.238514788646\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.171254200315715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18269251011766\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.202749893569404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.15585936088649\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.17882838399776\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.2022777457186\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.93445039609405\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.94618356790312\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 78.66236706826254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93449019654116\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.93441223448887\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.9344457136885\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93445788794293\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445894148417\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93445904391179\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.88164128990158\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93446186798762\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93446093150652\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.28317585437746\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.61477429353738\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 82.8392362612895\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.36636912671825\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 82.77274541119574\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.3396770453883\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 77.02748840413973\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 76.89342579989768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 76.68694023890289\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 77.0767349167075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 77.1363265165937\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 77.2644407903732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 45.82114263242993\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 45.70952484079097\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 45.70817246330072\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 45.64477958142369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 45.73381037674602\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 45.78875110119204\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.16187104009632\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.19683520482174\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.186345347199385\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.214096656348524\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.17891272463442\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 16, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.20241293377423\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -7.8635374156277305\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 6.780126371921414\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -135.088332409963\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 35.81676759708875\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 51.15686487593686\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -130.11099510646477\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.98232644722637\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 45.29095212709963\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 45.715301067414394\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.503820878852586\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.891223088918544\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.494504808625095\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.268194025670326\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.24255926281037\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.371450202465965\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.28577710672253\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.30478835328099\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.32034957879317\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17171367449171\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.17715482541017\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.18001758727118\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.19801612878751\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.15914137777417\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.15679973174754\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18711200251169\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.177326952300156\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.150095348392504\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15535793328993\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.134059281914\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.15101689177643\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16293537902486\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.147333782077645\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.20387148895413\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.12686062173359\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.18545553832675\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.1434359968203\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -18.322394313965894\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -315.8083839785975\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -121.37299044177045\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -153.0898046402077\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -186.39219927156753\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -68.46614722781223\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 76.78413329656708\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 80.22709026252532\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 79.5029581635676\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 79.88201363222967\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 79.22578154874114\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 77.9562674457651\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 64.67738625278041\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 62.05462150295713\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 62.51601297827026\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 64.6643213316724\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 63.655058060014305\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 63.57236851208229\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.386391994577316\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.38511008068335\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.4404201098433\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.404625996164214\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.39471231861091\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.33135597858797\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18285887966045\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.16476343481616\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.20229943298121\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.178688418753886\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.14557716984332\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.17757424750602\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.169283671447126\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16791430995365\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.152135561415044\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.140033140642785\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.17386539304421\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.183154101811105\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -2.0487934691630683\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -45.22294205818132\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 25.382218582217696\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -29.38287112737552\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: 7.426090825779208\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -55.79924017888265\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.95738420387681\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.90571661035723\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 81.84419971798222\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 80.83482065921457\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.06082145126103\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 81.38249025977706\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 75.6987649341813\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.04107841020532\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 76.16860514451682\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 76.28010916993405\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 76.18815746534177\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 76.40225688845068\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 47.63824843701888\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 47.06917495644316\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 46.99121458952483\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 47.187815662680634\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 47.65020414289497\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 47.46557551322882\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.20655744810664\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.20913787940992\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.17617355082055\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.222077382875646\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.162785034787476\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.16916981649386\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.16595346170275\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.17081209943452\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16723313947942\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17651352879749\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.155912521095196\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15935173362856\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 50.30504364230233\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 65.02283168012812\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 64.6608174327328\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 68.52624233993572\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 56.54675655269878\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 44.32989474041599\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 82.06459940554403\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 81.27381971068066\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.3150459877778\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 81.99234326680501\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 82.83902693947111\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 81.97619395371274\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 76.1019286992315\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 76.09466282032804\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 76.1169963290877\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.93130853123989\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 76.16713257187118\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.0234417851327\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 55.38024562137722\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 56.563510755766465\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 57.48074260487792\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 56.12896684398585\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 56.72737870035293\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 56.250008399064946\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.30257426530957\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.2447843563639\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.29681536135436\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.261443937765335\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.32793968691593\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.25868863698472\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.127189046570024\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.15042198436798\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.15862393545185\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16011377387492\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.149235058355096\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.17195423628537\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.93346242315704\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 77.80756494941316\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 75.7839515138779\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 76.8956950911627\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 80.41643940904015\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 79.73701541433857\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.59411893404898\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.37321708848937\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.81123219187523\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 82.60200392207653\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.03304878632868\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 83.30192128771515\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 76.14773433145363\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 76.12852695759202\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.99534840578315\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.64219604729234\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 76.32709266127048\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 76.40786996599263\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 77.18724534927222\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 76.53878372018255\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 76.76726984346695\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 77.00845649610957\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 75.9172702814182\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 76.49718689561851\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.553471555145286\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.71456692166166\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.715211789860696\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.94777416570758\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.835278926665154\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.73551216746113\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17070233667363\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.20699326077917\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.21098518673378\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.22464876738708\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.185884450208455\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.170550185282856\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 78.39543950205613\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 79.13602798251206\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 78.20991451730205\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.91686694193125\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 79.37337052287113\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 78.12449620974489\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.40008699148608\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.57325978081576\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.07190096876747\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 84.43219957792489\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.06601582151471\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.29641748947067\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.91437976205765\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 85.2918450988752\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.06096730599882\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.51330186445502\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 76.62028629842919\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.8341232694151\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 85.42647266712599\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 85.64520738112958\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 86.05419837518538\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 85.61943253567267\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 85.37394651655133\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 86.00344012603935\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 51.37782079852491\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 50.88564756596092\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 51.07264436246977\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 51.33027226338681\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 51.5128339695025\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 51.06221849633946\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.21662482874997\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.18111808969284\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.23297664024025\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.242892483432925\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.21341474964717\n",
            "Layers: 3, Neurons: 16, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.221460164683315\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 76.92326357321672\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -1764.2234548810893\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -2261.402099515727\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -2259.2642255226315\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -1090.774717107787\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -271.02022499520524\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 72.33713422920862\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 40.82362431422658\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 62.01852543124487\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 44.42536131284866\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 44.32593214772513\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 55.25191829082927\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.87947335355713\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.729428963227704\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 46.21148395593202\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 46.01168393722932\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.76624619672707\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 46.087557398408684\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.241872298595176\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.24138255695708\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.25684039541071\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.24358077139794\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.25792270590164\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21573246602794\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.141434237763924\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.12230190936809\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.21507612892184\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15322482876771\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.19232060585212\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18467746178452\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.14832419675167\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.14453715185724\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.17509044377732\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18375048072352\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.15528973387387\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.20117126533846\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 34.51363010794255\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -3185.392662782893\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -433.0882487353759\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -706.5642381089374\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -1570.0924943661298\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -898.0976397679324\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 58.251900155287295\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 75.07853201810181\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.59684933105282\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.82667513994774\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 74.75938089514858\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 76.95260074616942\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 71.86918648623579\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 71.69405354454823\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 71.90953631121215\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 72.61206177307994\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 71.83951485783467\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 72.17873497579946\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.797387359880325\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 46.345315045856815\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 46.70577829351451\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 46.438452672537686\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 46.765375985557846\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 46.7402184347929\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.212408942386574\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20645662527754\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.218309548065086\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.2734991421085\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.238131370536884\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.229833671129384\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.19071082462741\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18389069582047\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.18434282329389\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16867561267255\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14436810229176\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.18488007325766\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -1066.289069991849\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -947.9790812595894\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -375.0129534066935\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -1313.6581948839664\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -596.8081614703203\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -820.7321484225164\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 76.71810546687703\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 76.07772364760889\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.94246980574198\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.92971265037077\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 74.48374696650357\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 76.96229045766955\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 81.52205576602188\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 81.45668160014561\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 81.32234727737315\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 81.5501804446918\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 81.48865736715173\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 81.53332928656434\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 54.383868584012816\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 56.2656812225213\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 54.519630853911224\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 54.733885226233234\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 54.372616790929506\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 55.35674352503207\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.33863874541694\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.422663900716856\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.34809500516307\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.41374090763176\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.36757585048493\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.37037712635096\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.17783537164107\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.162705601888554\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.14978974405997\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.196248161873356\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19156050940138\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15907117272797\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 19.97314921365555\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -89.77002833417242\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -40.30709931344696\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 46.65808563173892\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -3.076984740602229\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 10.608804027468121\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.88441590783603\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.78103726075717\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.71391208006314\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.94450559861184\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.58006838559648\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.72531391314999\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.97718611886596\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 82.17154520793486\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 82.11658462419061\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.03484412669953\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.32001795181608\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 82.13399881231955\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 75.39867615489426\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 75.54104205425278\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 75.37067305781076\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 75.39841306223283\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 75.47878793614736\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 75.1070899656096\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.80463093567224\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.83548427839869\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.745951848373956\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.88974563975786\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.750492672194085\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.8635736286846\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.19908881107132\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.235754309181374\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.22509020582087\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.159252634808546\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.186483806144196\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.218021351019566\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 66.59691540272152\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -33.13047092263617\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 75.34203731652462\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 73.8300820113966\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 75.08142662267826\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -613.9258786440353\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.94268569390253\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.92065664393077\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.94124597125828\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.92105221940405\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.92359426810818\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.93757663341971\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 82.60627788284705\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 82.6416911866686\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.47749668087683\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.72666893107798\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.5211281061264\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.10270291157596\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 81.82695404889715\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 81.82389919630724\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 81.91734292016068\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 81.80089469883329\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 81.92649967879868\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 81.85075743060301\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 47.65411812270516\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 47.783058445984985\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 47.906128767582025\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 47.58197385870637\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 48.259913445799384\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 47.70067980520836\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.28439940865419\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.1792149821537\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.24315585092562\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.243342836492126\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.24568206235898\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.2523090919579\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.9347136643457\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.9435077194606\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.95959634784542\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 73.25497968561767\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.89500651416259\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93560765262394\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93445566380028\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.9344605803261\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.9344636238897\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.9344581220632\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.9344560149807\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93446257034846\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.76819938839648\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.66331990532461\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 85.22685346907018\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.85484842125557\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.01751076256035\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.76437589808539\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 84.22793734791547\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.10131198764533\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.32420278811847\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 84.00253993137888\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.12490182605154\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 84.34522877536975\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 62.46435069562285\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 62.118292601116785\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 62.22908146590807\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 62.17733746430953\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 61.97051212163003\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 62.59442913609992\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.45251107394215\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.370287164491295\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.393502917562266\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.41260463584644\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.471204634388606\n",
            "Layers: 3, Neurons: 32, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.39367599303484\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -8406073.376006905\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -3530793.526563099\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -10014015.138569236\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -2062727.4959244344\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -6503699.41503644\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -2880823.381281166\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: -1503.201093070115\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: -1198.1425812622117\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: -777.71540973577\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: -1173.3858898237468\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 5.548888317323253\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: -729.0995437979104\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 46.31622305765069\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.8801984086227\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 46.586384015792994\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.892439317236835\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 46.08781008757894\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 46.11445753611867\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.17892937019816\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.171103809978405\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20010656819725\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17354668495304\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.19375571137273\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.19594454333347\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.17952723988222\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.1563777337919\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.185491103010186\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.14368130406419\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.17881961620838\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.166437899891044\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15687450916068\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.17138790748538\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.153706206079704\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.145997206966626\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.18167467557442\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.18370983445368\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -183.38700119270234\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -117.16384636016014\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -83.33410312080456\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -526.2211264144611\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -234.6155707590142\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -361.02020813854523\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 64.87376619491899\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 60.43021533879733\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 48.32163822984713\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 61.05430752279021\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 44.57301808903398\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 52.67870355393176\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 72.97554663327813\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 81.30707978927911\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 63.829553919273806\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 75.10336652220919\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 68.96861200548472\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 77.595282841058\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.51831879165833\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 46.53507508237471\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 46.220622849866814\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.97821707178177\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 46.9890648547287\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 46.892911617811684\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.214554776545114\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18392181172924\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.1813584940646\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.20884795888893\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.20255115404039\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.20300761770455\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16685393757199\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.14039907163265\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.19375186886511\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.18701279822038\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.19405950836074\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.17288195086113\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -1907.4618515055622\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -2328.591787135597\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -1550.9474866345417\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -1356.600139048715\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -639.7280309023782\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -2571.979256568853\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 75.81617629706379\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 71.32720092726613\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 72.19104914634129\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 70.26210262531241\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 72.73394743784387\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 72.42410967804528\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 82.55523384051804\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 82.43802678050052\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.68412802344682\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 81.64351403690362\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.71516624848688\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 83.83983713573635\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 75.00285008880066\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 79.12537195054242\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 73.6745308896889\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 76.93257289962564\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 77.59359031241198\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 74.6912686962893\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.30348751541066\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.25844068153921\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.24420370246699\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.20939838915697\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.30204375493467\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.25579959151271\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.1637880305292\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.182442671880494\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.19694597536421\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.15805571068523\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.16050690323854\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15692277502813\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -1791.844717946874\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -2543.1051466004988\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -1360.9360091220751\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -1360.6099488756233\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -2489.0069554324896\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -2323.41836581799\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 76.90774265723891\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.90624826750995\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.92313313895731\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 76.87323847903481\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.94966710203673\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.76916323633637\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.76089734778716\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.78065426595649\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.94886601048957\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.7754311321977\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.83958988506126\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.95353244829782\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 83.14018729458287\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 83.10023207567549\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 83.46835453390482\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 82.91163804844783\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 83.83444747661419\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 83.3142674047004\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.72514576568062\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.83360717529349\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.80889964714149\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.67415464539432\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.78553663275916\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.78701057537424\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.1706859275808\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.20336084993786\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.16422365376448\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.16813416920651\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.1693178663085\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.16030933261046\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -1445.1719978663214\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -1401.6089419711354\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -945.6769709556005\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -1104.4295631353086\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: -753.1092408959053\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -819.7210859584771\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.90127057776763\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.90549735598606\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.90938305022573\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.94312695209487\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.91481499183575\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.90485533965608\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.08495071156527\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.81034356860945\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.90240896054506\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.8362005637622\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.89017198019025\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.13758745767808\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 84.0785654029217\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 83.83275686848607\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 83.80748548953983\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.81655061661077\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 84.07594754680345\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 83.76275039880635\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 55.43917807574286\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 54.64853314910761\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 53.91036770651978\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 52.72325003334605\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 53.589420988041184\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 54.558358847248996\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.19493241726711\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.21164322379116\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.177660746622074\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19699936290924\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.186348462364435\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.19673850663123\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 12.527109255130409\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 75.10910999925831\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: -154.6994242139552\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 65.73284317443871\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.84402682381568\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.93962182036569\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93446549685193\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445636616111\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93445578086042\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.93446549685193\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.9344560149807\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.93446432625053\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.30549065162269\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.13123916290161\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.93205153352234\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.10786582173448\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.46839835646512\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.43964045739146\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 85.26983892684427\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.83550127393036\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.09417185327949\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.99212348858973\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.8784436261375\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 83.98749401983649\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 82.18958139145197\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 82.90625383445116\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 82.58805574564171\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 82.94428631497564\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 82.61274202417769\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 82.89761345917849\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.4337748030067\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.25728861230816\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.345209720785654\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.35633444263074\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.37993959210868\n",
            "Layers: 3, Neurons: 32, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.297040045232805\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -36.13715291434796\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 76.95803878953917\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 55.55942068623088\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 51.1162078997894\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -34.33687957073015\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 55.75900905654492\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.23433320971343\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 46.70035602994289\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.260022731590055\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 47.64005873456604\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.77702603076763\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 46.396211753368924\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.25726241975919\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.285045200716645\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.32845442893865\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.25926749540281\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.291416722156555\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.297847162684945\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.1853165039196\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.1776589381594\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.186904984471674\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.17720250679322\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.1611195917491\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.14339525289019\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.17594673179982\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.16103145322935\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.18149072333304\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.176273616528086\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.16756250474625\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.192061367765845\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.15862774282857\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.170597801021415\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.17184352391303\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.159657949579604\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.14691175231342\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.170977759472585\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -29.75114606558065\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -244.87650436325276\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: 17.513061921441974\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -250.32387075362007\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -147.02371179531667\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -316.67890834771765\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 73.32570321449248\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 77.91238399223208\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.36711215424181\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 76.85031566758882\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 73.40165451746313\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 77.76362247562152\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 50.76201462315837\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 51.13858555119569\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 50.40784740338253\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 50.77552974475588\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 51.1518372984558\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 50.90663992585833\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.301929146943074\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.300498779964585\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.330971962631914\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.33830466612711\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.306341760817915\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.30476704114097\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.1670279598289\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.18764059108978\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.17113305746189\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.187982818098206\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.192653214802604\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.16297908804204\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.18181353923909\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17789426275415\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16218455832009\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.16631356071814\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.17827081006759\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16248297134196\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: 5.267718409870059\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 18.37780600175759\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: 17.581138245026917\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -34.87499850163023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -48.215715461733886\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: 16.380214420460536\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.33003510393583\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 76.94179529059102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 78.35543131709419\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.96118119579647\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.12011529405227\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 78.52432413397504\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 76.0146989665088\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.41488595076525\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 76.1666252917607\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 76.05980427181944\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 75.53728835995187\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 76.33901820678477\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 45.74958622547051\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 45.766552784317604\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 45.75971888338967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 45.747963706789506\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 45.762384088276264\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 45.84921168512799\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.176109671221184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.19754677748013\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.20492045548189\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.18077407393063\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.17389623728329\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.21253080512023\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.16489381499132\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.16306972542851\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.174417859143404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.1841376550027\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.18440034114022\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.19481920433921\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 76.64895833567454\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 59.27941782153516\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 75.68054052893577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 76.48792484870212\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 66.14488693137804\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 63.90801503276935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.68357290158932\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 80.62939775918332\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 80.67877972496392\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 80.78158872448533\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 80.99852374288649\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 80.68901165902592\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 79.11331750352491\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 80.25720027840178\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 80.92783949500443\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 80.68449468872399\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 80.75278965138273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 80.6977915059665\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 48.13780311062092\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 48.04297316517919\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 47.99360698971385\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 48.05369910765847\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 47.977621377909095\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 47.87292262352789\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.225016083675506\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.23066916320463\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.25976534697074\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.24089829233879\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.248106350021246\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.23723248139419\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.188868777511345\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17010552612339\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1865083576792\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18038531751767\n",
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.16663080553436\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.16711250634251\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.95771498656336\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.68164708671965\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 76.87481741544887\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 53.44305520593594\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.03964461665707\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.72371317428512\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 80.68675280074575\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.0288158148973\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 81.41108138651786\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.83846591119799\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 80.95200361215981\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.50338995625884\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.10087887318052\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.21306032561964\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 82.63606273400907\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.41026013501998\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 81.53095899434852\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 81.30613505543079\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 58.21834603350968\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 57.81909726797323\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 58.20593338978441\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 58.23568772483155\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 57.72681608298842\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 58.154063354656074\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.35314540125467\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.34681569154711\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.30713518929289\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.346302768029055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.310381243557686\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.3306628893425\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17097601842451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.18728559072254\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.16447170254945\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.16797313071918\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.18119803362244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.17570100986911\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.87227135743498\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.95066533236839\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.93444817195142\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 77.25442980732275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 76.9344432554256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 76.94592044597665\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 81.06148753076575\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.7536224055025\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 82.27460662203819\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 81.44610921861937\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.83693633319136\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 81.79013052158626\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.76071817188094\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.66985893725068\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.49492185656304\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 83.41989541721381\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 82.99923950564052\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 83.25810931768736\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 81.6325736914643\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 81.50306593379528\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 81.18823059134708\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 81.78630483530013\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 82.13003528040399\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.66310331215107\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 46.28174903020577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 46.194591774170455\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 46.27287102971079\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 46.201041319565064\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 46.26879639171941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 46.33091450240542\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.18451504818709\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.192772170931015\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.180908393466325\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.18036120588701\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.20328464005807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 32, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.19348055941646\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -873.0114282535601\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: 37.83501938562719\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: 46.36763741332632\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -132.05579865239434\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: 58.84235195515481\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -20.68412353492213\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.18350464912109\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.52827894777962\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.18211572486114\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 46.774688240527915\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 44.316863851971135\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.75646364419087\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.41128387874743\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.40205575204781\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.39665084499058\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.25390662593352\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.35420564835299\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.29075509705304\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.19405769442872\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.19633683721107\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20886975903197\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.16813193658481\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.20058069024113\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.18651891349539\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16427670432398\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.1571201746208\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.168589281889396\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.15606963514387\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.17737453577466\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.16407134902477\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16488516377625\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.136706733966825\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.16634872528654\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.18021232377071\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.14909059914935\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.14466192232503\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -1341.0718735745002\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -1128.5975774568208\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -860.0883929520033\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -788.4903031544161\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -856.2930181523049\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -4256.461952832818\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 64.19603843592024\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 58.520581562673634\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 74.8507491857282\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 74.12613731533528\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 66.49246460732071\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 75.42951544537415\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 73.15484140675599\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 73.61337399089479\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 72.89249837192759\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 73.65009161536156\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 73.41882598139475\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 71.91769736362774\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 46.215435142661356\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.82825622436675\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 46.10701764806231\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.96002147531928\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 46.341214279053325\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 46.261562636149975\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.18771830656486\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.20122063215815\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.199396100879795\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.212674663571114\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.20604096584188\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.19776337975825\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16131696833556\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16500138289376\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.165886835518954\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.19599222299819\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.15298979406541\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.178273261825176\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -288.9290085248578\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -361.3505458365376\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -472.5988278098156\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -151.181449163998\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -692.4930830627371\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -843.5160920845855\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 80.2038595193549\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 75.76474759329037\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.13186735991641\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 79.58261725268787\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 81.06671520243397\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 79.20504199438822\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 77.15984618110497\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 75.93847907636648\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 75.72567398720973\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 76.70465090378154\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 76.40997087356456\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.65083574325041\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 53.81873804991575\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 54.79968636896494\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 53.51822601043268\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 53.09238997189807\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 53.737289734530556\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 53.62875740876542\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.28800543687386\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.249518162476576\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.24474124568842\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.26299848216598\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.26153540626701\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.25306553224057\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.185155885400896\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.171905715278584\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.2029827418178\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.1656683355139\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.186944263610364\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17555881738444\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 39.9116830127103\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -28.690178717147518\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 24.54024891767367\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -19.384690274852055\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -65.86960956974417\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 48.431152670055134\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.46582867205716\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.2811187722009\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 81.7662852323508\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.20614159111781\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 83.03594291095858\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 81.96701713663708\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 76.30444496309889\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 77.51436412768845\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.45016911297773\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 80.46389065074152\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.05682483888194\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.18221480132155\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 74.36121224716882\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 74.1365779237268\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 73.8845107616663\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 74.05705149438036\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 74.19929801434633\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 74.44948141539193\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.53326958483554\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.54790090591625\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.51507933792277\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.52710556595915\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.56334677006939\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.52569386841515\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.189262837190135\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.17487750497639\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.16400788105827\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.205921585838425\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.19976359205824\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.194136689593535\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 70.36029251224473\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 70.19674097572563\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 68.3308221368347\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 73.61945236419689\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 75.7947400945057\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 64.91160976324312\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 82.09945096702562\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 82.80749543581057\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 80.12241437145244\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.02101234023398\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 82.13739312102066\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.68296913283055\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 85.5829560532623\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.25824204486221\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 83.23030090624681\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 82.90282775123869\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 82.68040980239184\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.78292825700315\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 79.61895964270951\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 80.31319770403277\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 79.51074666431867\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 79.71222225989608\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 79.5966035888988\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 76.90220889891525\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 47.92110815343469\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 47.93620913683523\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 48.265801104357934\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 47.94903466510141\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 47.729601200825755\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 48.23850792854465\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.20059367335474\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.19218735564944\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.209705164748215\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19794248381189\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.186379867265416\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.20433592000968\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 80.24943962287391\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 78.64377617333005\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 78.20000588871027\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 80.2110515794948\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 78.26093106696008\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 79.70331654404148\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 82.89066440354009\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 82.4574577959278\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 82.59807132696072\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.55722862685553\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 82.26100280726601\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 82.52334370849047\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.39326187935102\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 85.2802267632907\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.00293385273751\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.98942398960946\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 85.19684952888444\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.02783776820277\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 87.40296172510402\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 86.58996492772894\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 86.99966643384283\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 86.27064800555102\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 87.15741721066561\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 87.02387715225069\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 63.16520298766504\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 63.177831311037494\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 62.87906163123817\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 62.57290070112702\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 62.42013269153005\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 62.967628373649774\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.30142541602093\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.28637906978007\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.309942752886656\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.28899271763135\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.31030945284817\n",
            "Layers: 3, Neurons: 32, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.303106974370245\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -1226.389857834676\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -2068.0849785433447\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -32.11442544261842\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 1.8130021411704056\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -1485.5923055715384\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -790.1216901011699\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 58.82720078094097\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 38.050777433839485\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 68.50562677650467\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 66.55192724534456\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 45.96068900985922\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 76.3450262013367\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 46.47090342751764\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 46.810629948099134\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 46.58186928678118\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 46.30558597250464\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 46.925418087015856\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 47.45745511726259\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.29415899188207\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.335177436876805\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.3324278958762\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.33026461239788\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.344100919408184\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.3299020734906\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.18701733233951\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.23014185074924\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.20954086382041\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16961235165042\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.187671358915395\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.17260635282385\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.16074267750456\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.166991899393025\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.136873616690856\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.17706671305284\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.172146151283854\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.17820062629498\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -6995.445135692367\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -3978.6077447736866\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -2472.6266122458765\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -3272.216373585539\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -3882.4933772056\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -493.30765127541235\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 32.06211887560085\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 7.2456227000023965\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 75.35965077027444\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 16.17963690660812\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 39.689901927718275\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 55.67475750933766\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 74.82157743429836\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 73.76804667869014\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 74.65400210001206\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 74.50559921176854\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 73.97939957829787\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 73.93965315443829\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 51.14209828268855\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 50.98562777613972\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 50.78656005274074\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 50.40635957267945\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 51.18301065479333\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 50.30882598341404\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.36923588974109\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.40596300013732\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.3911911247789\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.36707112357723\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.37116714276296\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.3499323219964\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17728896517954\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.18269722386737\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.20454283859431\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.18595404757918\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.184080858093644\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.14549650096109\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -2323.226424649981\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -6326.58905111239\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -4618.871337984273\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -6382.378691983122\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -3287.713712480821\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -4990.18849491753\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 65.18582746815216\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 70.40036867622402\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.961332847206\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 53.47044727836114\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 70.13959895617943\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 73.42208692567486\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 79.8831780349152\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 76.3639939091971\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.52157508490137\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 78.93977002435413\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 80.09298512246997\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 76.16245490726402\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 73.15296849849629\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 73.60899377609972\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 73.8831706498837\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 73.36390777342065\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 73.47171098645455\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 73.21277114317981\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.80136858735635\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.86452644013974\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.88051671834572\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.83589134931473\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.86497765164472\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.92186835930637\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.202872401091234\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.196397029782254\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.1843859880804\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.22246371664601\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.24727922826034\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.227389532313936\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -859.9453469625049\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -1481.0896669423669\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -470.05029278145383\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -543.3978905950325\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -1050.9120876486384\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -375.4051198545982\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 74.23186740794765\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.2179522702737\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 76.94464536841728\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 74.76526035766835\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 75.1070403760084\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.85154044586989\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 80.0405711491171\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 77.00724981825242\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.14799841034673\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.1727276280048\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 82.29088547314818\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 79.31677757207581\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 79.80031640126325\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 79.66239793848433\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 79.68549524269937\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 79.74904978188418\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 79.74002135773934\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 79.83496286606578\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 47.677302300175526\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 47.81739555284993\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 47.28262093682649\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 47.51723813516773\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 47.5589624866929\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 47.81007591407784\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.258418493770215\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.302028413826264\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.269180084634066\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.2491167676394\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.26282030381733\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.27614154650372\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -599.6618291798522\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -42.40890005424098\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -47.97120451698551\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -357.0763115979814\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 58.499320029846594\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -3.4392100145042237\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.91659805186644\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.84808670354155\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 68.06886038654007\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.88369952905302\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.80271278911044\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.95407406507346\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 83.10020910628143\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 84.07363707414751\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 85.27237969337408\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 85.11117100761363\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 77.06210858894117\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.44127759578633\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 82.12127008061323\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 82.14294321654992\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 82.11041618408625\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 82.1974055264981\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 82.07999053475132\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 82.04361409669092\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 55.847231005634626\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 55.684790528959184\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 55.84265077164405\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 55.78437296695804\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 55.935365049876516\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 55.94308931433382\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.40816878776327\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.38161200693068\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.34309176230138\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.37895871045537\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.41113384173755\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.42384763124303\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 76.95281915112295\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: -1154.3863576428846\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 75.8686621416836\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 66.40659371783187\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: -805.8949687739739\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 52.59242600300873\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 71.44829238288742\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93537493706847\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93452086629748\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.93735758463542\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 75.21824654190303\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.87575053107844\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 83.65879207998114\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.05188982761427\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 84.24917613975289\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 82.06678928247942\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.18309519976937\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 84.8584260271114\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 85.231652382374\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.83605315322203\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 85.10182946588877\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 84.9504172577911\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.83395566234289\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 85.29534903622516\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 77.73858720423117\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 77.78120855193144\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 77.7582010108939\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 77.77322458224046\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 77.77409084921464\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 77.87109599609141\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 46.1823637182794\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 46.27147131236888\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 46.17232454111168\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 46.11170798345777\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 46.24941446152875\n",
            "Layers: 3, Neurons: 64, Activation: Sigmoid, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.94616912839785\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -74783534.53442654\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -46114780.87504795\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -22580148.242711928\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -56600941.3008247\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -46426704.8518412\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -97880121.00930189\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: -14628.389164464064\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 16.221278856026522\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 66.30473231647404\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: -8334.71264732721\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: -4406.183449438262\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: -6122.3456828970075\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 48.70993994093509\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 48.81507569270249\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 48.99863013829771\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 49.32532593716503\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 47.9988474556894\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 48.14692949992649\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.20828992438456\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.26058870137286\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.23429943272208\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.24572903889839\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.21343204391085\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.21755770501142\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.14409076374582\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.17683670437375\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.17868398750446\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16006478893409\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.17259150523246\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.19790558169456\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.19030107463534\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.16341976632353\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.18390709780068\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.16354052228553\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16608644685449\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.15949667586814\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: 72.96649606556191\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -167.36997521696395\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -838.1663175465095\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -1345.6015055619484\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -864.2206454377638\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -406.54383780446875\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 46.134002635796314\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 49.88155632399488\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 57.162661345744894\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 44.24088383342829\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 56.87471829477653\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 32.89606261902676\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 49.4688777817041\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 45.54498852099501\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 48.03459890646911\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 55.103165612259176\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 46.32119464980582\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 45.674563513823664\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 53.504125297321046\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 54.27941158625374\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 53.86302740947926\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 54.1085121921529\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 54.20988547468168\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 54.096920717983444\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.251037676900474\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.243746114871165\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.219821585224395\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.22311696612663\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.260013865216244\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.228231906616514\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17646740698144\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.16535585647215\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16146838547298\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.156671965818404\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.14744096085155\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.169489646548755\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -5138.320327483698\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -4779.630382144227\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -2820.427829521481\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -5244.4300800728815\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -4190.781459771768\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -3960.152623945148\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 73.52807317513546\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 75.75951717070889\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 76.41030976266572\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 76.80682417529727\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 72.26355701559204\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 76.95523126307327\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 78.84953921720083\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 82.81476928760688\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 50.53044394235937\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 46.38914684065924\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 81.84043713991423\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 79.38415305112393\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 79.48801077047801\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 81.19246363594105\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 81.45351780846424\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 80.93546037706874\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 79.56882136600612\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 80.41885751666445\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.565580720811404\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.60808218086776\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.43728467269926\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.574092473227445\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.59845170071173\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.62265001303124\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.15895378812511\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.19184359416114\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.18608309999094\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.17387679148138\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.17441888270691\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.17618166085487\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -4729.14002565209\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -4960.29826548715\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -5986.49351505562\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -6232.283905111239\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -5761.944434455312\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -3064.395617568086\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 74.99513105912642\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 76.14992528753716\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 75.77926363577365\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 75.94811814472416\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 61.23584716586167\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 76.0639165213248\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 55.87824434807898\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.49459395721401\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.97505479760676\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 82.9983859177426\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 81.7839267329782\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.00865409654999\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 83.4233164686694\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 83.78609786785375\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 83.54535235451426\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 83.26523954218084\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 83.33187224721378\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 83.35446202256554\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 49.412008935937855\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 48.74429080007997\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 48.17089712752388\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 48.80289635257699\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 48.381350930275744\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 48.74906857808431\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.192572853253246\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.209235467815475\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.206374277506846\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.19525183305447\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.20014971279617\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.20992245648726\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -2228.9122884301883\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -4569.263761028002\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: -3549.5588349875334\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: -4950.166738588416\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: -3532.3732978519374\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -1772.7321873801304\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 76.94557585019373\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 76.89602956124362\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 76.94653281682655\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 76.94347936164203\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 76.94259268962244\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.94675552374018\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.04755086061093\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 76.84366393738613\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.42124488941774\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 83.90480362687661\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.3259662222853\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.23729963763536\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 83.86238857206368\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 83.8526907980465\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 83.7572419455923\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 83.799459280344\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 83.78584391319939\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 83.88346700225702\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 78.8186426312703\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 80.4803634046548\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 80.04803524010018\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 79.57465659442605\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 79.16847805525089\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 79.11327299872349\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.260129626461676\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.25722808231384\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.28936951283664\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.259071229162195\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.286790208485925\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.24945025459807\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: -2204.0170244773685\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: -1706.2484566791334\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: -1169.689844948696\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: -1488.3072916666667\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: -1354.821034714231\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: -2192.243434143652\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 76.93444665016962\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 76.93445531261986\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 76.93444992785349\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 76.9344693598365\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 76.93446748687428\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 76.9344642091904\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.69560996481043\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 84.92098052519054\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 85.27264719956855\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 80.91418997190627\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 84.96364275614421\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.07936747631078\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.90632508218997\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 84.45620639414727\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 84.22490851194135\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 83.88173171739538\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 85.01022852102733\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 84.01915423843566\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 84.05406478176549\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 84.15621180620329\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 83.91108487637325\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 84.22492511984852\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 83.66355403063166\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 83.88611462823175\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 46.379011167755614\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 46.27563010105285\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 46.26602628905974\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 46.521717191519855\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 46.572205867399695\n",
            "Layers: 3, Neurons: 64, Activation: ReLU, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 46.35185118740346\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: 58.415926079847004\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -183.0896126030504\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -170.1515013384188\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: 14.518398318566895\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -33.156960695513284\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: -181.56336928858906\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 46.706799374351014\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 44.79987350971616\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.75680901933823\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 47.917831015394796\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 47.56435282348351\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 48.94228960482791\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.296326223167824\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.253519822622245\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.335572874770115\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.33691173884482\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.35266276447536\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.34125189718087\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.175762691559704\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.18759421732721\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.17383262325016\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.18127543664182\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.17976057621045\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.180496701089744\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.179608613545575\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.163615621754275\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.16562944515685\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.16329786294766\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.172543688839795\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.18234366307404\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.18754353227345\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.1643936243328\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.17006662002281\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.174795444665385\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.18084289106684\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.171313288160405\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -569.2206641673858\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -694.9267353369535\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -409.582281313868\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -579.5903232283275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -265.227658147779\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -824.7612235387897\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 81.40957736127625\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 78.27603169314285\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: 79.46809981579885\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 81.13331448313922\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 80.74568080865518\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 78.99804144926257\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 54.98787412434919\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 55.642440888814924\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 55.313111972113795\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 55.970115687228\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 54.8671700536496\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 54.88542774787203\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 45.39013749225001\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 45.39488527650891\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 45.42112884470728\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 45.38030393155864\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 45.37162824899335\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 45.38170518822994\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.17925746652459\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.17148641978027\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.183791416483935\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.187207835597476\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.17549146651596\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.176261700525465\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.17795364949373\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.174918536491205\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.16750349090544\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.184158192691\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.1800023452002\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.175624686297944\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -98.1895434486029\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: 68.81224052449684\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -3.3964838248173423\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -92.04968088469745\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -33.786467744968476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -26.139952466091888\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 81.77581716873293\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 80.80809891338224\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 80.51375827708827\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 81.10252840929577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 80.99576435008633\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 80.84032340025966\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 74.71550622844221\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 74.95232207865654\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 74.8065983019069\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 74.81612582658505\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 74.71737863462366\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 74.84714588535486\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 46.26461261801214\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 46.15433715658257\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 46.32339660719889\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 46.23122059571867\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 46.293046689829154\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 46.32038200820686\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.220447048490584\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.19985615898469\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.21639007336228\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.215686381714846\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.223465334433996\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.225457499881884\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.168141011422605\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.1636405468668\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16674265877416\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16459405657452\n",
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.19344987777123\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.15073974734357\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: 54.697072761105446\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: 13.7066245033314\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: 45.179160644507846\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: 60.03363623580669\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: 66.9522673003647\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: 15.913504603085594\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 81.79644669890175\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 82.06170875540538\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 82.39042196504266\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 82.51629452253604\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 81.40677521957673\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 82.95765518639158\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 81.54359332436934\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 81.27037005905544\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 81.93730234688252\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 81.31136432935966\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 80.7534452613213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 81.5702932348277\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 50.51115545222344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 50.48160115832061\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 50.49108652305823\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 50.33680663238689\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 50.45430655126422\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 50.48503125450097\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 45.24641013017984\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 45.259525047055725\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 45.229232715529236\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 45.267103539260354\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 45.231334810164135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 45.25823716438612\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.18327460979016\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.180884277841734\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.1834765104359\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.18295779398428\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.193247573445205\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.18119535929055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: 76.87510921710928\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: 76.96199060812461\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 77.4229557284643\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 77.60225296752134\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: 77.8751088981204\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: 76.76699455826046\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 81.36167860963951\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 81.46842773636503\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 83.7967556337767\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 83.05514213084992\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 84.20664317954487\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 81.85463977399621\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 84.24651810617159\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 83.54835250160912\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 84.99056093986344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 84.56919407451295\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 84.1712908969787\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 83.1583844979787\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 67.1286392577727\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 66.92435936585761\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 66.8424462119784\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 67.04401608852304\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 66.9532669573668\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 67.04402941143005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 45.43181359905094\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 45.46651629193057\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 45.48586932301384\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 45.459813819472984\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 45.44862658071207\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 45.49105302579658\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.17090334500284\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.191157598657185\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.18381922994141\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.19958064007231\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.180216416849476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.19201992964854\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 77.04024989073606\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 76.93446526273165\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 76.98344817049987\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: 76.93436353747123\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 78.00289354518222\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 77.89480112044346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 83.18364626610466\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.87206654609004\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 83.11484679253934\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 83.2534967824148\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 83.03257034983633\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 83.71707468504735\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 84.57459625449727\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.07500805875831\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 83.7851491759095\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.66821573774706\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 82.33347972234091\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 85.2503824069393\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 83.14344436856216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 83.16625567290991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 82.8695094580846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 82.16891834184408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 83.4208937902125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 81.33732640976461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 47.315302019518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 47.348092300113606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 47.34965024801063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 47.289882695761776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 47.33078782111235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 47.35432263942436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.20665896271249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.20410186184929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.22657760113512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.21640233504027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.206299522054984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers: 3, Neurons: 64, Activation: Softmax, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.203218624484485\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 16, Accuracy: -178.90242640616387\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 32, Accuracy: -253.33270729571228\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 64, Accuracy: -17.57175621878475\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 128, Accuracy: -304.5436769579854\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 256, Accuracy: -37.60472957960801\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 10, Batch Size: 512, Accuracy: 30.441070849473885\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 16, Accuracy: 45.521500634195974\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 32, Accuracy: 43.115319589979585\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 64, Accuracy: 46.62893352514946\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 128, Accuracy: 45.4912545858076\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 256, Accuracy: 38.94897922616285\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 1, Batch Size: 512, Accuracy: 45.33158375549738\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 16, Accuracy: 45.245658494917926\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 32, Accuracy: 45.610831755945426\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 64, Accuracy: 45.76365451772505\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 128, Accuracy: 45.9572576666283\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 256, Accuracy: 45.56466065306469\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.1, Batch Size: 512, Accuracy: 45.28815075895872\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 16, Accuracy: 45.2357936193311\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 32, Accuracy: 45.222756861404335\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 64, Accuracy: 45.20274073183451\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 128, Accuracy: 45.2153376391244\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 256, Accuracy: 45.21121456636109\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.01, Batch Size: 512, Accuracy: 45.22598121659263\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 16, Accuracy: 45.16910004911391\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 32, Accuracy: 45.17889888799779\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 64, Accuracy: 45.189037913807375\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 128, Accuracy: 45.162972922365086\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 256, Accuracy: 45.164261246627\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.001, Batch Size: 512, Accuracy: 45.183489744385895\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 16, Accuracy: 45.176675887689086\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 32, Accuracy: 45.18118773192478\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 64, Accuracy: 45.15142696966586\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 128, Accuracy: 45.162247820059996\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 256, Accuracy: 45.16799194543574\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 1, LR: 0.0001, Batch Size: 512, Accuracy: 45.175579659140475\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 16, Accuracy: -3304.83723221769\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 32, Accuracy: -7046.835725387029\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 64, Accuracy: -11802.921163063647\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 128, Accuracy: -5152.3308468560035\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 256, Accuracy: -6272.893710471132\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 10, Batch Size: 512, Accuracy: -3160.9717468507542\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 16, Accuracy: 12.346770546653051\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 32, Accuracy: 16.378707744446952\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 64, Accuracy: -11.409027888302049\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 128, Accuracy: 0.34519605312851587\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 256, Accuracy: 35.64688109510193\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 1, Batch Size: 512, Accuracy: 39.808935836031836\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 16, Accuracy: 78.06586216727939\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 32, Accuracy: 78.00232191126055\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 64, Accuracy: 77.81951419594739\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 128, Accuracy: 77.07366797765827\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 256, Accuracy: 78.41496080194436\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.1, Batch Size: 512, Accuracy: 72.9015235622861\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 16, Accuracy: 48.61357582970241\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 32, Accuracy: 48.94491032080308\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 64, Accuracy: 49.35402891315351\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 128, Accuracy: 49.410915032977975\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 256, Accuracy: 48.88306833646469\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.01, Batch Size: 512, Accuracy: 48.9479232005815\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 16, Accuracy: 45.246450788549375\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 32, Accuracy: 45.21711459102846\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 64, Accuracy: 45.24083017028645\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 128, Accuracy: 45.20892645828635\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 256, Accuracy: 45.25544247021638\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.001, Batch Size: 512, Accuracy: 45.221328528649394\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 16, Accuracy: 45.16878240218703\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 32, Accuracy: 45.17667430771704\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 64, Accuracy: 45.1779980104403\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 128, Accuracy: 45.167717869606264\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 256, Accuracy: 45.155323731089084\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 10, LR: 0.0001, Batch Size: 512, Accuracy: 45.16256697849494\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 16, Accuracy: -1597.3291300812762\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 32, Accuracy: -1909.8535074307904\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 64, Accuracy: -829.703304256385\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 128, Accuracy: -1746.26561358371\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 256, Accuracy: -1173.6175674947688\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 10, Batch Size: 512, Accuracy: -1467.804572541091\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 16, Accuracy: 66.00364588301073\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 32, Accuracy: 73.30625939424074\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 64, Accuracy: 67.69755462965473\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 128, Accuracy: 55.53099804926889\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 256, Accuracy: 75.50781317218126\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 1, Batch Size: 512, Accuracy: 56.74552474481007\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 16, Accuracy: 81.89747679036927\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 32, Accuracy: 82.51803056194402\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 64, Accuracy: 80.7513667817403\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 128, Accuracy: 80.77923748746521\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 256, Accuracy: 82.07892684585343\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.1, Batch Size: 512, Accuracy: 80.66858770840919\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 16, Accuracy: 71.84371160281128\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 32, Accuracy: 71.6272391219408\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 64, Accuracy: 71.617096263328\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 128, Accuracy: 71.13041977613513\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 256, Accuracy: 71.0177051001323\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.01, Batch Size: 512, Accuracy: 71.6611863280056\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 16, Accuracy: 45.420020257041\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 32, Accuracy: 45.37892064791262\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 64, Accuracy: 45.426271500617496\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 128, Accuracy: 45.472348963621734\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 256, Accuracy: 45.464298618402964\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.001, Batch Size: 512, Accuracy: 45.49819684077282\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 16, Accuracy: 45.19093830798801\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 32, Accuracy: 45.18162555293844\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 64, Accuracy: 45.16942462886927\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 128, Accuracy: 45.16302987961917\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 256, Accuracy: 45.18265087673431\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 25, LR: 0.0001, Batch Size: 512, Accuracy: 45.18484566597919\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 16, Accuracy: -444.47999365630625\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 32, Accuracy: -653.3090150552918\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 64, Accuracy: -245.51478341661928\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 128, Accuracy: -251.36390172433795\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 256, Accuracy: -459.15705803028686\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 10, Batch Size: 512, Accuracy: -433.2628926144371\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 16, Accuracy: 80.62005771363336\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 32, Accuracy: 77.6092800437421\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 64, Accuracy: 74.956613291165\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 128, Accuracy: 79.29264214928689\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 256, Accuracy: 76.30057288990379\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 1, Batch Size: 512, Accuracy: 80.95835077748885\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 16, Accuracy: 83.9312407629308\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 32, Accuracy: 83.50267030564495\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 64, Accuracy: 83.43833352671668\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 128, Accuracy: 83.34546512052114\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 256, Accuracy: 83.86921507671138\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.1, Batch Size: 512, Accuracy: 83.53792726467705\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 16, Accuracy: 77.54379323308936\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 32, Accuracy: 76.13610200695393\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 64, Accuracy: 76.16364523328306\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 128, Accuracy: 76.04242699421545\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 256, Accuracy: 76.03912297180433\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.01, Batch Size: 512, Accuracy: 76.27752690741418\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 16, Accuracy: 47.148911483631565\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 32, Accuracy: 47.154966258298494\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 64, Accuracy: 46.94690451766162\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 128, Accuracy: 46.77249993357305\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 256, Accuracy: 47.27293431078697\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.001, Batch Size: 512, Accuracy: 46.8778717684112\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 16, Accuracy: 45.19348559681416\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 32, Accuracy: 45.207388783438184\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 64, Accuracy: 45.20559897710969\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 128, Accuracy: 45.22256738090788\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 256, Accuracy: 45.19094958899619\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 50, LR: 0.0001, Batch Size: 512, Accuracy: 45.19574924192445\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 16, Accuracy: -171.2835650193449\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 32, Accuracy: -48.274394724165106\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 64, Accuracy: 48.903825549179444\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 128, Accuracy: 44.47575110423414\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 256, Accuracy: -57.955987284272226\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 10, Batch Size: 512, Accuracy: -143.85209841887337\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 16, Accuracy: 79.13267188662992\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 32, Accuracy: 80.4327893595518\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 64, Accuracy: 79.71596634063312\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 128, Accuracy: 81.73622535205133\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 256, Accuracy: 81.08807165594361\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 1, Batch Size: 512, Accuracy: 76.40586656585799\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 16, Accuracy: 85.40325024048397\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 32, Accuracy: 85.5079283218618\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 64, Accuracy: 85.81452070546416\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 128, Accuracy: 85.67527272027253\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 256, Accuracy: 83.7639392094453\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.1, Batch Size: 512, Accuracy: 84.69930291953195\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 16, Accuracy: 76.15436267157737\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 32, Accuracy: 80.2782315735987\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 64, Accuracy: 81.4756511626776\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 128, Accuracy: 75.9343998378202\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 256, Accuracy: 77.0593693579208\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.01, Batch Size: 512, Accuracy: 79.76689251437698\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 16, Accuracy: 56.247463661635756\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 32, Accuracy: 56.374004980042656\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 64, Accuracy: 56.60603857973228\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 128, Accuracy: 56.27591104584266\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 256, Accuracy: 56.65032120833783\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.001, Batch Size: 512, Accuracy: 56.78108102295835\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 16, Accuracy: 45.2359810755647\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 32, Accuracy: 45.25386673682321\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 64, Accuracy: 45.24014688719965\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 128, Accuracy: 45.236294166401805\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 256, Accuracy: 45.255419839136756\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 100, LR: 0.0001, Batch Size: 512, Accuracy: 45.240305644611354\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 16, Accuracy: 8.337660216260879\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 32, Accuracy: 26.82114825561489\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 64, Accuracy: 72.07708074162187\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 128, Accuracy: -227.46362274251862\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 256, Accuracy: 31.63574018284512\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 10, Batch Size: 512, Accuracy: 58.36198106230929\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 16, Accuracy: 80.5007656625646\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 32, Accuracy: 81.33124213589988\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 64, Accuracy: 80.80274054406931\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 128, Accuracy: 80.76348817252455\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 256, Accuracy: 81.09080736967262\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 1, Batch Size: 512, Accuracy: 80.06709047602108\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 16, Accuracy: 85.28408323010585\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 32, Accuracy: 83.95832960295596\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 64, Accuracy: 82.74068290148378\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 128, Accuracy: 84.55980828468444\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 256, Accuracy: 83.37984629458379\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.1, Batch Size: 512, Accuracy: 80.48449603634225\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 16, Accuracy: 86.95171755394536\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 32, Accuracy: 86.89764410680311\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 64, Accuracy: 85.98087621640556\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 128, Accuracy: 87.15816244660185\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 256, Accuracy: 84.92880780439886\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.01, Batch Size: 512, Accuracy: 87.02111330785009\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 16, Accuracy: 78.96462648181016\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 32, Accuracy: 80.07195425435964\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 64, Accuracy: 80.20198727824655\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 128, Accuracy: 79.0180838711654\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 256, Accuracy: 79.12274794693784\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.001, Batch Size: 512, Accuracy: 78.9903311103891\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 16, Accuracy: 45.96880472166943\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 32, Accuracy: 45.82155362112569\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 64, Accuracy: 45.9143392773653\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 128, Accuracy: 45.83948330495915\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 256, Accuracy: 45.87834301817624\n",
            "Layers: 3, Neurons: 64, Activation: Tanh, Epochs: 250, LR: 0.0001, Batch Size: 512, Accuracy: 45.886899835384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonversi hasil eksperimen ke dalam format DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Menyimpan DataFrame hasil ke file CSV\n",
        "results_df.to_csv(\"mlp_regression_hidden_layer_123.csv\", index=False)\n",
        "\n",
        "# Menampilkan pesan sukses setelah penyimpanan file\n",
        "print(\"Hasil eksperimen telah berhasil disimpan ke file 'mlp_regression_hidden_layer_123.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-7Z6bldW5Ez",
        "outputId": "67a516c5-8113-4eab-e502-cff7c3f01809"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil eksperimen telah berhasil disimpan ke file 'mlp_regression_hidden_layer_123.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memilih hyperparameter yang relevan dan menghitung rata-rata MAE untuk setiap kombinasi\n",
        "hyperparameters = ['layers', 'neurons', 'activation', 'epochs', 'lr', 'batch_size']\n",
        "mean_mae_by_hyperparameter = results_df.groupby(hyperparameters)['mae'].mean().reset_index()\n",
        "\n",
        "# Membuat plot rata-rata MAE terhadap setiap hyperparameter\n",
        "for param in hyperparameters:\n",
        "    plt.figure(figsize=(10, 6))  # Menentukan ukuran plot\n",
        "    sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
        "    plt.title(f'Mean MAE vs. {param.capitalize()}', fontsize=14)  # Menambahkan judul\n",
        "    plt.xlabel(param.capitalize(), fontsize=12)  # Menambahkan label pada sumbu X\n",
        "    plt.ylabel('Mean MAE', fontsize=12)  # Menambahkan label pada sumbu Y\n",
        "    plt.xticks(rotation=45)  # Memutar label pada sumbu X agar mudah dibaca\n",
        "    plt.grid(True)  # Menambahkan grid pada plot untuk memudahkan pembacaan\n",
        "    plt.show()  # Menampilkan plot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U6PLoZiMXdja",
        "outputId": "2298cf8e-0613-4d70-efcc-44b11ed2c242"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAI3CAYAAACRaGpaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUj0lEQVR4nO3deViU9f7/8dcMIIuKG4qKG5mm5kYSSppbLllZtvw0LTU1s9SySEtKRY6padqxxaVFNEvT9LhkmYqmx9PRNBcqKfetrwriCoLiwNy/P7yYE4E6t4EzMM/HdXHVfOZzz7zv4Y3wmvu+P2MxDMMQAAAAAMBpVlcXAAAAAABFDUEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAFBqLxaK2bdu6ugwAKHAEKQC4RY4cOSKLxSKLxaLKlSsrKysr33m///67Y16tWrVubZEFKGcffH19debMmXznnDt3Tv7+/o6519O+fXtZLBY1bNjwuvNq1arleLxrfR05cuRmd+uW27hx4w33h6ACALeet6sLAABP4+3treTkZK1atUoPP/xwnvtnz54tq7V4vM/l7e2tK1euaP78+XrppZfy3D9//nxdvnxZ3t7e1wyWknTo0CFHoEhMTNTWrVvVvHnza8738vLSqFGjrnl/2bJlTe2HO2jWrJkeeuihfO8ryoEbAIoqghQA3GL33HOPfv75Z8XFxeUJUllZWfriiy/UoUMH/fvf/3ZRhQWndu3aMgxDc+bMyTdIxcXF6Y477pAk7d2795qPExcXJ8MwNHz4cE2ZMkWzZ8++bpDy9vbW2LFj/3b97iQ8PLzY7RMAFGXF4y1PAChC/P399eSTT+rbb7/VqVOnct33zTffKDk5Wf3797/m9oZhKC4uTi1btlRgYKACAgIUHh6uuLi4PHNPnDihmJgYtWjRQpUqVZKvr69q1aqlwYMH53luSXrmmWdksVh0+PBhvf/++6pXr558fX1Vs2ZNxcbGym63m97ffv36KSEhQTt37sw1/vPPP2vXrl3q16/fdbfPzs7W3LlzVaFCBY0fP1633367Fi5cqPT0dNO1OGvcuHGyWCyaN29evvcvXbpUFotFb775pmNs586deuKJJ1SjRg35+vqqYsWKuvvuuzV+/PhCqzM/OaeQPvPMM0pMTNSDDz6osmXLqlSpUurUqZN27NiR73ZHjx7VgAEDFBISohIlSqhatWoaMGCAjh07lu/8tLQ0xcbGqnHjxgoICFCZMmUUFham0aNHy2az5ZmfnJysvn37KigoSP7+/mrRooU2btyYZ97Jkyc1bNgw1alTR/7+/ipbtqzq16+v559/XhcuXPhbrw0AFCSCFAC4QP/+/ZWVlaXPP/8813hcXJzKly+vbt265budYRh66qmnNGDAAKWkpKhXr1569tlnlZ6ergEDBmj48OG55m/atElTp05VcHCwevbsqRdffFG1a9fWzJkzFRkZec0/TEeMGKFx48YpMjJSzz//vCRp7NixGj16tOl97du3r7y8vDRnzpxc47Nnz5aXl5f69Olz3e3XrFmj48ePq0ePHipRooR69+6ttLQ0LV682HQtznr66adlsVj0xRdf5Ht/zvetd+/ekqSEhATdc889+u6779SqVStFRUXpiSeeUEBAgD7++ONCq/N6Dh06pJYtW+rSpUt64YUX9PDDD2vDhg1q3bq1tm7dmmvuvn37dPfddysuLk7NmjXTq6++qrCwMMXFxSk8PFz79u3LNf/UqVOKiIjQ2LFj5eXlpRdeeEH9+/dX5cqVNWnSpDwh9/z582rVqpUSExPVu3dvPfbYY9q+fbs6d+6s3bt3O+ZlZGSoZcuW+uCDD1S7dm29+OKLeuaZZ1S3bl19/vnnSklJKbwXDADMMgAAt8Thw4cNSUbnzp0NwzCMhg0bGnfeeafj/pMnTxre3t7Giy++aBiGYfj6+ho1a9bM9Rgff/yxIcno16+fceXKFcd4Zmam0bVrV0OSsX37dsd4cnKykZaWlqeWzz77zJBkvPXWW7nG+/bta0gyQkNDjRMnTjjGU1JSjLJlyxqlS5c2MjMzndpfScYdd9xhGIZhPPTQQ0b58uWNy5cvG4ZhGJcvXzbKly9vdO3a1TAMw7jjjjuMa/1KeuyxxwxJxpYtWwzDMIyDBw8aFovFaNWqVb7za9asaXh5eRkxMTH5fs2cOdOp+lu1amV4eXnleh0MwzDOnDljlChRwggPD3eMRUVFGZKM5cuX53mc06dPO/V817JhwwZDktGsWbNr7lPOa2MY/+szScbIkSNzPdbq1asNSUajRo1yjbdr186QZHz00Ue5xqdPn25IMtq3b59r/PHHHzckGW+88UaeepOSkgybzea4nVPL4MGDjezsbMf4p59+akgyBg0a5Bj7+uuvDUnGyy+/nOdx09LSHP0DAO6AIAUAt8hfg9S7775rSDJ+/PFHwzAM4+233zYkGbt27TIMI/8g1bhxY6NkyZJGRkZGnsf/5ZdfDEnGq6++esNa7Ha7ERgYaLRt2zbXeE6QiouLy7NNzn2//PKLM7ubK0gtXbrUkGQsXLjQMAzDWLhwoSHJWLZsmWEY1w5Sp06dMnx8fIy6devmGm/VqpUhydizZ0+ebWrWrOn44z2/ryZNmjhV/0cffWRIMqZOnZprfMaMGYYkY9q0aY6xnCC1Zs0apx7bjJwgdb2vf/7zn475OX1WtmzZfEP0fffdlytwHz161JBkNGjQwLDb7bnmZmdnG/Xq1TMkGceOHTMM42rgt1gsRu3atXOF+WuRZJQsWTJPLTabzfD29jbuuusux1hOkIqOjnb69QEAV+HUPgBwkaefflo+Pj6Oa5vmzJmjsLAwNW3aNN/5GRkZ+vXXX1W2bFlNmjRJY8eOzfW1cOFCSdKePXtybbd06VJ17txZFStWlLe3tywWi6xWq1JTU3XixIl8n6tZs2Z5xqpVqybp6mlaZj300EOqVKmSY1/j4uJUqVKla65Cl+Ozzz6TzWZznEKXI+d0wPyuC5MkX19fGVffLMzzlZCQ4FTN3bt3l6+vb57TL7/44gt5e3urZ8+eueZarVY9+uij6t+/v7788ksdP37cqedx1qBBg665Ty+//HKe+WFhYSpVqlSe8XvvvVeStGvXLklyvB5t2rTJswS91WpV69atc83bvn27DMNQu3bt5OPj41TtdevWzVOLt7e3goODc/VT69atVaVKFb399tt68MEHNXPmTP32228yDMOp5wGAW4kg9SebNm1S165dVbVqVVksFi1fvtz0YxiGoSlTpqhu3bry9fVVSEjILb/QGEDRULFiRXXt2lULFy7UunXrtHfv3usuMnHu3DkZhqHjx48rNjY2z9eECRMkKdf1KVOnTtXjjz+uXbt2qVOnTnr11VcVExOjmJgYlSlTRpmZmfk+V2BgYJ4xb++rC71mZ2eb3lcfHx89/fTTWrdunTZv3qx169apd+/ejse8ltmzZ8tiseQJUt27d5efn5/mzZt33WXT/46yZcvqoYceUkJCgn777TdJ0sGDB7V582Z16tRJlSpVcsxt3ry5Nm7cqNatW2vBggXq1auXqlWrpoiICG3YsKFQ6ruR4ODg647nXB+Xmpp63flVqlTJNS9nu5CQEKdrya+fpKs99ed+KlOmjH788Uf16dNHP/74owYPHqw777xTNWvW1IwZM5x+PgC4FQhSf5Kenq4mTZpo+vTpN/0Yw4YN06effqopU6Zoz549+vrrrxUREVGAVQIoTgYMGKDU1FQ988wz8vPz01NPPXXNuTl/jDZr1uyaRyYMw3D84Z6VlaVx48apSpUq2r17t+bPn+84khUTE6MrV67ckn3MMWDAANntdnXv3l12u10DBgy47vzNmzdrz549Mgwjz4fsli1bVpcvX1ZSUpJWrVpVaDXnBLico1I5i0/8NdhJV4/0fPfddzp37pw2bNigqKgo/frrr3rwwQd16NChQqvxWpKTk687XqZMGUn/66trzU9KSso1L+czuAr6iFuOGjVqaO7cuUpJSdGuXbs0adIk2e12DRkyRF9++WWhPCcA3Aw+R+pPunTpoi5dulzz/szMTL355pv68ssvdf78eTVs2FCTJk1yfKL877//rpkzZ2r37t2Oz0UJDQ29FaUDKKI6d+6skJAQHT9+XE8++aTKlSt3zbmlS5dW/fr19fvvv+v8+fM3/FDZ06dP68KFC7rvvvtyHT2Rrp6edenSpYLYBac1aNBAzZs319atW9WiRQvVr1//uvNnz54t6eq/zVWrVs1z//nz5/Wvf/1Ls2fPzveDjQvCAw88oAoVKmjBggUaP3685s+fr9KlS+uRRx655jb+/v5q27at2rZtq7Jly2rMmDGKj4/XoEGDCqXGa9m1a5cuXryY55S6//znP5KunvonyXEq6aZNm2QYRq7T+wzD0KZNm3LNCw8Pl9Vq1YYNG2Sz2Zw+vc8sq9Wqpk2bqmnTpoqMjFTr1q319ddf5zqlEgBciSNSJgwdOlRbtmzRwoUL9csvv+j//b//p/vvv1/79++XJK1cuVK33XabvvnmG4WGhqpWrVp69tlndfbsWRdXDsBdeXl5afny5Vq2bJkmTpx4w/kvvfSSMjIyNHDgwHw/R+nw4cM6cuSIJKlSpUry9/fXzp07lZGR4Zhz7tw5vfjiiwW2D2bExcVp2bJljpB0LRcvXtRXX32lkiVL6quvvtKnn36a5+urr75StWrVtGrVKsdRk4Lm4+OjHj166NixY5o8ebL279+vxx9/XP7+/rnmbdmyRZcvX86zfc5RHj8/P8fY6dOntWfPHp0+fbpQas5x/vz5PKeWr1mzRuvXr1fDhg0d18HVqFFD7dq1U2JiYp5rzj7++GP9/vvvat++vapXry7p6imAjz/+uA4ePKjY2Ng8z3vq1KmbPt0yMTEx3yNj+b2OAOBqHJFy0rFjxzRnzhwdO3bM8c7o8OHDtXr1as2ZM0cTJkzQoUOHdPToUS1evFjz5s1Tdna2XnnlFT3xxBP6/vvvXbwHANxVeHi4wsPDnZo7aNAg/fjjj/rss8/03//+Vx06dFDVqlWVnJysPXv2aOvWrVqwYIFq1aolq9WqwYMHa+rUqWrSpIm6du2q1NRUfffdd6pZs2a+R3kKW4MGDdSgQYMbzlu0aJEuXryovn375rtggnT1iEWfPn00YcIEffbZZ3r99dcd92VlZWns2LHXfPwnn3xS9erVc6rm3r17a8aMGRozZozj9l9NmjTJ8RlNoaGh8vPz086dO7V+/XrddtttevTRRx1zP/zwQ8XGxiomJua6Nf7V9u3brznfz89PI0eOzDV27733aubMmY4jgEeOHNHixYvl7++vTz/9NNfcmTNnqlWrVho4cKBWrlypBg0aKDExUV9//bUqVqyomTNn5po/Y8YM7d69W+PHj9eqVavUvn17GYahffv2ae3atUpOTr7hEdP8xMfHa8SIEWrZsqXq1q2rChUq6NChQ/r666/l5+enIUOGmH5MACg0t2h1wCJHf1qW1zAM45tvvnEs4frnL29vb6N79+6GYRjGwIEDDUnG3r17Hdvt2LHjmkv0AvAsf13+/EbyW/48x6JFi4wOHToY5cqVM3x8fIyQkBCjbdu2xtSpU42UlBTHvCtXrhjjx4836tSpY/j6+ho1atQwXn31VSMtLc2oWbNmnsfPWeL88OHDeZ4zJibGkGRs2LDBqfr1p+XPb+Svy59HRkY69Vz79u0zJOVaHv1Gy5//9d93Z9SpU8eQZFSrVi3XZyHlWL16tdGnTx/jjjvuMEqXLm2UKlXKaNCggfHGG2/k+n4Yxv9ex5iYGKee25nlz8uUKeOYn9Nnffv2NXbv3m088MADRmBgoFGyZEmjQ4cOuT5n7M+OHDli9OvXz6hSpYrh7e1tVKlSxejXr59x5MiRfOdfuHDBGD16tFGvXj3D19fXKFOmjNG0aVNjzJgxuZZFl2S0adMm38f4aw/+9ttvxrBhw4ywsDCjQoUKhq+vr3HbbbcZffv2NRITE516vQDgVrEYBmuK5sdisWjZsmXq1q2bpKvvjj711FNKTEyUl5dXrrmlSpVS5cqVFRMTowkTJshmsznuu3TpkgICArR27Vp17NjxVu4CAMADHTlyRKGhoerbt6/mzp3r6nIAoNji1D4nhYWFKTs7W6dOnXJ8BsdftWzZUllZWTp48KBq164tSdq3b58kqWbNmresVgAAAACFiyD1JxcvXtSBAwcctw8fPqyEhASVL19edevW1VNPPaU+ffpo6tSpCgsLU0pKitavX6/GjRvrwQcfVIcOHXTXXXepf//+mjZtmmO51o4dO6pu3bou3DMAAAAABYlV+/5k+/btCgsLcywJGxUVpbCwMMcFxnPmzFGfPn306quv6o477lC3bt30008/qUaNGpKuXvi8cuVKBQUFqXXr1nrwwQdVv359LVy40GX7BAAAAKDgcY0UAAAAAJjEESkAAAAAMIkgBQAAAAAmefxiE3a7XSdOnFDp0qVlsVhcXQ4AAAAAFzEMQ2lpaapataqs1usfc/L4IHXixAlVr17d1WUAAAAAcBN//PGHqlWrdt05Hh+kSpcuLenqixUYGOjialzDZrNp7dq16tSpk3x8fFxdDlyAHgA9AHoAEn0AeiA1NVXVq1d3ZITr8fgglXM6X2BgoEcHqYCAAAUGBnrkDwzoAdADoAdwFX0AeuAqZy75YbEJAAAAADDJrYLUpk2b1LVrV1WtWlUWi0XLly+/4TaZmZl68803VbNmTfn6+qpWrVqKi4sr/GIBAAAAeCy3OrUvPT1dTZo0Uf/+/fXYY485tU337t2VnJys2bNn6/bbb9fJkydlt9sLuVIAAAAAnsytglSXLl3UpUsXp+evXr1a//73v3Xo0CGVL19eklSrVq1Cqg4AAAAArnKrIGXW119/rfDwcE2ePFmff/65SpYsqYcffljjxo2Tv79/vttkZmYqMzPTcTs1NVXS1QvrbDbbLanb3eTst6fuP+gB0AOgB3AVfQBP7wEz+12kg9ShQ4f0ww8/yM/PT8uWLdPp06c1ePBgnTlzRnPmzMl3m4kTJyo2NjbP+Nq1axUQEFDYJbu1+Ph4V5cAF6MHQA+AHoBEH8BzeyAjI8PpuRbDMIxCrOWmWSwWLVu2TN26dbvmnE6dOuk///mPkpKSVKZMGUnS0qVL9cQTTyg9PT3fo1L5HZGqXr26Tp8+7dHLn8fHx6tjx44evcylJ6MHQA+AHoBEH4AeSE1NVVBQkC5cuHDDbFCkj0hVqVJFISEhjhAlSfXr15dhGPq///s/1alTJ882vr6+8vX1zTPu4+Pjkc3yZ7wGoAdAD4AegEQfwHN7wMw+u9Xy52a1bNlSJ06c0MWLFx1j+/btk9VqVbVq1VxYGQAAAIDizK2C1MWLF5WQkKCEhARJ0uHDh5WQkKBjx45JkqKjo9WnTx/H/F69eqlChQrq16+ffvvtN23atEkjRoxQ//79r7nYBAAAAAD8XW4VpLZv366wsDCFhYVJkqKiohQWFqYxY8ZIkk6ePOkIVZJUqlQpxcfH6/z58woPD9dTTz2lrl276v3333dJ/QAAAAA8g1tdI9W2bVtdb+2LuXPn5hmrV6+ex64qAgAAAMA13OqIFAAAAAAUBQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYJJbfY5UUbJkW4qrSyg49iz5SFqx47RkLR4t8URERVeXAAAAgGKMI1IAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCS3ClKbNm1S165dVbVqVVksFi1fvtzpbf/73//K29tbTZs2LbT6AAAAAEBysyCVnp6uJk2aaPr06aa2O3/+vPr06aP77ruvkCoDAAAAgP/xdnUBf9alSxd16dLF9HbPP/+8evXqJS8vL1NHsQAAAADgZrhVkLoZc+bM0aFDh/TFF1/orbfeuuH8zMxMZWZmOm6npqZKkmw2m2w2m/NPbM8yXavbsmfn/m8xYOp7CcfrxevmuegB0AOQ6APQA2b2u0gHqf3792vkyJH6z3/+I29v53Zl4sSJio2NzTO+du1aBQQEOP3cPk7PLDp8Una4uoQCs2qVqysomuLj411dAlyMHgA9AIk+gOf2QEZGhtNzi2yQys7OVq9evRQbG6u6des6vV10dLSioqIct1NTU1W9enV16tRJgYGBTj/Oih2nTdXr1uzZ8knZIVvFZpLVy9XVFIhHmgW5uoQixWazKT4+Xh07dpSPT3F8mwA3Qg+AHoBEH4AeyDlbzRlFNkilpaVp+/bt2rVrl4YOHSpJstvtMgxD3t7eWrt2rdq3b59nO19fX/n6+uYZ9/HxMdcs1iL70l2b1avY7Jcn/uAXBNM/Byh26AHQA5DoA3huD5jZ5yL7V3NgYKB+/fXXXGMzZszQ999/ryVLlig0NNRFlQEAAAAo7twqSF28eFEHDhxw3D58+LASEhJUvnx51ahRQ9HR0Tp+/LjmzZsnq9Wqhg0b5tq+UqVK8vPzyzMOAAAAAAXJrYLU9u3b1a5dO8ftnGuZ+vbtq7lz5+rkyZM6duyYq8oDAAAAAEluFqTatm0rwzCuef/cuXOvu/3YsWM1duzYgi0KAAAAAP7C6uoCAAAAAKCoIUgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAktwpSmzZtUteuXVW1alVZLBYtX778uvOXLl2qjh07qmLFigoMDFRkZKTWrFlza4oFAAAA4LHcKkilp6erSZMmmj59ulPzN23apI4dO2rVqlXasWOH2rVrp65du2rXrl2FXCkAAAAAT+bt6gL+rEuXLurSpYvT86dNm5br9oQJE7RixQqtXLlSYWFhBVwdAAAAAFzlVkek/i673a60tDSVL1/e1aUAAAAAKMbc6ojU3zVlyhRdvHhR3bt3v+aczMxMZWZmOm6npqZKkmw2m2w2m/NPZs+66Trdjj0793+LAVPfSzheL143z0UPgB6ARB+AHjCz3xbDMIxCrOWmWSwWLVu2TN26dXNq/oIFCzRw4ECtWLFCHTp0uOa8sWPHKjY2Nt/tAwICbrZcAAAAAEVcRkaGevXqpQsXLigwMPC6c4tFkFq4cKH69++vxYsX68EHH7zu3PyOSFWvXl2nT5++4Yv1Zyt2nHZ6rtuzZ8snZYdsFZtJVi9XV1MgHmkW5OoSihSbzab4+Hh17NhRPj4+ri4HLkAPgB6ARB+AHkhNTVVQUJBTQarIn9r35Zdfqn///lq4cOENQ5Qk+fr6ytfXN8+4j4+PuWaxFvmXLi+rV7HZL0/8wS8Ipn8OUOzQA6AHINEH8NweMLPPbvVX88WLF3XgwAHH7cOHDyshIUHly5dXjRo1FB0drePHj2vevHmSrp6O17dvX7333ntq3ry5kpKSJEn+/v4qU6aMS/YBAAAAQPHnVqv2bd++XWFhYY6ly6OiohQWFqYxY8ZIkk6ePKljx4455n/88cfKysrSkCFDVKVKFcfXsGHDXFI/AAAAAM/gVkek2rZtq+tdsjV37txctzdu3Fi4BQEAAABAPtzqiBQAAAAAFAUEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACT3CpIbdq0SV27dlXVqlVlsVi0fPnyG26zceNG3XXXXfL19dXtt9+uuXPnFnqdAAAAADybWwWp9PR0NWnSRNOnT3dq/uHDh/Xggw+qXbt2SkhI0Msvv6xnn31Wa9asKeRKAQAAAHgyb1cX8GddunRRly5dnJ4/a9YshYaGaurUqZKk+vXr64cfftA///lPde7cubDKBAAAAODh3CpImbVlyxZ16NAh11jnzp318ssvX3ObzMxMZWZmOm6npqZKkmw2m2w2m/NPbs8yVatbs2fn/m8xYOp7CcfrxevmuegB0AOQ6APQA2b2u0gHqaSkJAUHB+caCw4OVmpqqi5duiR/f/8820ycOFGxsbF5xteuXauAgACnn9vHfLluzydlh6tLKDCrVrm6gqIpPj7e1SXAxegB0AOQ6AN4bg9kZGQ4PbdIB6mbER0draioKMft1NRUVa9eXZ06dVJgYKDTj7Nix+nCKM817NnySdkhW8VmktXL1dUUiEeaBbm6hCLFZrMpPj5eHTt2lI9PcXybADdCD4AegEQfgB7IOVvNGUU6SFWuXFnJycm5xpKTkxUYGJjv0ShJ8vX1la+vb55xHx8fc81iLdIvXf6sXsVmvzzxB78gmP45QLFDD4AegEQfwHN7wMw+u9WqfWZFRkZq/fr1ucbi4+MVGRnpoooAAAAAeAK3ClIXL15UQkKCEhISJF1d3jwhIUHHjh2TdPW0vD59+jjmP//88zp06JBee+017dmzRzNmzNBXX32lV155xRXlAwAAAPAQbhWktm/frrCwMIWFhUmSoqKiFBYWpjFjxkiSTp486QhVkhQaGqpvv/1W8fHxatKkiaZOnapPP/2Upc8BAAAAFCq3uiCmbdu2MgzjmvfPnTs332127dpViFUBAAAAQG5udUQKAAAAAIoCghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMcjpITZ48Wb///rvjdnZ2trZt26aLFy/mmfvjjz+qf//+BVMhAAAAALgZp4PUyJEjcy0zfv78eUVGRmrbtm155h48eFCfffZZwVQIAAAAAG7mb53ad73PfAIAAACA4oprpAAAAADAJIIUAAAAAJhEkAIAAAAAk7zNTF61apWSkpIkSRkZGbJYLFq8eLESEhJyzduxY0eBFQgAAAAA7sZUkFqwYIEWLFiQa+yjjz7Kd67FYrn5qgAAAADAjTkdpA4fPlyYdQAAAABAkeF0kKpZs6apB7bb7aaLAQAAAICioMAXm/jpp5/08ssvKyQkpKAfGgAAAADcgqlrpK7lwIEDmj9/vhYsWKADBw7Iy8tLrVq1KoiHBgAAAAC3c9NB6tSpU1q4cKHmz5+v7du3S5Luu+8+jR07Vg888IDKlClTYEUCAAAAgDsxdWpfenq6Pv/8c91///2qVq2aRo4cqRo1amjKlCkyDEPPP/+8evbsSYgCAAAAUKw5HaR69uyp4OBgPfvss/Ly8lJcXJxOnTqlxYsX6+GHHy7MGgEAAADArTh9at+iRYsUGhqquLg4tWnTpjBrAgAAAAC35vQRqeHDh8tms6l9+/Zq1KiRJk6cqEOHDhVmbQAAAADglpwOUpMnT9axY8e0bt06NW/eXO+8847q1Kmj5s2b66OPPpLFYinMOgEAAADAbZj+HKl27drp008/VVJSkr766itVq1ZNH3zwgQzDUGxsrCZMmKBff/21MGoFAAAAALdw0x/IW6JECT3++OP617/+paSkJH300UcqX768Ro8eraZNm+q2224ryDoBAAAAwG3cdJD6szJlymjgwIHasGGDjh49qgkTJqh06dIF8dAAAAAA4HYKJEj9WbVq1fT666/r559/LuiHBgAAAAC34PTy5zt37jT94HfddZfpbQAAAADA3TkdpMLDw51emc8wDFksFmVnZ990YQAAAADgrpwOUpLk5+enBx98UJ07d5a3t6lNAQAAAKDYcDoNffTRR1qwYIGWLl2qjRs36oknnlCvXr3UqlWrwqwPAAAAANyO04tN/HlVvhEjRujHH39U69atVatWLUVHR+uXX34pzDoBAAAAwG2YXrUvJCREI0aM0M6dO5WYmKinn35aX331lcLCwtSoUSOtWbOmMOoEAAAAALfxt5Y/r1+/vt566y0tW7ZMbdq0UWJiorZu3VpQtQEAAACAW7rpIHX48GFNmDBBjRo1UlhYmP744w+NGjVKzzzzTAGWBwAAAADux9TSe6dOndKiRYu0YMECbd26VZUrV1b37t01e/ZsRUREFFaNAAAAAOBWnA5SnTp10oYNG1SqVCk99thjGjdunNq3by+r9W+dHQgAAAAARY7TQWrdunXy9/fX3XffrZSUFL3//vt6//33rznfYrFoxYoVBVIkAAAAALgTp4NUjRo1ZLFYtH//fqfmWyyWmy4KAAAAANyZ00HqyJEjhVgGAAAAABQdXOAEAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATHLLIDV9+nTVqlVLfn5+at68ubZt23bd+dOmTdMdd9whf39/Va9eXa+88oouX758i6oFAAAA4Gmc/hypv1qzZo1mz56tQ4cO6dy5czIMI9f9FotFBw8eNP24ixYtUlRUlGbNmqXmzZtr2rRp6ty5s/bu3atKlSrlmb9gwQKNHDlScXFxuueee7Rv3z4988wzslgsevfdd2929wAAAADgmm4qSL3zzjsaOXKkgoODFRERoUaNGhVYQe+++64GDhyofv36SZJmzZqlb7/9VnFxcRo5cmSe+Zs3b1bLli3Vq1cvSVKtWrXUs2dPbd26tcBqAgAAAIA/u6kg9d5776l9+/ZatWqVfHx8CqyYK1euaMeOHYqOjnaMWa1WdejQQVu2bMl3m3vuuUdffPGFtm3bpoiICB06dEirVq1S7969852fmZmpzMxMx+3U1FRJks1mk81mc75Ye5bzc92dPTv3f4sBU99LOF4vXjfPRQ+AHoBEH4AeMLPfNxWkzp07pyeeeKJAQ5QknT59WtnZ2QoODs41HhwcrD179uS7Ta9evXT69Gm1atVKhmEoKytLzz//vN54441850+cOFGxsbF5xteuXauAgACnay3YPXcPPik7XF1CgVm1ytUVFE3x8fGuLgEuRg+AHoBEH8BzeyAjI8PpuTcVpCIiIrR3796b2bTAbdy4URMmTNCMGTPUvHlzHThwQMOGDdO4ceM0evToPPOjo6MVFRXluJ2amqrq1aurU6dOCgwMdPp5V+w4XSD1uwV7tnxSdshWsZlk9XJ1NQXikWZBri6hSLHZbIqPj1fHjh0L/A0SFA30AOgBSPQB6IGcs9WccVNBasaMGerSpYvCw8Md1yYVhKCgIHl5eSk5OTnXeHJysipXrpzvNqNHj1bv3r317LPPSpIaNWqk9PR0Pffcc3rzzTdlteZemNDX11e+vr55HsfHx8dcs1hvep0O92X1Kjb75Yk/+AXB9M8Bih16APQAJPoAntsDZvb5ppY/79Gjh7KystS7d2+VKVNGd955pxo3bpzrq0mTJqYft0SJEmrWrJnWr1/vGLPb7Vq/fr0iIyPz3SYjIyNPWPLyunpU5a8rCQIAAABAQbipww/ly5dXhQoVVKdOnYKuR1FRUerbt6/Cw8MVERGhadOmKT093bGKX58+fRQSEqKJEydKkrp27ap3331XYWFhjlP7Ro8era5duzoCFQAAAAAUpJsKUhs3bizgMv6nR48eSklJ0ZgxY5SUlKSmTZtq9erVjgUojh07lusI1KhRo2SxWDRq1CgdP35cFStWVNeuXTV+/PhCqxEAAACAZ3PLC2KGDh2qoUOH5nvfX0Oct7e3YmJiFBMTcwsqAwAAAIC/GaRsNpv27NmjCxcuyG6357m/devWf+fhAQAAAMAt3VSQstvtio6O1owZM6671np2dvH5gFcAAAAAyHFTq/ZNmDBB77zzjp5++mnNmzdPhmHo7bff1qxZsxwr9q1Zs6agawUAAAAAt3BTQWru3Lnq3r27Zs6cqfvvv1+S1KxZMw0cOFBbt26VxWLR999/X6CFAgAAAIC7uKkg9X//939q3769JDk+3Pby5cuSrn4W1NNPP63PP/+8gEoEAAAAAPdyU0GqQoUKunjxoiSpVKlSCgwM1KFDh3LNOXfu3N+vDgAAAADc0E0tNhEWFqaffvrJcbtdu3aaNm2awsLCZLfb9f7776tJkyYFViQAAAAAuJObOiL13HPPKTMzU5mZmZKk8ePH6/z582rdurXatGmj1NRUTZ06tUALBQAAAAB3cVNHpB5++GE9/PDDjtsNGjTQwYMHtXHjRnl5eemee+5R+fLlC6xIAAAAAHAnf+sDef+sTJkyeuSRRwrq4QAAAADAbd3UqX3S1Q/bXbhwoQYNGqRHH31Uv/76qyTpwoULWrp0qZKTkwusSAAAAABwJzcVpM6fP6+WLVuqV69e+vLLL/X1118rJSVF0tVV/F566SW99957BVooAAAAALiLmwpSI0eOVGJiotasWaNDhw7JMAzHfV5eXnriiSe0atWqAisSAAAAANzJTQWp5cuX68UXX1THjh1lsVjy3F+3bl0dOXLk79YGAAAAAG7ppoLUhQsXFBoaes37bTabsrKybrooAAAAAHBnNxWkateurZ07d17z/rVr16pBgwY3XRQAAAAAuLObClLPPvus4uLitGjRIsf1URaLRZmZmXrzzTe1evVqDRo0qEALBQAAAAB3cVOfIzVs2DAlJiaqZ8+eKlu2rCSpV69eOnPmjLKysjRo0CANGDCgIOsEAAAAALdxU0HKYrHok08+Ud++fbVkyRLt379fdrtdtWvXVvfu3dW6deuCrhMAAAAA3MZNBakcrVq1UqtWrQqqFgAAAAAoEm7qGikAAAAA8GROH5F6+OGHTT2wxWLRihUrTBcEAAAAAO7O6SD1zTffyM/PT5UrV3as1Hc9+X1QLwAAAAAUB04HqZCQEB0/flxBQUHq1auXnnzySVWuXLkwawMAAAAAt+T0NVJ//PGHNmzYoLCwMI0bN07Vq1dXhw4dNGfOHKWlpRVmjQAAAADgVkwtNtGmTRt99NFHSkpK0pIlS1ShQgUNHTpUlSpV0mOPPaYlS5YoMzOzsGoFAAAAALdwU6v2+fj46JFHHtGiRYuUnJzsCFc9evTQ5MmTC7pGAAAAAHArf2v588zMTK1Zs0YrVqzQrl275Ofnp1q1ahVQaQAAAADgnkwHKbvdrjVr1uiZZ55RcHCwevbsqUuXLumTTz7RqVOn1Lt378KoEwAAAADchtOr9m3evFkLFizQ4sWLdebMGbVo0UITJkxQ9+7dFRQUVJg1AgAAAIBbcTpItWrVSv7+/nrggQfUs2dPxyl8x44d07Fjx/Ld5q677iqQIgEAAADAnTgdpCTp0qVL+te//qWlS5ded55hGLJYLMrOzv5bxQEAAACAO3I6SM2ZM6cw6wAAAACAIsPpINW3b9/CrAMAAAAAioy/tfw5AAAAAHgighQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGCSWwap6dOnq1atWvLz81Pz5s21bdu2684/f/68hgwZoipVqsjX11d169bVqlWrblG1AAAAADyNt6sL+KtFixYpKipKs2bNUvPmzTVt2jR17txZe/fuVaVKlfLMv3Llijp27KhKlSppyZIlCgkJ0dGjR1W2bNlbXzwAAAAAj+B2Qerdd9/VwIED1a9fP0nSrFmz9O233youLk4jR47MMz8uLk5nz57V5s2b5ePjI0mqVavWrSwZAAAAgIdxqyB15coV7dixQ9HR0Y4xq9WqDh06aMuWLflu8/XXXysyMlJDhgzRihUrVLFiRfXq1Uuvv/66vLy88szPzMxUZmam43ZqaqokyWazyWazOV+sPcv5ue7Onp37v8WAqe8lHK8Xr5vnogdAD0CiD0APmNlvtwpSp0+fVnZ2toKDg3ONBwcHa8+ePfluc+jQIX3//fd66qmntGrVKh04cECDBw+WzWZTTExMnvkTJ05UbGxsnvG1a9cqICDA6Vp9nJ5ZdPik7HB1CQWGS+RuTnx8vKtLgIvRA6AHINEH8NweyMjIcHquWwWpm2G321WpUiV9/PHH8vLyUrNmzXT8+HG98847+Qap6OhoRUVFOW6npqaqevXq6tSpkwIDA51+3hU7ThdI/W7Bni2flB2yVWwmWfMexSuKHmkW5OoSihSbzab4+Hh17NjRcYosPAs9AHoAEn0AeiDnbDVnuFWQCgoKkpeXl5KTk3ONJycnq3LlyvluU6VKFfn4+OQ6ja9+/fpKSkrSlStXVKJEiVzzfX195evrm+dxfHx8zDWL1a1euoJh9So2++WJP/gFwfTPAYodegD0ACT6AJ7bA2b22a2WPy9RooSaNWum9evXO8bsdrvWr1+vyMjIfLdp2bKlDhw4ILvd7hjbt2+fqlSpkidEAQAAAEBBcKsgJUlRUVH65JNP9Nlnn+n333/XCy+8oPT0dMcqfn369Mm1GMULL7ygs2fPatiwYdq3b5++/fZbTZgwQUOGDHHVLgAAAAAo5tzuPK4ePXooJSVFY8aMUVJSkpo2barVq1c7FqA4duyYrNb/5b/q1atrzZo1euWVV9S4cWOFhIRo2LBhev311121CwAAAACKObcLUpI0dOhQDR06NN/7Nm7cmGcsMjJSP/74YyFXBQAAAABXud2pfQAAAADg7ghSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwyS2D1PTp01WrVi35+fmpefPm2rZtm1PbLVy4UBaLRd26dSvcAgEAAAB4NLcLUosWLVJUVJRiYmK0c+dONWnSRJ07d9apU6euu92RI0c0fPhw3XvvvbeoUgAAAACeyu2C1LvvvquBAweqX79+atCggWbNmqWAgADFxcVdc5vs7Gw99dRTio2N1W233XYLqwUAAADgidwqSF25ckU7duxQhw4dHGNWq1UdOnTQli1brrndP/7xD1WqVEkDBgy4FWUCAAAA8HDeri7gz06fPq3s7GwFBwfnGg8ODtaePXvy3eaHH37Q7NmzlZCQ4NRzZGZmKjMz03E7NTVVkmSz2WSz2Zwv1p7l/Fx3Z8/O/d9iwNT3Eo7Xi9fNc9EDoAcg0QegB8zst1sFKbPS0tLUu3dvffLJJwoKCnJqm4kTJyo2NjbP+Nq1axUQEOD0c/s4PbPo8EnZ4eoSCsyqVa6uoGiKj493dQlwMXoA9AAk+gCe2wMZGRlOz3WrIBUUFCQvLy8lJyfnGk9OTlblypXzzD948KCOHDmirl27OsbsdrskydvbW3v37lXt2rVzbRMdHa2oqCjH7dTUVFWvXl2dOnVSYGCg07Wu2HHa6bluz54tn5QdslVsJlm9XF1NgXikmXPBGlfZbDbFx8erY8eO8vEpjm8T4EboAdADkOgD0AM5Z6s5w62CVIkSJdSsWTOtX7/esYS53W7X+vXrNXTo0Dzz69Wrp19//TXX2KhRo5SWlqb33ntP1atXz7ONr6+vfH1984z7+PiYaxarW710BcPqVWz2yxN/8AuC6Z8DFDv0AOgBSPQBPLcHzOyz2/3VHBUVpb59+yo8PFwRERGaNm2a0tPT1a9fP0lSnz59FBISookTJ8rPz08NGzbMtX3ZsmUlKc84AAAAABQUtwtSPXr0UEpKisaMGaOkpCQ1bdpUq1evdixAcezYMVmtbrXYIAAAAAAP43ZBSpKGDh2a76l8krRx48brbjt37tyCLwgAAAAA/oRDOwAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCS3DFLTp09XrVq15Ofnp+bNm2vbtm3XnPvJJ5/o3nvvVbly5VSuXDl16NDhuvMBAAAA4O9yuyC1aNEiRUVFKSYmRjt37lSTJk3UuXNnnTp1Kt/5GzduVM+ePbVhwwZt2bJF1atXV6dOnXT8+PFbXDkAAAAAT+F2Qerdd9/VwIED1a9fPzVo0ECzZs1SQECA4uLi8p0/f/58DR48WE2bNlW9evX06aefym63a/369be4cgAAAACewtvVBfzZlStXtGPHDkVHRzvGrFarOnTooC1btjj1GBkZGbLZbCpfvny+92dmZiozM9NxOzU1VZJks9lks9mcL9ae5fxcd2fPzv3fYsDU9xKO14vXzXPRA6AHINEHoAfM7LdbBanTp08rOztbwcHBucaDg4O1Z88epx7j9ddfV9WqVdWhQ4d87584caJiY2PzjK9du1YBAQFO1+rj9Myiwydlh6tLKDCrVrm6gqIpPj7e1SXAxegB0AOQ6AN4bg9kZGQ4PdetgtTf9fbbb2vhwoXauHGj/Pz88p0THR2tqKgox+3U1FTHdVWBgYFOP9eKHaf/dr1uw54tn5QdslVsJlm9XF1NgXikWZCrSyhSbDab4uPj1bFjR/n4FMe3CXAj9ADoAUj0AeiBnLPVnOFWQSooKEheXl5KTk7ONZ6cnKzKlStfd9spU6bo7bff1rp169S4ceNrzvP19ZWvr2+ecR8fH3PNYnWrl65gWL2KzX554g9+QTD9c4Bihx4APQCJPoDn9oCZfXarxSZKlCihZs2a5VooImfhiMjIyGtuN3nyZI0bN06rV69WeHj4rSgVAAAAgAdzu8MPUVFR6tu3r8LDwxUREaFp06YpPT1d/fr1kyT16dNHISEhmjhxoiRp0qRJGjNmjBYsWKBatWopKSlJklSqVCmVKlXKZfsBAAAAoPhyuyDVo0cPpaSkaMyYMUpKSlLTpk21evVqxwIUx44dk9X6vwNpM2fO1JUrV/TEE0/kepyYmBiNHTv2VpYOAAAAwEO4XZCSpKFDh2ro0KH53rdx48Zct48cOVL4BQEAAADAn7jVNVIAAAAAUBQQpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMcssgNX36dNWqVUt+fn5q3ry5tm3bdt35ixcvVr169eTn56dGjRpp1apVt6hSAAAAAJ7I7YLUokWLFBUVpZiYGO3cuVNNmjRR586dderUqXznb968WT179tSAAQO0a9cudevWTd26ddPu3btvceUAAAAAPIXbBal3331XAwcOVL9+/dSgQQPNmjVLAQEBiouLy3f+e++9p/vvv18jRoxQ/fr1NW7cON1111368MMPb3HlAAAAADyFt6sL+LMrV65ox44dio6OdoxZrVZ16NBBW7ZsyXebLVu2KCoqKtdY586dtXz58nznZ2ZmKjMz03H7woULkqSzZ8/KZrM5XWtG2jmn57o9e7Z8MjJkSzsvWb1cXU2BOHOm8N8jSN2Yf7gvirIMizJsITqy4j15WwxXl1MgAtv2d3UJRYrNZlNGRobOnDkjHx8fV5cDF6AHINEHoAfS0tIkSYZx47+H3CpInT59WtnZ2QoODs41HhwcrD179uS7TVJSUr7zk5KS8p0/ceJExcbG5hkPDQ29yaoBuKfXXF0AAAAootLS0lSmTJnrznGrIHUrREdH5zqCZbfbdfbsWVWoUEEWi8WFlblOamqqqlevrj/++EOBgYGuLgcuQA+AHgA9AIk+AD1gGIbS0tJUtWrVG851qyAVFBQkLy8vJScn5xpPTk5W5cqV892mcuXKpub7+vrK19c311jZsmVvvuhiJDAw0CN/YPA/9ADoAdADkOgDeHYP3OhIVA63WmyiRIkSatasmdavX+8Ys9vtWr9+vSIjI/PdJjIyMtd8SYqPj7/mfAAAAAD4u9zqiJQkRUVFqW/fvgoPD1dERISmTZum9PR09evXT5LUp08fhYSEaOLEiZKkYcOGqU2bNpo6daoefPBBLVy4UNu3b9fHH3/syt0AAAAAUIy5XZDq0aOHUlJSNGbMGCUlJalp06ZavXq1Y0GJY8eOyWr934G0e+65RwsWLNCoUaP0xhtvqE6dOlq+fLkaNmzoql0ocnx9fRUTE5PnlEd4DnoA9ADoAUj0AegBMyyGM2v7AQAAAAAc3OoaKQAAAAAoCghSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAQC6sQQQAwI0RpAAAkqTz589LkiwWi2sLAeBWDMPgDRYgHwQpD5Wdne3qEuDm+KXpWRISEtS1a1f98ssvri4FboR/BzxbZmamJCkrK4s3WDzQkSNH9Mknn2j27Nlau3atq8txS273gbwofPv27dPKlSvVq1cvValSxdXlwMX27dun2bNn69SpU2ratKkeeOAB1alTRxaLRYZh8MvTA/z888+KiIjQyy+/rMaNG+e6jx7wDAcOHNCSJUt04cIFNW7cWF27dlWpUqX4d8CDJSYmavTo0UpLS5OXl5feeOMNtWjRQiVKlHB1abgFfv31V7Vr10516tRRSkqKkpOT9eSTT+of//gHfzv+CUekPMyBAwcUGRmpESNG6IMPPtDp06ddXRJc6LffflNERIR++eUXpaWlKSYmRoMHD9ann34qSY4/olB8JSYmKjIyUtHR0Zo8ebIMw9DZs2d1+PBhSZzm5wkSExN19913a/Xq1dq8ebP69OmjZ555RmvWrJHEvwOeaP/+/brnnntUsWJFhYWFqXTp0mrbtq0mTJigY8eOubo8FLKLFy9q0KBB6tWrl7Zs2aIffvhBixcv1tKlS9W/f38dPHjQ1SW6DYvBv44eIz09XS+99JLsdrvuvvtuDR06VMOHD9drr72moKAgV5eHW+zKlSsaMGCA/P399fHHH0u6GrRHjRqlo0ePqmfPnnrppZdcXCUK05kzZ9SiRQuVLl1aO3fulCT1799fv/zyi06cOKE6derovffeU5MmTQhUxdSlS5fUvXt31axZUx9++KEkaefOnRo0aJDKli2rwYMH69FHH3VxlbjVRo8erW3btjnCtCR98MEHio2N1bPPPqtXXnlFwcHBLqwQheny5ctq2bKlXnvtNfXo0cMxvm/fPrVs2VKtWrXSkiVL5OXl5cIq3QNHpDyI1WpVs2bNdP/992vw4MFauHChpkyZosmTJ3NkygOVKFFCycnJjj+QDcPQ7bffrsmTJ6tevXpasmSJVq5c6eIqUZgqVKig+++/XyVLltTYsWMVERGhkydPatCgQZoxY4ZsNpu6devmePeR992KH39/f509e9bxZprdbtddd92lzz//XFlZWfr444/1888/u7hK3GqXLl1y/H9WVpYk6cUXX9T48eP14YcfatmyZZKu9guKl+zsbGVnZys5OVl79+51jNtsNtWtW1fr169XfHy8Jk6c6MIq3QdByoP4+/urb9++jncXunfvri+//FJTpkzRpEmTdObMGUlX/2HMOa0HxVN2drZsNpuqVaums2fPOi4ottvtqlGjhkaPHq2srCzNnz/fxZWisOT8AfTBBx8oIiJCs2bNUqVKlTR37lwNHDhQ3bp10+bNm1WqVCm99dZbkjjNrzjJ+f6npaXJ19dXp06dknQ1LGdlZalevXqaPn26du/erTlz5riyVLhAjRo1tGXLFp04cULe3t66cuWKJGnQoEF67bXXNGLECP3xxx+yWvkzsrjIWbXVy8tLJUuW1KuvvqpPPvlE33zzjSTJx8dHNptNjRs3VnR0tL755hudPXvW499g4yfAw5QsWVLS1T+kDcNQjx49tGDBAk2dOlWTJk3SiRMnNHz4cA0fPlwZGRkurhYFLWe1Ri8vL/n4+Khv375atmyZPvroI1ksFlmtVmVnZ+u2227TxIkTtXjxYiUmJrq4ahSk9PR0paWl6eLFi46xqVOnasSIEerfv78qVaok6X+9Uq9ePaWnp7ukVhSOhIQEPfLII0pPT1fp0qU1ePBgzZo1S0uXLpWXl5esVqtsNpsaNGigyZMna968eVwX42Gef/55hYWF6fHHH9eZM2dUokQJXb58WZL03HPPqVy5ctq+fbuLq0RByW/V1gceeEAtW7bU5MmTHSv2+fj4SJKCgoKUmpoqPz8/j3+DjSDloXLOa7Xb7XryySf15Zdfatq0aWrfvr0++OADjR49WgEBAS6uEgVp3759mjZtmk6ePOkYa9OmjSZNmqRXXnnFscBETm+ULl1ad9xxhyN8o+j77bff9Nhjj6lNmzaqX7++5s+f7whMr776qh566CHHL0UvLy/Ham0NGjSQxKl9xcHPP/+se+65R3feeafjZ7tbt24aMmSIevXqpZUrV8pqtTr+YCpbtqwqV67MvwPF2L59+/T666+rX79+eu+997R//36VKFFCMTExstvt6tGjh86ePSs/Pz9Jkq+vr0qWLOnoERRtOau2RkZG5lq19Y477tCAAQNUrlw5jRo1SgsXLpR09RS/Q4cOqVKlSnyUjiQZ8Gh2u92w2+2GYRhG+/btjfLlyxu//PKLi6tCQdu/f79Rvnx5w2KxGNHR0UZKSorjvvT0dCM2NtawWCzGqFGjjJ07dxpnzpwxRo4cadx+++3GqVOnXFg5CkpiYqJRoUIF45VXXjHmz59vREVFGT4+PsauXbvynW+z2YxRo0YZVapUMfbv339ri0Wh+Pnnn42SJUsaI0aMyDWelZVlnD592hgyZIjh4+NjzJw50zh58qRx6dIlY+TIkUaTJk2Ms2fPuqhqFKbExESjTJkyxv333288/vjjRpkyZYz27dsb8+bNMwzDMFauXGlEREQYoaGhxpo1a4zvv//eGDVqlFG5cmXj6NGjLq4ef9fu3bsNf39/Y8yYMYZhXP2b8MyZM8aBAwccc7Zs2WI8//zzhre3t9GkSROjRYsWRrly5a75u8PTsGoflJ2drREjRmjatGlKSEjI8zkyKNqutVrjiBEjVLFiRUlXj0x+8cUXev311+Xl5aXSpUsrNTVVK1eu1F133eXiPcDfdfbsWfXs2VP16tXTe++95xhv166dGjVqpPfffz/XZwXFx8frgw8+0E8//aRVq1YpLCzMVaWjgCQlJSksLExNmjTR6tWrlZ2dreHDh2vv3r06evSoXnjhBTVs2FC//vqrhg8frpCQEJUuXVonT57UmjVr6IFi6Hortx46dEjPPvusnnvuOf3+++8aN26c1q1bp3LlysnHx0fz5s3jd0MRd6NVW2vXrq0PP/xQTZo00cWLF7V7926tW7dOFStW1H333afbb7/dxXvgHvhAXkiS7rzzTu3cuZMQVQzlrNZYoUIF9ejRQ0FBQXryySclyRGmrFar+vTpo9atW+vYsWPKyMhQo0aNFBIS4uLqURBsNpvOnz+vJ554QtLV4Gy1WhUaGqqzZ89KUq7VG0NDQx3Xx9SrV89ldaNgRUZG6o8//tCKFSs0a9Ys2Ww2NW3aVKGhoZo2bZratWunadOmqU2bNtqzZ48Mw1CLFi1Us2ZNV5eOQpCzcmtoaKik3Cu3xsTEaN68eapevbq6dOmiBQsWaM+ePQoMDFSJEiX4yJRiIGfV1oSEBI0dO1arVq1ShQoVNGjQIFWsWFGTJ09W165d9f333+v2229XixYt1KJFC1eX7XY4IgVJ4pPri7n09PRc1zgsWrRIPXv21KuvvqrXX39dQUFBysrK0okTJ1SjRg0XVorCsn//ftWpU0fS1WDl4+Oj0aNH6+jRo5o3b55jXkZGhgICApSdnc1nhBQzJ0+e1MiRI7V48WK1atVKX375pSpUqCBJmj9/voYMGaIvvvhCDz30kIsrRWHLzs6W3W7XoEGDlJaWpi+++EIlSpSQYRiyWq06dOiQnn76aVWvXl2LFi2SxN8JxUnOm2nS1etj58+fr/DwcM2ePTvX54M1bNhQ4eHhmjt3rosqdX8ckYIkljUu7v68WqPValWPHj1kGIZ69eoli8Wil19+WVOmTHH8UR0QEEBPFDM5IcputzsuEjcMw7HstSRNnDhRJUqU0LBhw+Ttza+H4qZKlSqaOHGiQkJC1KFDB1WoUMHxx/FTTz2lsWPH6t///jdBqhjLeYMk56tv376677779NFHH+mll16SxWLJtXJr+/btlZiYqDvvvJPfCcVAenq67Ha7DMNQYGCgpKurtlatWlWhoaG5Vm318vJi1VYn8JsS8CA5K7HlrNZosVjUu3dvff311zp48KB++uknVucq5qxWa653lnPelRwzZozeeust7dq1ixBVjFWtWlUjR450rMBmsVhkGIbOnj2rihUrci1UMbZv3z6tXLlSvXr1UpUqVSTlXrk1ICBAzz77LCu3FlO//fabXnnlFaWkpCg5OVmTJ0/Wk08+KS8vL7366qu6cuXKDVdtJUznxW9LwMP8+VqYHj166OOPP1ZCQoJ27typRo0aubg63Ao5vxC9vb1VvXp1TZkyRZMnT9b27dvVpEkTV5eHQpbzTnQOi8Wi999/X6dPn1bLli1dVBUK04EDBxQZGalz587pzJkzioqKclzn9MILLyg9PV3PPfecjh49qscee0w1a9bU4sWLZbPZCFLFwG+//abWrVurT58+Cg8P144dO9SvXz/deeedatq0qaSr18zlyMrKUmxsrP773/9q4sSJkjhz6Vq4RgrwUKzWiPHjx2v06NEKDAzUunXrFB4e7uqScIstXLhQGzZs0OLFi7V+/XqOSBVDrNzq2Vi1tXBxRArwYKzW6Nk6d+6s0aNHa/PmzY7TN+BZGjRooC+++EL/+c9/dOedd7q6HBQCVm71bKzaWrg4IgV4MM55xl9XdITnuXLlSq7TelD8sHKrZ2PV1sLDESnAgxGiQIgCIar4Y+VWz8aqrYWHVwoAAMADsHKrZ2PV1oJndXUBAAAAuDUsFotj2fsePXro3nvvVUpKinbu3OlYwQ3FV84VPazaWjCInQAAAB4k54N3R4wYoQ0bNighIYGPv/AQOUehfHx89MknnygwMFA//PADqzPeJI5IAQAAeCBWbvVcnTt3liRt3ryZj774G1i1DwAAwAOxcqtnY9XWv48gBQAAAAAmcWofAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQDATZg7d64sFou2b9/u6lIAAC5AkAIAuK2csHKtrx9//NHVJQIAPJS3qwsAAOBG/vGPfyg0NDTP+O233+6CagAAIEgBAIqALl26KDw83NVlAADgwKl9AIAi7ciRI7JYLJoyZYr++c9/qmbNmvL391ebNm20e/fuPPO///573XvvvSpZsqTKli2rRx55RL///nueecePH9eAAQNUtWpV+fr6KjQ0VC+88IKuXLmSa15mZqaioqJUsWJFlSxZUo8++qhSUlJyzdm+fbs6d+6soKAg+fv7KzQ0VP379y/YFwIAcEtxRAoA4PYuXLig06dP5xqzWCyqUKGC4/a8efOUlpamIUOG6PLly3rvvffUvn17/frrrwoODpYkrVu3Tl26dNFtt92msWPH6tKlS/rggw/UsmVL7dy5U7Vq1ZIknThxQhERETp//ryee+451atXT8ePH9eSJUuUkZGhEiVKOJ73xRdfVLly5RQTE6MjR45o2rRpGjp0qBYtWiRJOnXqlDp16qSKFStq5MiRKlu2rI4cOaKlS5cW8qsGAChMBCkAgNvr0KFDnjFfX19dvnzZcfvAgQPav3+/QkJCJEn333+/mjdvrkmTJundd9+VJI0YMULly5fXli1bVL58eUlSt27dFBYWppiYGH322WeSpOjoaCUlJWnr1q25Tin8xz/+IcMwctVRoUIFrV27VhaLRZJkt9v1/vvv68KFCypTpow2b96sc+fOae3atbke66233iqIlwYA4CKc2gcAcHvTp09XfHx8rq/vvvsu15xu3bo5QpQkRUREqHnz5lq1apUk6eTJk0pISNAzzzzjCFGS1LhxY3Xs2NExz263a/ny5eratWu+12XlBKYczz33XK6xe++9V9nZ2Tp69KgkqWzZspKkb775Rjab7W+8CgAAd8IRKQCA24uIiLjhYhN16tTJM1a3bl199dVXkuQINnfccUeeefXr19eaNWuUnp6uixcvKjU1VQ0bNnSqtho1auS6Xa5cOUnSuXPnJElt2rTR448/rtjYWP3zn/9U27Zt1a1bN/Xq1Uu+vr5OPQcAwP1wRAoAgL/By8sr3/GcUwAtFouWLFmiLVu2aOjQoTp+/Lj69++vZs2a6eLFi7eyVABAASJIAQCKhf379+cZ27dvn2MBiZo1a0qS9u7dm2fenj17FBQUpJIlS6pixYoKDAzMd8W/v6NFixYaP368tm/frvnz5ysxMVELFy4s0OcAANw6BCkAQLGwfPlyHT9+3HF727Zt2rp1q7p06SJJqlKlipo2barPPvtM58+fd8zbvXu31q5dqwceeECSZLVa1a1bN61cuVLbt2/P8zx/XWziRs6dO5dnm6ZNm0q6unQ6AKBo4hopAIDb++6777Rnz5484/fcc4+s1qvvCd5+++1q1aqVXnjhBWVmZmratGmqUKGCXnvtNcf8d955R126dFFkZKQGDBjgWP68TJkyGjt2rGPehAkTtHbtWrVp00bPPfec6tevr5MnT2rx4sX64YcfHAtIOOOzzz7TjBkz9Oijj6p27dpKS0vTJ598osDAQEd4AwAUPQQpAIDbGzNmTL7jc+bMUdu2bSVJffr0kdVq1bRp03Tq1ClFREToww8/VJUqVRzzO3TooNWrVysmJkZjxoyRj4+P2rRpo0mTJik0NNQxLyQkRFu3btXo0aM1f/58paamKiQkRF26dFFAQICp2tu0aaNt27Zp4cKFSk5OVpkyZRQREaH58+fnek4AQNFiMcyeowAAgBs5cuSIQkND9c4772j48OGuLgcA4CG4RgoAAAAATCJIAQAAAIBJBCkAAAAAMIlrpAAAAADAJI5IAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASf8f4ltZyNEjii4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n",
            "<ipython-input-28-485544f28447>:8: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(data=mean_mae_by_hyperparameter, x=param, y='mae', ci=None, palette=\"pastel\")  # Membuat bar plot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}